#!/bin/bash

"exec" "python" "-u" "-Wignore" "$0" "$@"

import re
import os
import sys
import time
import shutil
import atexit
import datetime
import optparse
import commands
import urllib
import shelve
import copy
import types

# default cloud/site
defaultCloud = None
defaultSite  = 'AUTO'

# max lookup for cross site option
maxCrossSite = 50

# error code
EC_Config    = 10
EC_Dataset   = 40
EC_Post      = 50
EC_Archive   = 60
EC_Split     = 70
EC_MyProxy   = 80
EC_Submit    = 90

usage = """%prog [options]

  HowTo is available at https://twiki.cern.ch/twiki/bin/view/PanDA/PandaRun"""

# command-line parameters
optP = optparse.OptionParser(usage=usage,conflict_handler="resolve")
optP.add_option('--version',action='store_const',const=True,dest='version',default=False,
                help='Displays version')
optP.add_option('--inDS',action='store',dest='inDS',default='',type='string',
                help='Name of an input dataset or dataset container')
optP.add_option('--inDsTxt',action='store',dest='inDsTxt',default='',type='string',
                help='a text file which contains the list of datasets to run over. newlines are replaced by commas and the result is set to --inDS. lines starting with # are ignored')
optP.add_option('--goodRunListXML', action='store', dest='goodRunListXML', default='',
                type='string', help='Good Run List XML which will be converted to datasets by AMI')
optP.add_option('--goodRunListDataType', action='store', dest='goodRunDataType', default='',
                type='string', help='specify data type when converting Good Run List XML to datasets, e.g, AOD (default)')
optP.add_option('--goodRunListProdStep', action='store', dest='goodRunProdStep', default='',
                type='string', help='specify production step when converting Good Run List to datasets, e.g, merge (default)')
optP.add_option('--goodRunListDS', action='store', dest='goodRunListDS', default='',
                type='string', help='A comma-separated list of pattern strings. Datasets which are converted from Good Run List XML will be used when they match with one of the pattern strings. Either \ or "" is required when a wild-card is used. If this option is omitted all datasets will be used')
optP.add_option('--eventPickEvtList',action='store',dest='eventPickEvtList',default='',
                type='string', help='a file name which contains a list of runs/events for event picking')
optP.add_option('--eventPickDataType',action='store',dest='eventPickDataType',default='',
                type='string', help='type of data for event picking. one of AOD,ESD,RAW')
optP.add_option('--eventPickStreamName',action='store',dest='eventPickStreamName',default='',
                type='string', help='stream name for event picking. e.g., physics_CosmicCaloEM')
optP.add_option('--eventPickDS',action='store',dest='eventPickDS',default='',
                type='string', help='A comma-separated list of pattern strings. Datasets which are converted from the run/event list will be used when they match with one of the pattern strings. Either \ or "" is required when a wild-card is used. e.g., data\*')
optP.add_option('--eventPickStagedDS',action='store',dest='eventPickStagedDS',default='',
                type='string', help='--eventPick options create a temporary dataset to stage-in interesting files when those files are available only on TAPE, and then a stage-in request is automatically sent to DaTRI. Once DaTRI transfers the dataset to DISK you can use the dataset as an input using this option')
optP.add_option('--eventPickAmiTag',action='store',dest='eventPickAmiTag',default='',
                type='string', help='AMI tag used to match TAG collections names. This option is required when you are interested in older data than the latest one. Either \ or "" is required when a wild-card is used. e.g., f2\*')
optP.add_option('--eventPickNumSites',action='store',dest='eventPickNumSites',default=1,
                type='int', help='The event picking service makes a temprary dataset container to stage files to DISK. The consistuent datasets are distibued to N sites (N=1 by default)')
optP.add_option('--eventPickSkipDaTRI',action='store_const',const=True,dest='eventPickSkipDaTRI',default=False,
                help='Skip sending a staging request to DaTRI for event picking')
optP.add_option('--eventPickWithGUID',action='store_const',const=True,dest='eventPickWithGUID',default=False,
                help='Using GUIDs together with run and event numbers in eventPickEvtList to skip event lookup')
optP.add_option('--express', action='store_const',const=True,dest='express',default=False,
                help="Send the job using express quota to have higher priority. The number of express subjobs in the queue and the total execution time used by express subjobs are limited (a few subjobs and several hours per day, respectively). This option is intended to be used for quick tests before bulk submission. Note that buildXYZ is not included in quota calculation. If this option is used when quota has already exceeded, the panda server will ignore the option so that subjobs have normal priorities. Also, if you submit 1 buildXYZ and N runXYZ subjobs when you only have quota of M (M < N),  only the first M runXYZ subjobs will have higher priorities")
optP.add_option('--debugMode', action='store_const',const=True,dest='debugMode',default=False,
                help="Send the job with the debug mode on. If this option is specified the subjob will send stdout to the panda monitor every 5 min. The number of debug subjobs per user is limited. When this option is used and the quota has already exceeded, the panda server supresses the option so that subjobs will run without the debug mode. If you submit multiple subjobs in a single job, only the first subjob will set the debug mode on. Note that you can turn the debug mode on/off by using pbook after jobs are submitted" )
optP.add_option('--useContElementBoundary',action='store_const',const=True,dest='useContElementBoundary',default=False,
                help="Split job in such a way that sub jobs do not mix files of different datasets in the input container. See --useNthFieldForLFN too")
optP.add_option('--addNthFieldOfInDSToLFN',action='store',dest='addNthFieldOfInDSToLFN',default='',type='string',
                help="A middle name is added to LFNs of output files when they are produced from one dataset in the input container or input dataset list. The middle name is extracted from the dataset name. E.g., if --addNthFieldOfInDSToLFN=2 and the dataset name is data10_7TeV.00160387.physics_Muon..., 00160387 is extracted and LFN is something like user.hoge.TASKID.00160387.blah. Concatenate multiple field numbers with commas if necessary, e.g., --addNthFieldOfInDSToLFN=2,6.")
optP.add_option('--addNthFieldOfInFileToLFN',action='store',dest='addNthFieldOfInFileToLFN',default='',type='string',
                help="A middle name is added to LFNs of output files similarly as --addNthFieldOfInDSToLFN, but strings are extracted from input file names")
optP.add_option('--buildInLastChunk',action='store_const',const=True,dest='buildInLastChunk',default=False,
                help="Produce lib.tgz in the last chunk when jobs are split to multiple chunks due to the limit on the number of files in each chunk or due to --useContElementBoundary/--loadXML")
optP.add_option('--followLinks',action='store_const',const=True,dest='followLinks',default=False,
                help="Resolve symlinks to directories when building the input tarball. This option requires python2.6 or higher")
optP.add_option('--outDS',action='store',dest='outDS',default='',type='string',
                help='Name of an output dataset. OUTDS will contain all output files')
optP.add_option('--destSE',action='store', dest='destSE',default='',type='string',
                help='Destination strorage element')
optP.add_option('--libDS',action='store',dest='libDS',default='',type='string',
                help='Name of a library dataset')
optP.add_option('--removedDS', action='store', dest='removedDS', default='',
                type='string', help="don't use datasets in the input dataset container")
optP.add_option('--useHomeDir', action='store_const',const=True,dest='useHomeDir',default=False,
                help='execute prun just under the HOME dir')
optP.add_option('--noBuild', action='store_const',const=True,dest='nobuild',default=False,
                help='Skip buildGen')
optP.add_option('--noCompile', action='store_const',const=True,dest='noCompile',default=False,
                help='Just upload a tarball in the build step to avoid the tighter size limit imposed by --noBuild. The tarball contains binaries compiled on your local computer, so that compilation is skipped in the build step on remote WN')
optP.add_option('--individualOutDS',action='store_const',const=True,dest='individualOutDS',default=False,
                help='Create individual output dataset for each data-type. By default, all output files are added to one output dataset')
optP.add_option('--transferredDS',action='store', dest='transferredDS',default='',type='string',
                help='Specify a comma-separated list of patterns so that only datasets which match the given patterns are transferred when --destSE is set. Either \ or "" is required when a wildcard is used. If omitted, all datasets are transferred')
optP.add_option('--secondaryDSs',action='store',dest='secondaryDSs',default='',type='string',
                help='List of secondary datasets when the job requires multiple inputs. See PandaRun wiki page for detail')
optP.add_option('--reusableSecondary',action='store',dest='reusableSecondary',default='',type='string',
                help='A comma-separated list of secondary streams which reuse files when all files are used')
optP.add_option('--site',action='store',dest='site',default=defaultSite,type='string',
                help='Site name where jobs are sent. If omitted, jobs are automatically sent to sites where input is available. A comma-separated list of sites can be specified (e.g. siteA,siteB,siteC), so that best sites are chosen from the given site list. If AUTO is appended at the end of the list (e.g. siteA,siteB,siteC,AUTO), jobs are sent to any sites if input is not found in the previous sites')
optP.add_option('--cloud',action='store',dest='cloud',default=None,type='string',
                help='Cloud where jobs are submitted. default is set according to your VOMS country group')
optP.add_option('--match',action='store',dest='match',default='',type='string',
                help='Use only files matching with given pattern')
optP.add_option('--antiMatch',action='store',dest='antiMatch',default='',type='string',
                help='Skip files matching with given pattern')
optP.add_option('--notSkipLog',action='store_const',const=True,dest='notSkipLog',default=False,
                help="Don't skip log files in input datasets (obsolete. use --useLogAsInput instead)")
optP.add_option('--skipScan',action='store_const',const=True,dest='skipScan',default=False,
                help='Skip LFC lookup at job submission')
optP.add_option('--memory',action='store',dest='memory',default=-1,type='int',
                help='Required memory size in MB. e.g., for 1GB --memory 1024')
optP.add_option('--maxCpuCount',action='store',dest='maxCpuCount',default=-1,type='int',
                help='Required CPU count in seconds. Mainly to extend time limit for looping job detection')
optP.add_option('--safetySize',action='store',dest='safetySize',default=500,type='int',
                help='Specify the expected size of input sandbox in MB. 500 by default')
optP.add_option('--useShortLivedReplicas', action='store_const', const=True, dest='useShortLivedReplicas', default=False,
                help="Use replicas even if they have very sort lifetime")
optP.add_option('--useDirectIOSites', action='store_const', const=True, dest='useDirectIOSites', default=False,
                help="Use only sites which use directIO to read input files")
optP.add_option('--official',action='store_const',const=True,dest='official',default=False,
                help='Produce official dataset')
optP.add_option('--unlimitNumOutputs', action='store_const', const=True, dest='unlimitNumOutputs',  default=False,
                help='Remove the limit on the number of outputs. Note that having too many outputs per job causes a severe load on the system. You may be banned if you carelessly use this option') 
optP.add_option('--descriptionInLFN',action='store',dest='descriptionInLFN',default='',
                help='LFN is user.nickname.jobsetID.something (e.g. user.harumaki.12345.AOD._00001.pool) by default. This option allows users to put a description string into LFN. i.e., user.nickname.jobsetID.description.something')
optP.add_option('--useRootCore',action='store_const',const=True,dest='useRootCore',default=False,
                help='Use RootCore. See PandaRun wiki page for detail')
optP.add_option('--useMana',action='store_const',const=True,dest='useMana',default=False,
                help='Use Mana. See PandaRun wiki page for detail')
optP.add_option('--manaVer',action='store',dest='manaVer',default='',type='string',
                help='Specify a mana version if you want to use an older version. If omitted, the latest version is used. e.g., --manaVer=20130301' )
optP.add_option('--useAthenaPackages',action='store_const',const=True,dest='useAthenaPackages',default=False,
                help='Use Athena packages. See PandaRun wiki page for detail')
optP.add_option('--gluePackages', action='store', dest='gluePackages',  default='',
                help='list of glue packages which pathena cannot fine due to empty i686-slc4-gcc34-opt. e.g., External/AtlasHepMC,External/Lhapdf')
optP.add_option('--nFiles',action='store',dest='nFiles',default=0,type='int',
                help='Use a limited number of files in the input dataset')
optP.add_option('--nSkipFiles',action='store',dest='nSkipFiles',default=0,type='int',
                help='Skip N files in the input dataset')
optP.add_option('--nFilesPerJob',action='store',dest='nFilesPerJob',default=None,type='int',
                help='Number of files on which each sub-job runs (default 50). Note that this is the number of files per sub-job in the primary dataset even if --secondaryDSs is used')
optP.add_option('--nJobs',action='store',dest='nJobs',default=-1,type='int',
                help='Maximum number of sub-jobs. If the number of input files (N_in) is less than nJobs*nFilesPerJob, only N_in/nFilesPerJob sub-jobs will be instantiated')
optP.add_option('--nEvents',action='store',dest='nEvents',default=-1,type='int',
                help='The total number of events to be processed. This option is considered only when either --inDS or --pfnList is not used')
optP.add_option('--nEventsPerChunk',action='store',dest='nEventsPerChunk',default=-1,type='int',
                help='Set granuarity to split events. The number of events per job is multiples of nEventsPerChunk. This option is considered only when --nEvents is used but --nJobs is not used. If this option is not set, nEvents/20 is used as nEventsPerChunk')
optP.add_option('--nGBPerJob',action='store',dest='nGBPerJob',default=-1,
                help='Instantiate one sub job per NGBPERJOB GB of input files. --nGBPerJob=MAX sets the size to the default maximum value')
optP.add_option('--maxFileSize',action='store',dest='maxFileSize',default=1024*1024,type='int',
                help='Maximum size of files to be sent to WNs (default 1024*1024B)')
optP.add_option('--athenaTag',action='store',dest='athenaTag',default='',type='string',
                help='Tags to setup Athena on remote WNs, e.g., --athenaTag=AtlasProduction,14.2.24.3')
optP.add_option('--rootVer',action='store',dest='rootVer',default='',type='string',
                help='Specify a ROOT version which is not included in Athena. This option can not be used together with --noBuild, e.g., --rootVer=5.28/00' )
optP.add_option('--workDir',action='store',dest='workDir',default='.',type='string',
                help='All files under WORKDIR will be transfered to WNs (default=./)')
optP.add_option('--extFile',action='store',dest='extFile',default='',
                help='root or large files under WORKDIR are not sent to WNs by default. If you want to send some skipped files, specify their names, e.g., data.root,data.tgz')
optP.add_option('--excludeFile',action='store',dest='excludeFile',default='',
                help='specify a comma-separated string to exclude files and/or directories when gathering files in local working area. Either \ or "" is required when a wildcard is used. e.g., doc,\*.C')
optP.add_option('--inputFileList', action='store', dest='inputFileListName', default='',
                type='string', help='name of file which contains a list of files to be run in the input dataset')
optP.add_option('--crossSite',action='store',dest='crossSite',default=maxCrossSite,
                type='int',help='submit jobs to N sites at most when datasets in container split over many sites (N=%s by default)' % maxCrossSite)
optP.add_option('--outputs',action='store',dest='outputs',default='',type='string',
                help='Names of output files. Comma separated. e.g., --outputs out1.dat,out2.txt. You can specify a suffix for each output container like <datasetNameSuffix>:<outputFileName>. e.g., --outputs AAA:out1.dat,BBB:out2.txt. In this case output container names are outDS_AAA/ and outDS_BBB/ instead of outDS_out1.dat/ and outDS_out2.txt/')
optP.add_option('--allowNoOutput',action='store',dest='allowNoOutput',default='',type='string',
                help='A comma-separated list of regexp patterns. Output files are allowed not to be produced if their filenames match with one of regexp patterns. Jobs go to finished even if they are not produced on WN')
optP.add_option('--excludedSite', action='append', dest='excludedSite',  default=[],
                help="list of sites which are not used for site section, e.g., ANALY_ABC,ANALY_XYZ")
optP.add_option('--useLogAsInput',action='store_const',const=True,dest='useLogAsInput',default=False,
                help="log.tgz files in inDS are ignored by default. This option allows log files to be used as input")
optP.add_option('--noSubmit',action='store_const',const=True,dest='nosubmit',default=False,
                help="Don't submit jobs")
optP.add_option('--prodSourceLabel', action='store', dest='prodSourceLabel', default='',
                help="set prodSourceLabel")
optP.add_option('--processingType', action='store', dest='processingType', default='prun',
                help="set processingType")
optP.add_option('--seriesLabel', action='store', dest='seriesLabel',  default='',
                help="set seriesLabel")
optP.add_option('--workingGroup', action='store', dest='workingGroup',  default=None,
                help="set workingGroup")
optP.add_option('--tmpDir',action='store',dest='tmpDir',default='',type='string',
                help='Temporary directory where an archive file is created')
optP.add_option('--voms', action='store', dest='vomsRoles',  default=None, type='string',
                help="generate proxy with paticular roles. e.g., atlas:/atlas/ca/Role=production,atlas:/atlas/fr/Role=pilot")
optP.add_option('--noEmail', action='store_const', const=True, dest='noEmail',  default=False,
                help='Suppress email notification')
optP.add_option('--update', action='store_const', const=True, dest='update',  default=False,
                help='Update panda-client to the latest version')
optP.add_option('--spaceToken',action='store',dest='spaceToken',default='',type='string',
                help='spacetoken for outputs. e.g., ATLASLOCALGROUPDISK')
optP.add_option('--skipScout', action='store_const',const=True,dest='skipScout',default=False,
                help="skip scout jobs")
optP.add_option('--respectSplitRule', action='store_const',const=True,dest='respectSplitRule',default=False,
                help="force scout jobs to follow split rules like nGBPerJob")
optP.add_option('--nGBPerMergeJob',action='store',dest='nGBPerMergeJob',default=-1,
                help='Instantiate one merge job per NGBPERMERGEJOB GB of pre-merged files')
optP.add_option('--devSrv',action='store_const',const=True,dest='devSrv',default=False,
                help="Please don't use this option. Only for developers to use the dev panda server")
optP.add_option('--intrSrv', action='store_const', const=True, dest='intrSrv',  default=False,
                help="Please don't use this option. Only for developers to use the intr panda server")
optP.add_option('--outTarBall', action='store', dest='outTarBall', default='',
                type='string', help='Save a gzipped tarball of local files which is the input to buildXYZ')
optP.add_option('--inTarBall', action='store', dest='inTarBall', default='',
                type='string', help='Use a gzipped tarball of local files as input to buildXYZ. Generall the tarball is created by using --outTarBall')
optP.add_option('--exec',action='store',dest='jobParams',default='',type='string',
                help='execution string. e.g., --exec "./myscript arg1 arg2"')
optP.add_option('--bexec',action='store',dest='bexec',default='',type='string',
                help='execution string for build stage. e.g., --bexec "make"')
optP.add_option('--disableAutoRetry',action='store_const',const=True,dest='disableAutoRetry',default=False,
                help='disable automatic job retry on the server side')
optP.add_option('--myproxy',action='store',dest='myproxy',default='myproxy.cern.ch',type='string',
                help='Name of the myproxy server')
optP.add_option('--maxNFilesPerJob',action='store',dest='maxNFilesPerJob',default=200,type='int',
                help='The maximum number of files per job is 200 by default since too many input files result in a too long command-line argument on WN which crashes the job. This option relax the limit. In many cases it is better to use this option together with --writeInputToTxt')
optP.add_option('--writeInputToTxt',action='store',dest='writeInputToTxt',default='',type='string',
                help='Write the input file list to a file so that your application gets the list from the file instead of stdin. The argument is a comma separated list of StreamName:FileName. e.g., IN:input1.txt,IN2:input2.txt')
optP.add_option('--dbRelease',action='store',dest='dbRelease',default='',
                type='string', help='DBRelease or CDRelease (DatasetName:FileName). e.g., ddo.000001.Atlas.Ideal.DBRelease.v050101:DBRelease-5.1.1.tar.gz. If --dbRelease=LATEST, the latest DBRelease is used. Most likely the --useAthenaPackages or --athenaTag option is required to setup Athena runtime on WN')
optP.add_option('--dbRunNumber',action='store',dest='dbRunNumber', default='',
                type='string', help='RunNumber for DBRelease or CDRelease. If this option is used some redundant files are removed to save disk usage when unpacking DBRelease tarball. e.g., 0091890')
optP.add_option('--notExpandDBR',action='store_const',const=True,dest='notExpandDBR',default=False,
                help='By default, DBRelease.tar.gz is expanded on WN and gets deleted after changing environment variables accordingly. If you need tar.gz, use this option')
optP.add_option('--mergeOutput', action='store_const', const=True, dest='mergeOutput', default=False,
                help="merge output files")
optP.add_option('--mergeScript',action='store',dest='mergeScript',default='',type='string',
                help='Specify user-defied script execution string for output merging')
optP.add_option('--provenanceID',action='store',dest='provenanceID',default=-1,type='int',
                help='provenanceID')
optP.add_option('--useSiteGroup',action='store',dest='useSiteGroup',default=-1,type='int',
                help='Use only site groups which have group numbers not higher than --siteGroup. Group 0: T1 or undefined, 1,2,3,4: alpha,bravo,charlie,delta which are defined based on site reliability')
optP.add_option('-v',action='store_const',const=True,dest='verbose',default=False,
                help='Verbose')
optP.add_option('--long', action='store_const',const=True,dest='long',default=False,
                help='Send job to a long queue')
optP.add_option('--pfnList',action='store',dest='pfnList',default='',type='string',
                help='Name of file which contains a list of input PFNs. Those files can be un-registered in DDM')
optP.add_option('--outputPath',action='store',dest='outputPath', default='',
                type='string', help='Physical path of output directory relative to a root path')
optP.add_option('--useOldStyleOutput',action='store_const',const=True,dest='useOldStyleOutput',default=False,
                help="use output dataset and long LFN instead of output dataset container and short LFN")
optP.add_option('--disableRebrokerage',action='store_const',const=True,dest='disableRebrokerage',default=False,
                help="disable auto-rebrokerage")
optP.add_option('--useChirpServer',action='store',dest='useChirpServer', default='',
                type='string', help='The CHIRP server where output files are written to. e.g., --useChirpServer voatlas92.cern.ch')
optP.add_option('--useGOForOutput',action='store',dest='useGOForOutput',default='',metavar='GOENDPOINT',
                type='string', help='The Globus Online server where output files are written to. e.g., --useGOForOutput voatlas92.cern.ch')
optP.add_option('--enableJEM',action='store_const',const=True,dest='enableJEM',default=False,
                help="enable JEM")
optP.add_option('--configJEM', action='store', dest='configJEM', default='',
                type='string', help='configration parameters for JEM')
optP.add_option('--cmtConfig', action='store', dest='cmtConfig', default=None,
                type='string', help='CMTCONFIG=i686-slc5-gcc43-opt is used on remote worker-node by default even if you use another CMTCONFIG locally. This option allows you to use another CMTCONFIG remotely. e.g., --cmtConfig x86_64-slc5-gcc43-opt. If you use --libDS together with this option, make sure that the libDS was compiled with the same CMTCONFIG, in order to avoid failures due to inconsistency in binary files')
optP.add_option('--requireLFC',action='store_const',const=True,dest='requireLFC',default=False,
                help='Require that LFC lookup is 100% successful (a single miss will fail the submission)')
optP.add_option('--loadXML',action='store',dest='loadXML',default=None,
                help='Expert mode: load complete submission configuration from an XML file ')
optP.add_option('--forceStaged',action='store_const',const=True,dest='forceStaged',default=False,
                help='Force files from primary DS to be staged to local disk, even if direct-access is possible')
optP.add_option('--forceStagedSecondary',action='store_const',const=True,dest='forceStagedSecondary',default=False,
                help='Force files from secondary DSs to be staged to local disk, even if direct-access is possible')
optP.add_option('--queueData', action='store', dest='queueData', default='',
                type='string', help="Please don't use this option. Only for developers")
optP.add_option('--useNewCode',action='store_const',const=True,dest='useNewCode',default=False,
                help='When task are resubmitted with the same outDS, the original souce code is used to re-run on failed/unprocessed files. This option uploads new source code so that jobs will run with new binaries') 
optP.add_option('--allowTaskDuplication',action='store_const',const=True,dest='allowTaskDuplication',default=False,
                help="As a general rule each task has a unique outDS and history of file usage is recorded per task. This option allows multiple tasks to contribute to the same outDS. Typically useful to submit a new task with the outDS which was used by another broken task. Use this option very carefully at your own risk, since file duplication happens when the second task runs on the same input which the first task successfully processed")
optP.add_option('--useRucio',action='store_const',const=True,dest='useRucio',default=False,
                help="Use Rucio as DDM backend")
optP.add_option('--skipFilesUsedBy', action='store',dest='skipFilesUsedBy',default='',
                type='string', help='A comma-separated list of TaskIDs. Files used by those tasks are skipped when running a new task')
# internal parameters
optP.add_option('--panda_srvURL', action='store', dest='panda_srvURL', default='',
                type='string', help='internal parameter')
optP.add_option('--panda_cacheSrvURL', action='store', dest='panda_cacheSrvURL', default='',
                type='string', help='internal parameter')
optP.add_option('--panda_srcName', action='store', dest='panda_srcName', default='',
                type='string', help='internal parameter')
optP.add_option('--panda_inDS', action='store', dest='panda_inDS', default='',
                type='string', help='internal parameter')
optP.add_option('--panda_inDSForEP', action='store', dest='panda_inDSForEP', default='',
                type='string', help='internal parameter')
optP.add_option('--panda_origFullExecString', action='store', dest='panda_origFullExecString', default='',
                type='string', help='internal parameter')
optP.add_option('--panda_jobsetID',action='store',dest='panda_jobsetID',default=-1,
                type='int', help='internal parameter for jobsetID')
optP.add_option('--panda_parentJobsetID',action='store',dest='panda_parentJobsetID',default=-1,
                type='int', help='internal parameter for jobsetID')
optP.add_option('--panda_dbRelease', action='store', dest='panda_dbRelease', default='',
                type='string', help='internal parameter')
optP.add_option('--panda_exec', action='store', dest='panda_exec', default='',
                type='string', help='internal parameter')
optP.add_option('--panda_bexec', action='store', dest='panda_bexec', default='',
                type='string', help='internal parameter')
optP.add_option('--panda_suppressMsg',action='store_const',const=True,dest='panda_suppressMsg',default=False,
                help='internal parameter')

# parse options
options,args = optP.parse_args()
if options.verbose:
    print options
    print

# display version
from pandatools import PandaToolsPkgInfo
if options.version:
    print "Version: %s" % PandaToolsPkgInfo.release_version
    sys.exit(0)

from pandatools import Client
from pandatools import PsubUtils
from pandatools import AthenaUtils
from pandatools import GlobalConfig
from pandatools import AppConfig
from pandatools import MiscUtils
from pandatools import PLogger

# update panda-client
if options.update:
    res = PsubUtils.updatePackage(options.verbose)
    if res:
	sys.exit(0)
    else:
	sys.exit(1)

# full execution string
fullExecString = PsubUtils.convSysArgv()

# set dummy CMTSITE
if not os.environ.has_key('CMTSITE'):
    os.environ['CMTSITE'] = ''

# set grid source file
globalConf = GlobalConfig.getConfig()
if globalConf.grid_src != '' and not os.environ.has_key('PATHENA_GRID_SETUP_SH'):
    os.environ['PATHENA_GRID_SETUP_SH'] = globalConf.grid_src

# get logger
tmpLog = PLogger.getPandaLogger()

# set default
appConf = AppConfig.AppConfig('prun')
for tmpAppConfKey,tmpAppConfVal in appConf.getConfig().iteritems():
    # check lower characters just in case
    tmpAppConfKeys = [tmpAppConfKey,tmpAppConfKey.lower()]
    for tmpKey in tmpAppConfKeys:
        if hasattr(options,tmpKey):
            tmpSetAttFlag = False
	    if getattr(options,tmpKey) in [-1,0,None,'','AUTO',False]:
		setattr(options,tmpKey,tmpAppConfVal)
		tmpSetAttFlag = True
            elif getattr(options,tmpKey) == []:
                tmpSetValue = tmpAppConfVal.split(',')
                if '' in tmpSetValue:
                    tmpSetValue.remove('')
                setattr(options,tmpKey,tmpSetValue)
                tmpSetAttFlag = True
            if tmpSetAttFlag:
                # append parameter to metadata
                if isinstance(tmpAppConfVal,types.BooleanType):
                    fullExecString += ' --%s' % tmpKey
                else:
                    fullExecString += ' --%s=%s' % (tmpKey,tmpAppConfVal)
                if options.verbose:
                    tmpLog.debug("Use default option in panda.cfg %s=%s" % (tmpKey,getattr(options,tmpKey)))
            break

# use dev server
if options.devSrv:
    Client.useDevServer()

# use INTR server
if options.intrSrv:
    Client.useIntrServer()
    
# set server
if options.panda_srvURL != '':
    Client.setServer(options.panda_srvURL)
if options.panda_cacheSrvURL != '':
    Client.setCacheServer(options.panda_cacheSrvURL)

# version check
PsubUtils.checkPandaClientVer(options.verbose)

# obsolete options
if options.useOldStyleOutput:
    options.useOldStyleOutput = False
    tmpLog.warning("disabled --useOldStyleOutput since it is obsolete")

# noCompile uses noBuild stuff
if options.noCompile:
    options.nobuild = True

# not skip log files in inDS
if options.notSkipLog:
    options.useLogAsInput = True
    
# files to be deleted
delFilesOnExit = []
                            
# suffix for shadow dataset
suffixShadow = Client.suffixShadow

# load submission configuration from xml file (if provided)
xconfig = None
if options.loadXML != None:
    from pandatools import ParseJobXML
    xconfig = ParseJobXML.dom_parser(options.loadXML)
    tmpLog.info('dump XML config')
    xconfig.dump(options.verbose)
    if options.outDS=='':
        options.outDS=xconfig.outDS()
    options.outputs='all'
    options.jobParams='${XML_EXESTR}'
    options.inDS=xconfig.inDS()
    # check XML
    try:
        xconfig.files_in_DS(options.inDS)
    except:
        errtype,errvalue = sys.exc_info()[:2]
        print errvalue
        tmpLog.error('verification of XML failed')
        sys.exit(EC_Config)
    # inDS match and secondaryDS filter will be determined later from xconfig
    options.match=''
    options.secondaryDSs = xconfig.secondaryDSs_config(filter=False)
    # read XML
    xmlFH = open(options.loadXML)
    options.loadXML = xmlFH.read()
    xmlFH.close()

# save current dir
curDir = os.path.realpath(os.getcwd())

# remove whitespaces
if options.outputs != '':
    options.outputs = re.sub(' ','',options.outputs)

brokerageLogs  = []
userBrokerLogs = []

# exclude sites
if options.excludedSite != []:
    options.excludedSite = PsubUtils.splitCommaConcatenatedItems(options.excludedSite)

# use certain sites
includedSite = None
if re.search(',',options.site) != None:
    includedSite = PsubUtils.splitCommaConcatenatedItems([options.site])
    options.site = 'AUTO'

# set maxNFilesPerJob
PsubUtils.limit_maxNumInputs = options.maxNFilesPerJob 

# site specified
siteSpecified = True
if options.site == 'AUTO':
    siteSpecified = False

# cloud specified
if options.cloud != defaultCloud:
    # add logging info
    userBrokerLogs = PsubUtils.getUserBrokerageInfo(options.cloud,'cloud',userBrokerLogs)

# list of output files which can be skipped
options.allowNoOutput = options.allowNoOutput.split(',')

# use outputPath as outDS
if Client.isDQ2free(options.site):
    if options.outDS != '':
	options.outputPath = options.outDS
    else:
	options.outputPath = './'
    options.outDS = options.outputPath
else:
    # enforce to use output dataset container
    if not options.useOldStyleOutput and not options.outDS.endswith('/'):
        options.outDS = options.outDS + '/'

# keep original outDS
original_outDS_Name = options.outDS
     
# reset crossSite unless container is used for output 
if not original_outDS_Name.endswith('/'):
    options.crossSite = 0
    options.panda_jobsetID = None

usingContainerForOut = original_outDS_Name.endswith('/')

# read datasets from file
if options.inDsTxt != '':
    options.inDS = PsubUtils.readDsFromFile(options.inDsTxt)

# set inDS for recursive goodRunListXML
if options.panda_inDS != '':
    options.inDS = options.panda_inDS
    options.goodRunListXML = ''

# disable expiring file check
if options.useShortLivedReplicas:
    Client.useExpiringFiles()
    
# use staged dataset for event picking
if options.eventPickStagedDS != '':
    if options.panda_inDSForEP == '':
        options.panda_inDSForEP = options.eventPickStagedDS


# libDS
libds_file = '%s/libds_prun.dat' % os.environ['PANDA_CONFIG_ROOT']
if options.libDS == 'LAST':
    if not os.path.exists(libds_file):
        tmpLog.error("LAST cannot be used until you submit at least one job without --libDS")
        sys.exit(EC_Config)
    # read line
    tmpFile = open(libds_file)
    tmpLibDS = tmpFile.readline()
    tmpFile.close()
    # remove \n
    tmpLibDS = tmpLibDS.replace('\n','')
    # set
    options.libDS = tmpLibDS

# absolute path for PFN list
if options.pfnList != '':
    options.pfnList = os.path.realpath(options.pfnList)

# extract DBR from exec
tmpMatch = re.search('%DB:([^ \'\";]+)',options.jobParams)
if tmpMatch != None:
    options.dbRelease = tmpMatch.group(1)
    options.notExpandDBR = True

# check DBRelease
if options.dbRelease != '' and (options.dbRelease.find(':') == -1 and options.dbRelease !='LATEST'):
    tmpLog.error("invalid argument for --dbRelease. Must be DatasetName:FileName or LATEST")  
    sys.exit(EC_Config)

# check unmerge dataset
PsubUtils.checkUnmergedDataset(options.inDS,options.secondaryDSs)

# Good Run List
if options.goodRunListXML != '' and options.inDS != '':
    tmpLog.error("cannnot use --goodRunListXML and --inDS at the same time")
    sys.exit(EC_Config)

# event picking
if options.eventPickEvtList != '' and options.inDS != '':
    tmpLog.error("cannnot use --eventPickEvtList and --inDS at the same time")
    sys.exit(EC_Config)

# param check for event picking
if options.eventPickEvtList != '':
    if options.eventPickDataType == '':
        tmpLog.error("--eventPickDataType must be specified")
        sys.exit(EC_Config)

# check rootVer
if options.rootVer != '':
    if options.nobuild and not options.noCompile:
        tmpLog.error("--rootVer cannot be used together with --noBuild since ROOT is prepared in the build step")
        sys.exit(EC_Config)
    # change / to .
    options.rootVer = re.sub('/','.',options.rootVer)
    
# check writeInputToTxt
if options.writeInputToTxt != '':
    # remove %
    options.writeInputToTxt = options.writeInputToTxt.replace('%','')
    # loop over all StreamName:FileName
    for tmpItem in options.writeInputToTxt.split(','):
        tmpItems = tmpItem.split(':')
        if len(tmpItems) != 2:
            tmpLog.error("invalid StreamName:FileName in --writeInputToTxt : %s" % tmpItem)
            sys.exit(EC_Config)

# read list of files to be used                                                                                                                           
filesToBeUsed = []
if options.inputFileListName != '':
    rFile = open(options.inputFileListName)
    for line in rFile:
        line = re.sub('\n','',line)
        line = line.strip()
        if line != '':
            filesToBeUsed.append(line)
    rFile.close()
    
# check grid-proxy
gridPassPhrase,vomsFQAN = PsubUtils.checkGridProxy('',False,options.verbose,options.vomsRoles)

# add allowed sites
if not siteSpecified:
    tmpSt = Client.addAllowedSites(options.verbose)
    if not tmpSt:
        tmpLog.error("Failed to get allowed site list")
        sys.exit(EC_Config)

# removed datasets
if options.removedDS == '':
    options.removedDS = []
else:
    options.removedDS = options.removedDS.split(',')

# convert in/outTarBall to full path
if options.inTarBall != '':
    options.inTarBall = os.path.abspath(os.path.expanduser(options.inTarBall))
if options.outTarBall != '':
    options.outTarBall = os.path.abspath(os.path.expanduser(options.outTarBall))

# set workdir for mana
if options.useMana:
    tmpSt,tmpOut = MiscUtils.getManaSetupParam('workarea')
    if not tmpSt:
        tmpLog.error(tmpOut)
        sys.exit(EC_Config)
    options.workDir = tmpOut
    tmpLog.info("setting workDir=%s for mana" % options.workDir)
    # extract mana version number
    if options.manaVer == '':
        tmpSt,tmpOut = MiscUtils.getManaVer()
        if not tmpSt:
            tmpLog.error(tmpOut)
            sys.exit(EC_Config)
        options.manaVer = tmpOut    

# check working dir
options.workDir = os.path.realpath(options.workDir)
if options.workDir != curDir and (not curDir.startswith(options.workDir+'/')):
    tmpLog.error("you need to run prun in a directory under %s" % options.workDir)
    sys.exit(EC_Config)

# avoid gathering up the home dir
if os.environ.has_key('HOME') and not options.useHomeDir and not options.useAthenaPackages \
       and os.path.realpath(os.path.expanduser(os.environ['HOME'])) == options.workDir:
    tmpStr  = 'prun is executed just under the HOME directoy '
    tmpStr += 'and is going to send all files under the dir including ~/Mail/* and ~/private/*. '
    tmpStr += 'Do you really want that? (Please use --useHomeDir if you want to skip this confirmation)'
    tmpLog.warning(tmpStr)
    while True:
        tmpAnswer = raw_input('y/N: ')
        tmpAnswer = tmpAnswer.strip()
        if tmpAnswer in ['y','N']:
            break
    if tmpAnswer == 'N':
        sys.exit(EC_Config)
        
# run dir
runDir = '.'
if curDir != options.workDir:
    # remove special characters
    wDirString=re.sub('[\+]','.',options.workDir)
    runDir = re.sub('^'+wDirString+'/','',curDir)

# check maxCpuCount 
if options.maxCpuCount > Client.maxCpuCountLimit:
    tmpLog.error("too large maxCpuCount. Must be less than %s" % Client.maxCpuCountLimit)
    sys.exit(EC_Config)

# create tmp dir
if options.tmpDir == '':
    tmpDir = '%s/%s' % (curDir,MiscUtils.wrappedUuidGen())
else:
    tmpDir = '%s/%s' % (os.path.abspath(options.tmpDir),MiscUtils.wrappedUuidGen())    
os.makedirs(tmpDir)

# exit action
def _onExit(dir,files):
    for tmpFile in files:
        commands.getoutput('rm -rf %s' % tmpFile)        
    commands.getoutput('rm -rf %s' % dir)
atexit.register(_onExit,tmpDir,delFilesOnExit)

# parse tag
athenaVer = ''
cacheVer  = ''
nightVer  = ''
groupArea = ''
cmtConfig = ''
if options.useAthenaPackages:
    # get Athena versions
    stA,retA = AthenaUtils.getAthenaVer()
    # failed
    if not stA:
        tmpLog.error("You need to setup Athena runtime to use --useAthenaPackages")
        sys.exit(EC_CMT)
    workArea  = retA['workArea'] 
    athenaVer = 'Atlas-%s' % retA['athenaVer'] 
    groupArea = retA['groupArea'] 
    cacheVer  = retA['cacheVer'] 
    nightVer  = retA['nightVer']
    cmtConfig = retA['cmtConfig']    
    # override run directory
    sString=re.sub('[\+]','.',workArea)
    runDir = re.sub('^%s' % sString, '', curDir)
    if runDir == curDir:
        errMsg  = "You need to run prun in a directory under %s. " % workArea
        errMsg += "If '%s' is a read-only directory, perhaps you did setup Athena without --testarea or the 'here' tag of asetup." % workArea
        tmpLog.error(errMsg)        
        sys.exit(EC_Config)
    elif runDir == '':
        runDir = '.'
    elif runDir.startswith('/'):
        runDir = runDir[1:]
    runDir = runDir+'/'
elif options.athenaTag != '':
    # get list of Athena projects
    listProjects = Client.getCachePrefixes(options.verbose)
    items = options.athenaTag.split(',')
    usingNightlies = False
    for item in items:
        # releases
        match = re.search('^(\d+\.\d+\.\d+)',item)
        if match != None:
            athenaVer = 'Atlas-%s' % match.group(1)
            # cache
	    cmatch = re.search('^(\d+\.\d+\.\d+\.\d+\.*\d*)$',item)
	    if cmatch != None:
		cacheVer += '_%s' % cmatch.group(1)
        else:
            # nightlies
            match = re.search('^(\d+\.\d+\.X|\d+\.X\.\d+)$',item)
            if match != None:
                athenaVer = 'Atlas-%s' % match.group(1)
        # project
        if item.startswith('Atlas') or item in listProjects:
            # ignore AtlasOffline
            if item in ['AtlasOffline']:
                continue
            cacheVer = '-'+item+cacheVer
        # nightlies    
        if item.startswith('rel_'):
            usingNightlies = True
            if 'dev' in items:
                athenaVer = 'Atlas-dev'
            elif 'devval' in items:
                athenaVer = 'Atlas-devval'
            cacheVer  = '-AtlasOffline_%s' % item
	# CMTCONFIG
        if item in ['32','64']:
            tmpLog.warning("%s in --athenaTag is unsupported. Please use --cmtConfig instead" % item)
    # check cache
    if re.search('^-.+_.+$',cacheVer) == None:
        if re.search('^_\d+\.\d+\.\d+\.\d+$',cacheVer) != None:
            # use AtlasProduction
            cacheVer = '-AtlasProduction'+cacheVer
        elif 'AthAnalysisBase' in cacheVer or 'AthAnalysis' in cacheVer:
            # AthAnalysis
            cacheVer  = cacheVer + '_%s' % athenaVer
            athenaVer = ''
        else:
            # unknown
            cacheVer = ''
    # use dev nightlies
    if usingNightlies and athenaVer == '':
        athenaVer = 'Atlas-dev'

# set CMTCONFIG
options.cmtConfig = AthenaUtils.getCmtConfig(athenaVer,cacheVer,nightVer,options.cmtConfig)

# set mana version
if options.useMana and options.manaVer != '': 
    if options.manaVer != '':
        tmpLog.info("checking manaVer=%s on CVMFS" % options.manaVer)
        sMana,oMana,options.manaVer,options.cmtConfig = MiscUtils.checkManaVersion(options.manaVer,options.cmtConfig)
        if not sMana:
            tmpLog.error(oMana)
            sys.exit(EC_Config)
        tmpLog.info("full manaVer=%s cmtConfig=%s" % (options.manaVer,options.cmtConfig))

# check CMTCONFIG
if not AthenaUtils.checkCmtConfig(cmtConfig,options.cmtConfig,options.nobuild):
    sys.exit(EC_Config)
    
# event picking
if options.eventPickEvtList != '':
    epLockedBy = 'prun'
    if not options.nosubmit:
        # request event picking
        epStat,epOutput = Client.requestEventPicking(options.eventPickEvtList,
                                                     options.eventPickDataType,
                                                     options.eventPickStreamName,
                                                     options.eventPickDS,
                                                     options.eventPickAmiTag,
                                                     [],
                                                     options.inputFileListName,
                                                     options.outDS,
                                                     epLockedBy,
                                                     fullExecString,
                                                     options.eventPickNumSites,
                                                     options.eventPickWithGUID,
                                                     options.verbose)
        # set input dataset 
        options.inDS = epOutput
    else:
        options.inDS = 'dummy'
    tmpLog.info('requested Event Picking service to stage input as %s' % options.inDS)

# additinal files
if options.extFile == '':
    options.extFile = []
else:
    tmpItems = options.extFile.split(',')
    options.extFile = []
    # convert * to .*
    for tmpItem in tmpItems:
        options.extFile.append(tmpItem.replace('*','.*'))

# user-specified merging script
if options.mergeScript != '':
    # enable merging
    options.mergeOutput = True
    # add it to extFile
    if not options.mergeScript in options.extFile:
        options.extFile.append(options.mergeScript)

# glue packages
options.gluePackages = options.gluePackages.split(',')
try:
    options.gluePackages.remove('')
except:
    pass

# set excludeFile
AthenaUtils.setExcludeFile(options.excludeFile)

# LFN matching
if options.match != '':
    # convert . to \.
    options.match = options.match.replace('.','\.')
    # convert * to .*
    options.match = options.match.replace('*','.*')

# LFN anti-matching
if options.antiMatch != '':
    # convert . to \.
    options.antiMatch = options.antiMatch.replace('.','\.')
    # convert * to .*
    options.antiMatch = options.antiMatch.replace('*','.*')

# get job script
jobScript = ''
if options.jobParams == '':
    tmpLog.error("you need to give --exec\n  prun [--inDS inputdataset] --outDS outputdataset --exec 'myScript arg1 arg2 ...'")
    sys.exit(EC_Config)
orig_execStr  = options.jobParams
orig_bexecStr = options.bexec

# replace : to = for backward compatibility
for optArg in ['RNDM']:
    options.jobParams = re.sub('%'+optArg+':','%'+optArg+'=',options.jobParams)

# check output dataset
if options.outDS == '':
    tmpLog.error("no outDS is given\n  prun [--inDS inputdataset] --outDS outputdataset --exec 'myScript arg1 arg2 ...'")
    sys.exit(EC_Config)

# avoid inDS+pfnList
if options.pfnList != '':
    # don't use inDS
    if options.inDS != '':
	tmpLog.error("--pfnList and --inDS cannot be used at the same time")
	sys.exit(EC_Config)
    # use site    
    if options.site == 'AUTO':
	tmpLog.error("--site must be specified when --pfnList is used")
	sys.exit(EC_Config)
        
# secondary datasets
if options.secondaryDSs != '':
    # parse
    tmpMap = {}
    for tmpItem in options.secondaryDSs.split(','):
        tmpItems = tmpItem.split(':')
        if len(tmpItems) in [3,4,5]:
            tmpDsName = tmpItems[2]
            # change ^ to ,
            tmpDsName = tmpDsName.replace('^',',')
            # make map
            tmpMap[tmpDsName] = {'nFiles'     : int(tmpItems[1]),
                                 'streamName' : tmpItems[0],
                                 'pattern'    : '',
                                 'nSkip'      : 0,
                                 'files'      : []}
            # using filtering pattern
            if len(tmpItems) >= 4:
                tmpMap[tmpItems[2]]['pattern'] = tmpItems[3]
            # nSkip
            if len(tmpItems) >= 5:
                tmpMap[tmpItems[2]]['nSkip'] = int(tmpItems[4])
        else:         
            tmpLog.error("Wrong format %s in --secondaryDSs. Must be StreamName:nFilesPerJob:DatasetName[:Pattern[:nSkipFiles]]" \
                         % tmpItem)
            sys.exit(EC_Config)
    # set
    options.secondaryDSs = tmpMap
else:
    options.secondaryDSs = {}

# reusable secondary streams
if options.reusableSecondary == '':
    options.reusableSecondary = []
else:
    options.reusableSecondary = options.reusableSecondary.split(',')

# correct site
if options.site != 'AUTO':
    origSite = options.site
    # patch for BNL
    if options.site in ['BNL',"ANALY_BNL"]:
        options.site = "ANALY_BNL_SHORT"
    # patch for CERN
    if options.site in ['CERN']:
        options.site = "ANALY_CERN_XROOTD"        
    # try to convert DQ2ID to PandaID
    pID = PsubUtils.convertDQ2toPandaID(options.site)
    if pID != '':
        options.site = pID
    # add ANALY
    if not options.site.startswith('ANALY_'):
        options.site = 'ANALY_%s' % options.site
    # check
    if not Client.PandaSites.has_key(options.site):
        tmpLog.error("unknown siteID:%s" % origSite)
        sys.exit(EC_Config)
    # add logging info
    userBrokerLogs = PsubUtils.getUserBrokerageInfo(options.site,'site',userBrokerLogs)
    # set cloud
    options.cloud = Client.PandaSites[options.site]['cloud']

        
# get DN
distinguishedName = PsubUtils.getDN()

# get nickname
nickName = PsubUtils.getNickname()

if nickName == '':
    sys.exit(EC_Config)

# set Rucio accounting
PsubUtils.setRucioAccount(nickName,'prun',True)

# check outDS format
if not PsubUtils.checkOutDsName(options.outDS,distinguishedName,options.official,nickName,
                                options.site,vomsFQAN,options.mergeOutput):
    tmpLog.error("invalid output datasetname:%s" % options.outDS)
    sys.exit(EC_Config)

# check nGBPerJob
if not options.nGBPerJob in [-1,'MAX']:
    # convert to int
    try:
        if options.nGBPerJob != 'MAX':
            options.nGBPerJob = int(options.nGBPerJob)
    except:
        tmpLog.error("nGBPerJob must be an integer or MAX")
        sys.exit(EC_Config)
    # check negative    
    if options.nGBPerJob <= 0:
        tmpLog.error("nGBPerJob must be positive")
        sys.exit(EC_Config)




#####################################################################
# archive sources and send it to HTTP-reachable location

# create archive
if options.panda_srcName != '':
    # reuse src
    if options.verbose:    
        tmpLog.debug('reuse source files')
    archiveName = options.panda_srcName
    # go to tmp dir
    os.chdir(tmpDir)
else:
    if options.inTarBall == '':
        # copy RootCore packages 
        if options.useRootCore:
            # check $ROOTCOREDIR
            if not os.environ.has_key('ROOTCOREDIR'):
                tmpErrMsg  = '$ROOTCOREDIR is not definied in your enviornment. '
                tmpErrMsg += 'Please setup RootCore runtime beforehand'
                tmpLog.error(tmpErrMsg)
                sys.exit(EC_Config)
            # check grid_submit.sh
            rootCoreSubmitSh    = os.environ['ROOTCOREDIR'] + '/scripts/grid_submit.sh'
            rootCoreCompileSh   = os.environ['ROOTCOREDIR'] + '/scripts/grid_compile.sh'
            rootCoreRunSh       = os.environ['ROOTCOREDIR'] + '/scripts/grid_run.sh'
            rootCoreSubmitNbSh  = os.environ['ROOTCOREDIR'] + '/scripts/grid_submit_nobuild.sh'
            rootCoreCompileNbSh = os.environ['ROOTCOREDIR'] + '/scripts/grid_compile_nobuild.sh'
            rootCoreShList = [rootCoreSubmitSh,rootCoreCompileSh,rootCoreRunSh]
            if options.nobuild:
                rootCoreShList.append(rootCoreSubmitNbSh)
                if options.noCompile:
                    rootCoreShList.append(rootCoreCompileNbSh)
            for tmpShFile in rootCoreShList:
                if not os.path.exists(tmpShFile):
                    tmpErrMsg  = "%s doesn't exist. Please use a newer version of RootCore" % tmpShFile
                    tmpLog.error(tmpErrMsg)
                    sys.exit(EC_Config)
	    tmpLog.info("copy RootCore packages to current dir")
            # destination
	    pandaRootCoreWorkDirName = '__panda_rootCoreWorkDir'
            rootCoreDestWorkDir = curDir + '/' + pandaRootCoreWorkDirName
            # add all files to extFile
            options.extFile.append(pandaRootCoreWorkDirName + '/.*')
            # add to be deleted on exit
            delFilesOnExit.append(rootCoreDestWorkDir)
            if not options.nobuild:
                tmpStat = os.system('%s %s' % (rootCoreSubmitSh,rootCoreDestWorkDir))
            else:
                tmpStat = os.system('%s %s' % (rootCoreSubmitNbSh,rootCoreDestWorkDir))                
            tmpStat %= 255
            if tmpStat != 0:
                tmpErrMsg  = "%s failed with %s" % (rootCoreSubmitSh,tmpStat)
                tmpLog.error(tmpErrMsg)
                sys.exit(EC_Config)
            # copy build and run scripts
            shutil.copy(rootCoreRunSh,rootCoreDestWorkDir)            
            shutil.copy(rootCoreCompileSh,rootCoreDestWorkDir)
            if options.noCompile:
                shutil.copy(rootCoreCompileNbSh,rootCoreDestWorkDir)
        # gather Athena packages
	if options.useAthenaPackages:
	    archiveName = ""
	    if AthenaUtils.useCMake():
		# archive with cpack
		archiveName,archiveFullName = AthenaUtils.archiveWithCpack(True,tmpDir,options.verbose)
	    # set extFile
	    AthenaUtils.setExtFile(options.extFile)
	    if options.libDS == '' and not options.nobuild:
		# archive sources
		archiveName,archiveFullName = AthenaUtils.archiveSourceFiles(workArea,runDir,curDir,tmpDir,
                                                                             options.verbose,options.gluePackages,
                                                                             dereferenceSymLinks=options.followLinks,
                                                                             archiveName=archiveName)
	    else:
		# archive jobO
		archiveName,archiveFullName = AthenaUtils.archiveJobOFiles(workArea,runDir,curDir,
									   tmpDir,options.verbose,
                                                                           archiveName=archiveName)
	    # archive InstallArea
	    if options.libDS == '':
		AthenaUtils.archiveInstallArea(workArea,groupArea,archiveName,archiveFullName,
					       tmpDir,options.nobuild,options.verbose)
        # gather normal files
        if True:
            if options.useAthenaPackages:
                # go to workArea
                os.chdir(workArea)
                # gather files under work dir
                tmpLog.info("gathering files under %s/%s" % (workArea,runDir))
                archStartDir = runDir
                archStartDir = re.sub('/+$','',archStartDir)
            else:
                # go to workdir
                os.chdir(options.workDir)
                # gather files under work dir
                tmpLog.info("gathering files under %s" % options.workDir)
                archStartDir = '.'
	    # get files in the working dir
            if options.noCompile:
                skippedExt = []
            else:
                skippedExt   = ['.o','.a','.so']
	    skippedFlag  = False
	    workDirFiles = []
            if options.followLinks:
                osWalkList = os.walk(archStartDir,followlinks=True)
            else:
                osWalkList = os.walk(archStartDir)                
	    for tmpRoot,tmpDirs,tmpFiles in osWalkList:
		emptyFlag    = True
		for tmpFile in tmpFiles:
                    if options.useAthenaPackages:
                        if os.path.basename(tmpFile) == os.path.basename(archiveFullName):
                            if options.verbose:
                                print 'skip Athena archive %s' % tmpFile
                            continue
		    tmpPath = '%s/%s' % (tmpRoot,tmpFile)
		    # get size
		    try:
			size = os.path.getsize(tmpPath)
		    except:
			# skip dead symlink
			if options.verbose:
			    type,value,traceBack = sys.exc_info()
			    print "  Ignore : %s:%s" % (type,value)
			continue
		    # check exclude files
		    excludeFileFlag = False
		    for tmpPatt in AthenaUtils.excludeFile:
			if re.search(tmpPatt,tmpPath) != None:
			    excludeFileFlag = True
			    break
		    if excludeFileFlag:
			continue
		    # skipped extension
		    isSkippedExt = False
		    for tmpExt in skippedExt:
			if tmpPath.endswith(tmpExt):
			    isSkippedExt = True
			    break
		    # check root
		    isRoot = False
		    if re.search('\.root(\.\d+)*$',tmpPath) != None:
			isRoot = True
		    # extra files
		    isExtra = False
		    for tmpExt in options.extFile:
			if re.search(tmpExt+'$',tmpPath) != None:
			    isExtra = True
			    break
		    # regular files
		    if not isExtra:
			# unset emptyFlag even if all files are skipped
			emptyFlag = False
			# skipped extensions
			if isSkippedExt:
			    print "  skip %s %s" % (str(skippedExt),tmpPath)
			    skippedFlag = True
			    continue
			# skip root
			if isRoot:
			    print "  skip root file %s" % tmpPath
			    skippedFlag = True
			    continue
			# check size
			if size > options.maxFileSize:
			    print "  skip large file %s:%sB>%sB" % (tmpPath,size,options.maxFileSize)
			    skippedFlag = True
			    continue
		    # remove ./
		    tmpPath = re.sub('^\./','',tmpPath)
		    # append
		    workDirFiles.append(tmpPath)
		    emptyFlag = False
		# add empty directory		
		if emptyFlag and tmpDirs==[] and tmpFiles==[]:
		    tmpPath = re.sub('^\./','',tmpRoot)
		    # check exclude pattern
		    excludePatFlag = False
		    for tmpPatt in AthenaUtils.excludeFile:
			if re.search(tmpPatt,tmpPath) != None:
			    excludePatFlag = True
			    break
		    if excludePatFlag:
			continue
		    # skip tmpDir
		    if tmpPath.split('/')[-1] == tmpDir.split('/')[-1]:
			continue
		    # append
		    workDirFiles.append(tmpPath)
	    if skippedFlag:
		tmpLog.info("please use --maxFileSize or --extFile if you need to send the skipped files to WNs")
            # set archive name
            if not options.useAthenaPackages:    
                # create archive
                if options.libDS != '' or (options.nobuild and not options.noCompile):
                    # use 'jobO' for libDS/noBuild
                    archiveName     = 'jobO.%s.tar' % MiscUtils.wrappedUuidGen()
                else:
                    # use 'sources' for normal build
                    archiveName     = 'sources.%s.tar' % MiscUtils.wrappedUuidGen()
                archiveFullName = "%s/%s" % (tmpDir,archiveName)
	    # collect files
	    for tmpFile in workDirFiles:
                # avoid self-archiving 
                if os.path.basename(tmpFile) == os.path.basename(archiveFullName):
                    if options.verbose:
                        print 'skip self-archiving for %s' % tmpFile
                    continue
		if os.path.islink(tmpFile):
		    status,out = commands.getstatusoutput("tar --exclude '.[a-zA-Z]*' -uh '%s' -f '%s'" % (tmpFile,archiveFullName))
		else:
		    status,out = commands.getstatusoutput("tar --exclude '.[a-zA-Z]*' -uf '%s' '%s'" % (archiveFullName,tmpFile))
		if options.verbose:
		    print tmpFile
		if status != 0 or out != '':
		    print out
	# go to tmpdir
	os.chdir(tmpDir)

	# compress
	status,out = commands.getstatusoutput('gzip %s' % archiveName)
	archiveName += '.gz'
	if status !=0 or options.verbose:
	    print out

	# check archive
	status,out = commands.getstatusoutput('ls -l %s' % archiveName)
	if options.verbose:
	    print out
	if status != 0:
	    tmpLog.error("Failed to archive working area.\n        If you see 'Disk quota exceeded', try '--tmpDir /tmp'") 
	    sys.exit(EC_Archive)

	# check symlinks
	if options.useAthenaPackages:
	    tmpLog.info("checking symbolic links")
	    status,out = commands.getstatusoutput('tar tvfz %s' % archiveName)
	    if status != 0:
		tmpLog.error("Failed to expand archive")
		sys.exit(EC_Archive)
	    symlinks = []    
	    for line in out.split('\n'):
		items = line.split()
		if items[0].startswith('l') and items[-1].startswith('/'):
		    symlinks.append(line)
	    if symlinks != []:
		tmpStr  = "Found some unresolved symlinks which may cause a problem\n"
		tmpStr += "     See, e.g., http://savannah.cern.ch/bugs/?43885\n"
		tmpStr += "   Please ignore if you believe they are harmless"
		tmpLog.warning(tmpStr)
		for symlink in symlinks:
		    print "  %s" % symlink
    else:
        # go to tmp dir
        os.chdir(tmpDir)
        # use a saved copy
        if options.libDS == '' and (options.noCompile or not options.nobuild):
            archiveName     = 'sources.%s.tar' % MiscUtils.wrappedUuidGen()
            archiveFullName = "%s/%s" % (tmpDir,archiveName)
        else:
            archiveName     = 'jobO.%s.tar' % MiscUtils.wrappedUuidGen()
            archiveFullName = "%s/%s" % (tmpDir,archiveName)
        # make copy to avoid name duplication
        shutil.copy(options.inTarBall,archiveFullName)

    # save
    if options.outTarBall != '':
        shutil.copy(archiveName,options.outTarBall)
    
    # upload source files
    if not options.nosubmit:
        # upload sources via HTTP POST
        tmpLog.info("upload source files")
        status,out = Client.putFile(archiveName,options.verbose,useCacheSrv=True,reuseSandbox=True)
        if out.startswith('NewFileName:'):
            # found the same input sandbox to reuse
            archiveName = out.split(':')[-1]
        elif out != 'True':
            print out
            tmpLog.error("failed to upload source files with %s" % status)
            sys.exit(EC_Post)
        # good run list
        if options.goodRunListXML != '':
            options.goodRunListXML = PsubUtils.uploadGzippedFile(options.goodRunListXML,curDir,tmpLog,delFilesOnExit,
                                                                 options.nosubmit,options.verbose)


# look for pandatools package
for path in sys.path:
    if path == '':
        path = curDir
    if os.path.exists(path) and os.path.isdir(path) and 'pandatools' in os.listdir(path):
        # make symlink for module name.
        os.symlink('%s/pandatools' % path,'taskbuffer')
        break

# append tmpdir to import taskbuffer module
sys.path = [tmpDir]+sys.path
from taskbuffer.JobSpec  import JobSpec
from taskbuffer.FileSpec import FileSpec

# special handling
specialHandling = ''
if options.express:
    specialHandling += 'express,'
if options.debugMode:
    specialHandling += 'debug,'
specialHandling = specialHandling[:-1]
    
if options.verbose:
    print "== parameters =="
    print "Site       : %s" % options.site
    print "Athena     : %s" % athenaVer
    if groupArea != '':
        print "Group Area : %s" % groupArea
    if cacheVer != '':
        print "Cache      : %s" % cacheVer[1:]
    if nightVer != '':
        print "Nightly    : %s" % nightVer[1:]
    print "RunDir     : %s" % runDir
    print "exec       : %s" % options.jobParams




#####################################################################
# task making

# job name
jobName = 'prun.%s' % MiscUtils.wrappedUuidGen()

# make task
taskParamMap = {}
taskParamMap['taskName'] = options.outDS
if not options.allowTaskDuplication:
    taskParamMap['uniqueTaskName'] = True
taskParamMap['vo'] = 'atlas'
taskParamMap['architecture'] = options.cmtConfig
taskParamMap['transUses'] = athenaVer
if athenaVer != '':
    taskParamMap['transHome'] = 'AnalysisTransforms'+cacheVer+nightVer
else:
    taskParamMap['transHome'] = None
taskParamMap['processingType'] = 'panda-client-{0}-jedi-run'.format(PandaToolsPkgInfo.release_version)
if options.eventPickEvtList != '':
    taskParamMap['processingType'] += '-evp'
    taskParamMap['waitInput'] = 1
if options.goodRunListXML != '':
    taskParamMap['processingType'] += '-grl'
if options.prodSourceLabel == '':    
    taskParamMap['prodSourceLabel'] = 'user'
else:
    taskParamMap['prodSourceLabel'] = options.prodSourceLabel
if options.site != 'AUTO':
    taskParamMap['site'] = options.site
else:
    taskParamMap['site'] = None
taskParamMap['cloud'] = options.cloud
taskParamMap['excludedSite'] = options.excludedSite
if includedSite != None and includedSite != []:
    taskParamMap['includedSite'] = includedSite
else:
    taskParamMap['includedSite'] = None
if options.nFiles != 0:
    taskParamMap['nFiles'] = options.nFiles
if options.nFilesPerJob != None:    
    taskParamMap['nFilesPerJob'] = options.nFilesPerJob
if not options.nGBPerJob in [-1,'MAX']:
    # don't set MAX since it is the defalt on the server side
    taskParamMap['nGBPerJob'] = options.nGBPerJob
taskParamMap['cliParams'] = fullExecString
if options.noEmail:
    taskParamMap['noEmail'] = True
if options.skipScout:
    taskParamMap['skipScout'] = True
if options.respectSplitRule:
    taskParamMap['respectSplitRule'] = True
if options.disableAutoRetry:
    taskParamMap['disableAutoRetry'] = 1
if options.workingGroup != None:
    taskParamMap['workingGroup'] = options.workingGroup
if options.official:
    taskParamMap['official'] = True
taskParamMap['nMaxFilesPerJob'] = options.maxNFilesPerJob
if options.useNewCode:
    taskParamMap['fixedSandbox'] = archiveName
if options.useRucio:
    taskParamMap['ddmBackEnd'] = 'rucio'
if options.maxCpuCount > 0:
    taskParamMap['walltime'] = -options.maxCpuCount
if options.memory > 0:
    taskParamMap['ramCount'] = options.memory
if options.skipFilesUsedBy != '':
    taskParamMap['skipFilesUsedBy'] = options.skipFilesUsedBy
# source URL
matchURL = re.search("(http.*://[^/]+)/",Client.baseURLCSRVSSL)
if matchURL != None:
    taskParamMap['sourceURL'] = matchURL.group(1)
# XML config
if options.loadXML != None:
    taskParamMap['loadXML'] = options.loadXML
# middle name
if options.addNthFieldOfInFileToLFN != '':
    taskParamMap['addNthFieldToLFN'] = options.addNthFieldOfInFileToLFN
    taskParamMap['useFileAsSourceLFN'] = True
elif options.addNthFieldOfInDSToLFN != '':
    taskParamMap['addNthFieldToLFN'] = options.addNthFieldOfInDSToLFN
# dataset names
outDatasetName = options.outDS
logDatasetName = re.sub('/$','.log/',options.outDS)
# log
taskParamMap['log'] = {'dataset': logDatasetName,
                       'container': logDatasetName,
                       'type':'template',
                       'param_type':'log',
                       'value':'{0}.$JEDITASKID.${{SN}}.log.tgz'.format(logDatasetName[:-1])
                       }
if options.addNthFieldOfInFileToLFN != '':
    loglfn  = '{0}.{1}'.format(*logDatasetName.split('.')[:2])
    loglfn += '${MIDDLENAME}.$JEDITASKID._${SN}.log.tgz'
    taskParamMap['log']['value'] = loglfn
if options.spaceToken != '':
    taskParamMap['log']['token'] = options.spaceToken

# job parametes
taskParamMap['jobParameters'] = [
    {'type':'constant',
     'value': '-j "" --sourceURL ${SURL}',
     },
    {'type':'constant',
     'value': '-r {0}'.format(runDir),
     },
    ]
if options.loadXML == None:
    taskParamMap['jobParameters'] += [
        {'type':'constant',
         'value': '-p "',
         'padding':False,
         },
        ]
    taskParamMap['jobParameters'] += PsubUtils.convertParamStrToJediParam(options.jobParams,{},'',
                                                                          True,False,
                                                                          includeIO=False)
    taskParamMap['jobParameters'] += [
        {'type':'constant',
         'value': '"',
         },
        ]
else:
    taskParamMap['jobParameters'] += [
        {'type':'constant',
         'value': '-p "{0}"'.format(options.jobParams),
         },
        ]


# build
if options.nobuild and not options.noCompile:
    taskParamMap['jobParameters'] += [
        {'type':'constant',
         'value': '-a {0}'.format(archiveName),
         },
        ]
else:
    taskParamMap['jobParameters'] += [
        {'type':'constant',
         'value': '-l ${LIB}',
         },
        ]
# output
if options.outputs != '':
    outMap = {}
    dsSuffix = []
    for tmpLFN in options.outputs.split(','):
        tmpDsSuffix = ''
        if ':' in tmpLFN:
            tmpDsSuffix,tmpLFN = tmpLFN.split(':')
            if tmpDsSuffix in dsSuffix:
                tmpErrMsg  = "dataset name suffix '%s' is used for multiple files in --outputs. " % tmpDsSuffix
                tmpErrMsg += 'each output must have a unique suffix.' 
                tmpLog.error(tmpErrMsg)
                sys.exit(EC_Config)
            dsSuffix.append(tmpDsSuffix)
        tmpNewLFN = tmpLFN
	# change * to XYZ and add .tgz
	if '*' in tmpNewLFN:
	    tmpNewLFN = tmpNewLFN.replace('*','XYZ')
	    tmpNewLFN += '.tgz'
        if len(outDatasetName.split('.')) > 2:
            lfn = '{0}.{1}'.format(*outDatasetName.split('.')[:2])
        else:
            lfn = outDatasetName[:-1]
        if options.addNthFieldOfInDSToLFN != '' or options.addNthFieldOfInFileToLFN != '':
            lfn += '${MIDDLENAME}'
        lfn += '.$JEDITASKID._${{SN/P}}.{0}'.format(tmpNewLFN)
        if tmpDsSuffix == '':
            tmpDsSuffix = tmpNewLFN
        dataset = '{0}_{1}/'.format(outDatasetName[:-1],tmpDsSuffix)
        taskParamMap['jobParameters'] += MiscUtils.makeJediJobParam(lfn,dataset,'output',hidden=True,
                                                                    destination=options.destSE,
                                                                    token=options.spaceToken,
                                                                    allowNoOutput=options.allowNoOutput)
        outMap[tmpLFN] = lfn
    if options.loadXML:
        taskParamMap['jobParameters'] += [ 
            {'type':'constant',
             'value': '-o "${XML_OUTMAP}"',
             },
            ]
    else:
        taskParamMap['jobParameters'] += [ 
            {'type':'constant',
             'value': '-o "{0}"'.format(str(outMap)),
             },
            ]
# input
if options.inDS != '': 
    tmpDict = {'type':'template',
               'param_type':'input',
               'value':'-i "${IN/T}"',
               'dataset':options.inDS,
               'exclude':'\.log\.tgz(\.\d+)*$',
               }
    if options.useLogAsInput:
        del tmpDict['exclude']
    if options.loadXML == None:
        tmpDict['expand'] = True
    if options.nSkipFiles != 0:
        tmpDict['offset'] = options.nSkipFiles
    if options.match != '':
        tmpDict['include'] = options.match
    if options.antiMatch != '':
        if 'exclude' in tmpDict:
            tmpDict['exclude'] += ','+options.antiMatch
        else:
            tmpDict['exclude'] = options.antiMatch
    if filesToBeUsed != []:
        tmpDict['files'] = filesToBeUsed
    taskParamMap['jobParameters'].append(tmpDict)
    taskParamMap['dsForIN'] = options.inDS
elif options.pfnList != '':
    taskParamMap['pfnList'] = PsubUtils.getListPFN(options.pfnList)
    # use noInput mecahism
    taskParamMap['noInput'] = True
    if options.nFiles == 0:
        taskParamMap['nFiles'] = len(taskParamMap['pfnList'])
    taskParamMap['jobParameters'] += [
        {'type':'constant',
         'value':'-i "${IN/T}"',
         },
        ]
elif options.goodRunListXML != '':
    tmpDict = {'type':'template',
               'param_type':'input',
               'value':'-i "${IN/T}"',
               'dataset':'%%INDS%%',
	       'expand':True,
               'exclude':'\.log\.tgz(\.\d+)*$',
               'files':'%%INLFNLIST%%',
               }
    taskParamMap['jobParameters'].append(tmpDict)
    taskParamMap['dsForIN'] = '%%INDS%%'
else:
    # no input
    taskParamMap['noInput'] = True
    if options.nEvents > 0:
	taskParamMap['nEvents'] = options.nEvents
	if options.nJobs > 0:
	    taskParamMap['nEventsPerJob'] = options.nEvents / options.nJobs
        else:
            # set granularity
            if options.nEventsPerChunk > 0:
                taskParamMap['nEventsPerRange'] = options.nEventsPerChunk
            else:
                # use 1/20 by default
                taskParamMap['nEventsPerRange'] = options.nEvents / 20
                if taskParamMap['nEventsPerRange'] <= 0:
                    taskParamMap['nEventsPerRange'] = 1
    else:
        if options.nJobs > 0:
            taskParamMap['nEvents'] = options.nJobs
        else:
            taskParamMap['nEvents'] = 1
        taskParamMap['nEventsPerJob'] = 1

# param for DBR     
if options.dbRelease != '':
    dbrDS = options.dbRelease.split(':')[0]
    # change LATEST to DBR_LATEST
    if dbrDS == 'LATEST':
        dbrDS = 'DBR_LATEST'
    dictItem = {'type':'template',
                'param_type':'input',
                'value':'--dbrFile=${DBR}',
                'dataset':dbrDS,
                }
    taskParamMap['jobParameters'] += [dictItem]
    # no expansion
    if options.notExpandDBR:
        dictItem = {'type':'constant',
                    'value':'--noExpandDBR',
                    }
        taskParamMap['jobParameters'] += [dictItem]

# secondary
if options.secondaryDSs != {}:
    inMap = {'IN':'tmp_IN'}
    streamNames = ['IN']
    for tmpDsName,tmpMap in options.secondaryDSs.iteritems():
        # make template item
        streamName = tmpMap['streamName']
        if options.loadXML == None:
            expandFlag = True
        else:
            expandFlag = False
        # re-usability
        reusableAtt = False
        if streamName in options.reusableSecondary:
            reusableAtt = True
        dictItem = MiscUtils.makeJediJobParam('${'+streamName+'}',tmpDsName,'input',hidden=True,
                                              expand=expandFlag,include=tmpMap['pattern'],offset=tmpMap['nSkip'],
                                              nFilesPerJob=tmpMap['nFiles'],reusableAtt=reusableAtt)
        taskParamMap['jobParameters'] += dictItem
        inMap[streamName] = 'tmp_'+streamName 
        streamNames.append(streamName)
    # make constant item
    strInMap = str(inMap)
    # set placeholders
    for streamName in streamNames:
        strInMap = strInMap.replace("'tmp_"+streamName+"'",'${'+streamName+'/T}')
    dictItem = {'type':'constant',
                'value':'--inMap "%s"' % strInMap,
                }
    taskParamMap['jobParameters'] += [dictItem]

# misc
jobParameters = ''
# given PFN 
if options.pfnList != '':
    jobParameters += '--givenPFN '
# use Athena packages
if options.useAthenaPackages:
    jobParameters += "--useAthenaPackages "
# use CMake
if AthenaUtils.useCMake():
    jobParameters += "--useCMake "
# use RootCore
if options.useRootCore:
    jobParameters += "--useRootCore "
# use mana
if options.useMana:
    jobParameters += "--useMana "
    if options.manaVer != '':
        jobParameters += "--manaVer %s " % options.manaVer
# root
if options.rootVer != '':
    jobParameters += "--rootVer %s " % options.rootVer
# write input to txt
if options.writeInputToTxt != '':
    jobParameters += "--writeInputToTxt %s " % options.writeInputToTxt
# debug parameters
if options.queueData != '':
    jobParameters += "--overwriteQueuedata=%s " % options.queueData
# JEM
if options.enableJEM:
    jobParameters += "--enable-jem "
    if options.configJEM != '':
        jobParameters += "--jem-config %s " % options.configJEM
# set task param
if jobParameters != '':
    taskParamMap['jobParameters'] += [ 
        {'type':'constant',
         'value': jobParameters,
         },
        ]

# force stage-in
if options.forceStaged or options.forceStagedSecondary:
    taskParamMap['useLocalIO'] = 1

# build step
if options.nobuild and not options.noCompile:
    pass
else:
    jobParameters = '-i ${IN} -o ${OUT} --sourceURL ${SURL} '
    jobParameters += '-r {0} '.format(runDir)
    # exec
    if options.bexec != '':
	jobParameters += '--bexec "{0}" '.format(urllib.quote(options.bexec))
    # use Athena packages
    if options.useAthenaPackages:
        jobParameters += "--useAthenaPackages "
    # use RootCore
    if options.useRootCore:
        jobParameters += "--useRootCore "
    # no compile
    if options.noCompile:
        jobParameters += "--noCompile "
    # use CMake
    if AthenaUtils.useCMake():
        jobParameters += "--useCMake "
    # use mana
    if options.useMana:
        jobParameters += "--useMana "
	if options.manaVer != '':
	    jobParameters += "--manaVer %s " % options.manaVer
    # root
    if options.rootVer != '':
        jobParameters += "--rootVer %s " % options.rootVer
    # cmt config
    if not options.cmtConfig in ['','NULL',None]:
        jobParameters += "--cmtConfig %s " % options.cmtConfig
    # debug parameters
    if options.queueData != '':
        jobParameters += "--overwriteQueuedata=%s " % options.queueData
    # set task param
    taskParamMap['buildSpec'] = {
        'prodSourceLabel':'panda',
        'archiveName':archiveName,
        'jobParameters':jobParameters,
        }
    if options.prodSourceLabel != '':
         taskParamMap['buildSpec']['prodSourceLabel'] = options.prodSourceLabel

# preprocessing step

# good run list
if options.goodRunListXML != '':
    jobParameters = "--goodRunListXML {0} ".format(options.goodRunListXML)
    if options.goodRunDataType != '':
        jobParameters += "--goodRunListDataType {0} ".format(options.goodRunDataType)
    if options.goodRunProdStep != '':
        jobParameters += "--goodRunListProdStep {0} ".format(options.goodRunProdStep)
    if options.goodRunListDS != '':
        jobParameters += "--goodRunListDS {0} ".format(options.goodRunListDS)
    jobParameters += "--sourceURL ${SURL} "
    # set task param
    taskParamMap['preproSpec'] = {
        'prodSourceLabel':'panda',
        'jobParameters':jobParameters,
        }
    if options.prodSourceLabel != '':
         taskParamMap['preproSpec']['prodSourceLabel'] = options.prodSourceLabel


# merging
if options.mergeOutput:
    jobParameters = '-r {0} '.format(runDir)
    if options.mergeScript != '':
        jobParameters += '-j "{0}" '.format(options.mergeScript)
    if options.rootVer != '':
        jobParameters += "--rootVer %s " % options.rootVer
    if options.useRootCore:
        jobParameters += "--useRootCore "
    if not (options.nobuild and not options.noCompile):
        jobParameters += '-l ${LIB} '
    else:
        jobParameters += '-a {0} '.format(archiveName)
        jobParameters += "--sourceURL ${SURL} "
    jobParameters += '${TRN_OUTPUT:OUTPUT} ${TRN_LOG:LOG}'
    taskParamMap['mergeSpec'] = {}
    taskParamMap['mergeSpec']['useLocalIO'] = 1
    taskParamMap['mergeSpec']['jobParameters'] = jobParameters
    taskParamMap['mergeOutput'] = True
    if options.nGBPerMergeJob > 0:
        taskParamMap['nGBPerMergeJob'] = options.nGBPerMergeJob

# check task parameters
PsubUtils.checkTaskParam(taskParamMap,options.unlimitNumOutputs)

if options.verbose:
    tmpLog.debug("==== taskParams ====")
    tmpKeys = taskParamMap.keys()
    tmpKeys.sort()
    for tmpKey in tmpKeys:
        print '%s : %s' % (tmpKey,taskParamMap[tmpKey])



#####################################################################
# submission

if options.nosubmit:
    # no submit 
    pass
else:
    # upload proxy for glexec
    if Client.PandaSites.has_key(options.site):
        # delegation
        delResult = PsubUtils.uploadProxy(options.site,options.myproxy,gridPassPhrase,
                                          Client.PandaClouds[options.cloud]['pilotowners'],
                                          options.verbose)
        if not delResult:
            tmpLog.error("failed to upload proxy")
            sys.exit(EC_MyProxy)
    # submit task
    tmpLog.info("submit")
    status,tmpOut = Client.insertTaskParams(taskParamMap,options.verbose,True)
    # result
    if status != 0:
        tmpLog.error("task submission failed with {0}".format(status))
        sys.exit(EC_Submit)
    if tmpOut[0] in [0,3]:
        tmpLog.info(format(tmpOut[1]))
    else:
        tmpLog.error("task submission failed. {0}".format(tmpOut[1]))
        sys.exit(EC_Submit)

# go back to current dir
os.chdir(curDir)
# succeeded
sys.exit(0)
