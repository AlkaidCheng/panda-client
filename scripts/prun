#!/bin/bash

"exec" "python" "-u" "-Wignore" "$0" "$@"

import re
import os
import sys
import time
import atexit
import datetime
import optparse
import commands
import urllib
import shelve

# default cloud/site
defaultCloud = 'US'
defaultSite  = 'AUTO'

# suffix for shadow dataset
suffixShadow = "_shadow"

# error code
EC_Config    = 10
EC_Dataset   = 40
EC_Post      = 50
EC_Archive   = 60
EC_Split     = 70
EC_MyProxy   = 80

usage = """%prog [options]"""


# command-line parameters
optP = optparse.OptionParser(usage=usage,conflict_handler="resolve")
optP.add_option('--version',action='store_const',const=True,dest='version',default=False,
                help='Displays version')
optP.add_option('--inDS',action='store',dest='inDS',default='',type='string',
                help='Name of an input dataset or dataset container')
optP.add_option('--outDS',action='store',dest='outDS',default='',type='string',
                help='Name of an output dataset. OUTDS will contain all output files')
optP.add_option('--libDS',action='store',dest='libDS',default='',type='string',
                help='Name of a library dataset')
optP.add_option('--site',action='store',dest='site',default=defaultSite,type='string',
                help='Site name where jobs are sent (default:%s' % defaultSite)
optP.add_option('--cloud',action='store',dest='cloud',default=None,type='string',
                help='Cloud where jobs are submitted (default:%s)' % defaultCloud)
optP.add_option('--match',action='store',dest='match',default='',type='string',
                help='Use only files matching with given pattern')
optP.add_option('--official',action='store_const',const=True,dest='official',default=False,
                help='Produce official dataset')
optP.add_option('--nFiles',action='store',dest='nFiles',default=0,type='int',
                help='Use a limited number of files in the input dataset')
optP.add_option('--nFilesPerJob',action='store',dest='nFilesPerJob',default=None,type='int',
                help='Number of files on which each sub-job runs (default 20)')
optP.add_option('--nJobs',action='store',dest='nJobs',default=-1,type='int',
                help='Maximum number of sub-jobs. If the number of input files (N_in) is less than nJobs*nFilesPerJob, only N_in/nFilesPerJob sub-jobs will be instantiated')
optP.add_option('--maxFileSize',action='store',dest='maxFileSize',default=1024*1024,type='int',
                help='Maximum size of files to be sent to WNs (default 1024*1024B)')
optP.add_option('--athenaTag',action='store',dest='athenaTag',default='',type='string',
                help='Tags to setup Athena on remote WNs, e.g., --athenaTag=AtlasProduction,14.2.24.3')
optP.add_option('--workDir',action='store',dest='workDir',default='.',type='string',
                help='All files under WORKDIR will be transfered to WNs (default=./)')
optP.add_option('--extFile',action='store',dest='extFile',default='',
                help='root or large files under WORKDIR are not sent to WNs by default. If you want to send some skipped files, specify their names, e.g., data.root,data.tgz')
optP.add_option('--outputs',action='store',dest='outputs',default='',type='string',
                help='Names of output files. Comma separated. e.g., --outputs out1.dat,out2.txt')
optP.add_option('--noSubmit',action='store_const',const=True,dest='nosubmit',default=False,
                help="Don't submit jobs")
optP.add_option('--tmpDir',action='store',dest='tmpDir',default='',type='string',
                help='Temporary directory where an archive file is created')
optP.add_option('--spaceToken',action='store',dest='spaceToken',default='',type='string',
                help='spacetoken for outputs. e.g., ATLASLOCALGROUPDISK')
optP.add_option('--devSrv',action='store_const',const=True,dest='devSrv',default=False,
                help="Please don't use this option. Only for developers to use the dev panda server")
optP.add_option('--exec',action='store',dest='jobParams',default='',type='string',
                help='execution string. e.g., --exec "./myscript arg1 arg2"')
optP.add_option('--bexec',action='store',dest='bexec',default='',type='string',
                help='execution string for build stage. e.g., --bexec "make"')
optP.add_option('--myproxy',action='store',dest='myproxy',default='pandaprx.usatlas.bnl.gov',type='string',
                help='Name of the myproxy server')
optP.add_option('-v',action='store_const',const=True,dest='verbose',default=False,
                help='Verbose')

# parse options
options,args = optP.parse_args()
if options.verbose:
    print options

# display version
if options.version:
    from pandatools import PandaToolsPkgInfo
    print "Version: %s" % PandaToolsPkgInfo.release_version
    sys.exit(0)

# set dummy CMTSITE
if not os.environ.has_key('CMTSITE'):
    os.environ['CMTSITE'] = ''

from pandatools import Client
from pandatools import PsubUtils
from pandatools import AthenaUtils

# use dev server
if options.devSrv:
    Client.useDevServer()
    
# check grid-proxy
gridPassPhrase,vomsFQAN = PsubUtils.checkGridProxy('',False,options.verbose)

# save current dir
curDir = os.path.realpath(os.getcwd())

# check working dir
options.workDir = os.path.realpath(options.workDir)
if options.workDir != curDir and (not curDir.startswith(options.workDir+'/')):
    print "ERROR : you need to run prun in a directory under %s" % options.workDir
    sys.exit(EC_Config)

# run dir
runDir = '.'
if curDir != options.workDir:
    # remove special characters
    wDirString=re.sub('[\+]','.',options.workDir)
    runDir = re.sub('^'+wDirString+'/','',curDir)

# parse tag
athenaVer = ''
cacheVer  = ''
nightVer  = ''
if options.athenaTag != '':
    items = options.athenaTag.split(',')
    for item in items:
        # releases
        match = re.search('^(\d+\.\d+\.\d+)',item)
        if match != None:
            athenaVer = 'Atlas-%s' % match.group(1)
            # cache
            cacheVer += '_%s' % match
        # project
        if item.startswith('Atlas'):
            # ignore AtlasOffline
            if item in ['AtlasOffline']:
                continue
            cacheVer = '-'+item+cacheVer
        # nightlies    
        if item.startswith('rel_'):
            if 'dev' in items:
                nightVer  = '/dev'
            elif 'bugfix' in items:
                nightVer  = '/bugfix'
            else:
                print "ERROR : unsupported nightly %s" % line
                sys.exit(EC_Config)
    # check cache
    if re.search('^-.+_.+$',cacheVer) == None:
        cacheVer = ''


# additinal files
if options.extFile == '':
    options.extFile = []
else:
    tmpItems = options.extFile.split(',')
    options.extFile = []
    # convert * to .*
    for tmpItem in tmpItems:
        options.extFile.append(tmpItem.replace('*','.*'))


# LFN matching
if options.match != '':
    # convert * to .*
    options.match = options.match.replace('*','.*')

# get job script
jobScript = ''
if options.jobParams == '':
    print "ERROR : you need to give --exec"
    print "   prun [--inDS inputdataset] --outDS outputdataset --exec 'myScript arg1 arg2 ...'"
    sys.exit(EC_Config)


# check output dataset
if options.outDS == '':
    print "ERROR : no outDS is given"
    print "   prun [--inDS inputdataset] --outDS outputdataset --exec 'myScript arg1 arg2 ...'"
    sys.exit(EC_Config)

# correct site
if options.site != 'AUTO':
    origSite = options.site
    # patch for BNL
    if options.site in ['BNL',"ANALY_BNL"]:
        options.site = "ANALY_BNL_ATLAS_1"
    # try to convert DQ2ID to PandaID
    pID = PsubUtils.convertDQ2toPandaID(options.site)
    if pID != '':
        options.site = pID
    # add ANALY
    if not options.site.startswith('ANALY_'):
        options.site = 'ANALY_%s' % options.site
    # check
    if not Client.PandaSites.has_key(options.site):
        print "ERROR : unknown siteID:%s" % origSite
        sys.exit(EC_Config)
    # set cloud
    options.cloud = Client.PandaSites[options.site]['cloud']

# get DN
distinguishedName = PsubUtils.getDN()
if distinguishedName == '':
    sys.exit(EC_Config)

# check outDS format
if not options.official:
    if not PsubUtils.checkOutDsName(options.outDS,distinguishedName):
        print "ERROR : invalid output datasetname:%s" % options.outDS
        sys.exit(EC_Config)

# check if output dataset already exists
outputDSexist = False
tmpDatasets = Client.getDatasets(options.outDS,options.verbose)
if len(tmpDatasets) != 0:
    outputDSexist = True

# check if shadow dataset exists
shadowDSexist = False
tmpDatasets = Client.getDatasets("%s%s" % (options.outDS,suffixShadow),options.verbose)
if len(tmpDatasets) != 0:
    shadowDSexist = True

# set site/cloud if output/lib dataset already exists
if outputDSexist:
    if options.verbose:
        print "get locations for outDS:%s" % options.outDS
    # get locations for outDS    
    outDSlocations = Client.getLocations(options.outDS,[],'',True,options.verbose)
    if outDSlocations == []:
        print "ERROR : cannot find locations for existing output datasete:%s" % options.outDS
        sys.exit(EC_Config)
    # convert DQ2ID to Panda siteID
    fFlag = False
    for outDSlocation in outDSlocations:
        convID = PsubUtils.convertDQ2toPandaID(outDSlocation)
        if convID != '':
            options.site  = convID
            options.cloud = Client.PandaSites[convID]['cloud']
            fFlag = True
            break
    # not found
    if not fFlag:
        print "ERROR : cannot find supported sites for existing output datasete:%s" % options.outDS
        sys.exit(EC_Config)
elif options.libDS != '':
    if options.verbose:
        print "get locations for libDS:%s" % options.libDS
    # get locations for libDS    
    libDSlocations = Client.getLocations(options.libDS,[],'',True,options.verbose)
    if libDSlocations == []:
        print "ERROR : cannot find locations for existing lib datasete:%s" % options.libDS
        sys.exit(EC_Config)
    # convert DQ2ID to Panda siteID
    fFlag = False
    for libDSlocation in libDSlocations:
        convID = PsubUtils.convertDQ2toPandaID(libDSlocation)
        if convID != '':
            options.site  = convID
            options.cloud = Client.PandaSites[convID]['cloud']
            fFlag = True
            break
    # not found
    if not fFlag:
        print "ERROR : cannot find supported sites for existing lib datasete:%s" % options.libDS
        sys.exit(EC_Config)

        
# set cloud for brokerage
expCloudFlag = True    
if options.cloud == None:
    options.cloud = PsubUtils.getCloudUsingFQAN(defaultCloud,options.verbose)
else:
    # use cloud explicitly    
    expCloudFlag = True
if not PsubUtils.checkValidCloud(options.cloud):
    print "ERROR : unsupported cloud:%s" % options.cloud
    sys.exit(EC_Config)


# get files in input dataset
if options.inDS != '':
    # query files in shadow dataset
    shadowList = {}
    if shadowDSexist:
        shadowList = Client.queryFilesInDataset("%s%s" % (options.outDS,suffixShadow),options.verbose)
    # query files in dataset
    print "query files in dataset:%s" % options.inDS
    inputFileMap  = Client.queryFilesInDataset(options.inDS,options.verbose)
    # remove redundant files
    tmpKeys = inputFileMap.keys()
    for tmpLFN in tmpKeys:
        # filename matching
        if options.match != '':
            if re.search(options.match,tmpLFN) == None:
                del inputFileMap[tmpLFN]
                continue
        # files in shadow
        if tmpLFN in shadowList.keys():
            del inputFileMap[tmpLFN]            
            continue
    # append
    inputFileList = inputFileMap.keys()
    inputFileList.sort()
    # get locations
    if inputFileList != []:
        if options.site == "AUTO":
            dsLocationMap = Client.getLocations(options.inDS,inputFileList,options.cloud,False,
                                                options.verbose,expCloud=expCloudFlag)
            # no location
            if dsLocationMap == {}:
                if expCloudFlag:
                    print "ERROR : could not find supported locations in the %s cloud for %s" % (options.cloud,options.inDS)
                else:
                    print "ERROR : could not find supported locations for %s" % options.inDS
                sys.exit(EC_Dataset)
            # run brorage
            tmpSites = []
            for tmpItem in dsLocationMap.values():
                tmpSites += tmpItem
            status,out = Client.runBrokerage(tmpSites,athenaVer,verbose=options.verbose)
            if status != 0:
                print 'failed to run brokerage for automatic assignment: %s' % out
                sys.exit(EC_Config)
            if not Client.PandaSites.has_key(out):
                print 'brokerage gave wrong PandaSiteID:%s' % out
                sys.exit(EC_Config)
            # set site/cloud
            options.site  = out
            options.cloud = Client.PandaSites[options.site]['cloud']
        # scan local replica catalog
        dsLocation = Client.PandaSites[options.site]['ddm']
        if Client.getLFC(dsLocation) != None:
            # LFC
            tmpMissList = Client.getMissLFNsFromLFC(inputFileMap,dsLocation,True,options.verbose)
        elif Client.getLRC(dsLocation) != None:
            # LRC
            tmpMissList = Client.getMissLFNsFromLRC(inputFileMap,Client.getLRC(dsLocation),options.verbose)
        else:
            tmpMissList = []
        if options.verbose:
            print "%s holds %s files" % (options.site,len(inputFileList)-len(tmpMissList))
        # remove missing files
        newInputFiles = []
        for inputFile in inputFileList:
            if not inputFile in tmpMissList:
                newInputFiles.append(inputFile)
        inputFileList = newInputFiles
    # no available input files
    if inputFileList == []:
        print "ERROR : No (new) files available in %s" % options.inDS
        print "        pathena runs on files which were failed or were not used in"
        print "        previous submissions if it runs with the same inDS and outDS"
        sys.exit(EC_Dataset)
    # truncate
    if options.nFiles != 0:
        inputFileList = inputFileList[:options.nFiles]
else:
    # set cloud for no input dataset
    if options.site == "AUTO":
        # get sites belonging to a cloud
        tmpSites = []
        for tmpID,spec in Client.PandaSites.iteritems():
            if spec['cloud']==options.cloud and spec['status']=='online':
                # exclude long,xrootd,local queues
                if Client.isExcudedSite(tmpID):
                    continue
                tmpSites.append(tmpID)
        status,out = Client.runBrokerage(tmpSites,athenaVer,verbose=options.verbose)    
        if status != 0:
            print 'failed to run brokerage for automatic assignment: %s' % out  
            sys.exit(EC_Config)
        if not Client.PandaSites.has_key(out):
            print 'brokerage gave wrong PandaSiteID:%s' % out
            sys.exit(EC_Config)
        # set site
        options.site = out


# go to workdir
os.chdir(options.workDir)

# gather files under work dir
if options.libDS == '':
    print "gathering files under %s" % options.workDir

    # get files in the working dir
    skippedExt   = ['.o','.a','.so']
    skippedFlag  = False
    workDirFiles = []
    for tmpRoot,tmpDirs,tmpFiles in os.walk('.'):
	for tmpFile in tmpFiles:
	    tmpPath = '%s/%s' % (tmpRoot,tmpFile)
	    # get size
	    try:
		size = os.path.getsize(tmpPath)
	    except:
		# skip dead symlink
		if options.verbose:
		    type,value,traceBack = sys.exc_info()
		    print "  Ignore : %s:%s" % (type,value)
		continue
            # skipped extension
            isSkippedExt = False
            for tmpExt in skippedExt:
                if tmpPath.endswith(tmpExt):
                    isSkippedExt = True
                    break
	    # check root
	    isRoot = False
	    if re.search('\.root(\.\d+)*$',tmpPath) != None:
		isRoot = True
	    # extra files
	    isExtra = False
	    for tmpExt in options.extFile:
		if re.search(tmpExt+'$',tmpPath) != None:
		    isExtra = True
		    break
	    # regular files
	    if not isExtra:
                # skipped extensions
                if isSkippedExt:
		    print "   INFO : Skip %s %s" % (str(skippedExt),tmpPath)
		    skippedFlag = True
		    continue
		# skip root
		if isRoot:
		    print "   INFO : Skip root file %s" % tmpPath
		    skippedFlag = True
		    continue
		# check size
		if size > options.maxFileSize:
		    print "   WARNING : Skip large file %s:%sB>%sB" % (tmpPath,size,options.maxFileSize)
		    skippedFlag = True
		    continue

	    # remove ./
	    tmpPath = re.sub('^\./','',tmpPath)
	    # append
	    workDirFiles.append(tmpPath)
    if skippedFlag:
	print "  Please use --maxFileSize or --extFile if you need to send the skipped files to WNs"


# create tmp dir
if options.tmpDir == '':
    tmpDir = '%s/%s' % (curDir,commands.getoutput('uuidgen'))
else:
    tmpDir = '%s/%s' % (options.tmpDir,commands.getoutput('uuidgen'))    
os.makedirs(tmpDir)

# exit action
def _onExit(dir):
    commands.getoutput('rm -rf %s' % dir)
atexit.register(_onExit,tmpDir)

# create archive
if options.libDS == '':
    archiveName     = 'sources.%s.tar' % commands.getoutput('uuidgen')
    archiveFullName = "%s/%s" % (tmpDir,archiveName)

    # collect files
    for tmpFile in workDirFiles:
	if os.path.islink(tmpFile):
	    status,out = commands.getstatusoutput('tar -rh %s -f %s' % (tmpFile,archiveFullName))
	else:
	    status,out = commands.getstatusoutput('tar rf %s %s' % (archiveFullName,tmpFile))
	if options.verbose:
	    print tmpFile
	if status != 0 or out != '':
	    print out

# go to tmpdir
os.chdir(tmpDir)

# compress
if options.libDS == '':
    status,out = commands.getstatusoutput('gzip %s' % archiveName)
    archiveName += '.gz'
    if status !=0 or options.verbose:
	print out

    # check archive
    status,out = commands.getstatusoutput('ls -l %s' % archiveName)
    if options.verbose:
	print out
    if status != 0:
	print "ERROR : Failed to archive working area."
	print "        If you see 'Disk quota exceeded', try '--tmpDir /tmp'" 
	sys.exit(EC_Archive)


# look for pandatools package
for path in sys.path:
    if path == '':
        path = curDir
    if os.path.exists(path) and 'pandatools' in os.listdir(path):
        # make symlink for module name.
        os.symlink('%s/pandatools' % path,'taskbuffer')
        break

# append tmpdir to import taskbuffer module
sys.path = [tmpDir]+sys.path
from taskbuffer.JobSpec  import JobSpec
from taskbuffer.FileSpec import FileSpec


# max total filesize on each WN
maxTotalSize = long(5*1024*1024*1024)

# set number of files per job
if options.inDS != '' and options.nFilesPerJob == None:
    # count total size for inputs
    totalSize = 0
    for tmpLFN in inputFileList:
        vals = inputFileMap[tmpLFN]
        try:
            totalSize += long(vals['fsize'])
        except:
            pass
    # the number of files per max total
    tmpNSplit,tmpMod = divmod(totalSize,maxTotalSize)
    if tmpMod != 0:
        tmpNSplit += 1
    tmpNFiles,tmpMod = divmod(len(inputFileList),tmpNSplit)
    if tmpMod != 0:
        tmpNFiles += 1
    # set upper limit
    upperLimitOnFiles = 20
    if tmpNFiles > upperLimitOnFiles:
        tmpNFiles = upperLimitOnFiles
    # check again just in case
    iDiv = 0
    subTotal = 0
    for tmpLFN in inputFileList:
        vals =inputFileMap[tmpLFN]
        try:
            subTotal += long(vals['fsize'])
        except:
            pass
        iDiv += 1
        if iDiv >= tmpNFiles:
            # check
            if subTotal > maxTotalSize:
                # decrement
                tmpNFiles -= 1
                break
            # reset
            iDiv = 0
            subTotal = 0
    # set
    options.nFilesPerJob = tmpNFiles
                                                            

# calculate number of jobs
if options.nJobs == -1:
    if options.inDS == '':
        options.nJobs = 1
    else:
        rest = len(inputFileList) % options.nFilesPerJob
        if rest == 0:
            options.nJobs = len(inputFileList) / options.nFilesPerJob
        else:
            options.nJobs = (len(inputFileList)-rest) / options.nFilesPerJob
            options.nJobs += 1


# read jobID
jobDefinitionID = 0
jobid_file = '%s/pjobid.dat' % os.environ['PANDA_CONFIG_ROOT']
if os.path.exists(jobid_file):
    try:
        # read line
        tmpJobIdFile = open(jobid_file)
        tmpID = tmpJobIdFile.readline()
        tmpJobIdFile.close()
        # remove \n
        tmpID = tmpID.replace('\n','')
        # convert to int
        jobDefinitionID = long(tmpID) + 1
    except:
        pass


# full execution string
fullExecString = PsubUtils.convSysArgv()

# job name
jobName = 'prun.%s' % commands.getoutput('uuidgen')

# make jobs
jobList = []

# buildJob
if options.libDS == '': 
    jobB = JobSpec()
    jobB.jobDefinitionID   = jobDefinitionID
    jobB.jobName           = jobName
    jobB.metadata          = fullExecString
    if athenaVer != '':
        jobB.AtlasRelease  = athenaVer
        jobB.homepackage   = 'AnalysisTransforms'+cacheVer+nightVer
    jobB.transformation    = '%s/buildGen-00-00-01' % Client.baseURLSUB
    jobB.destinationDBlock = 'user%s.%s.lib._%s' % (time.strftime('%y',time.gmtime()),distinguishedName,
                                                    time.time())
    jobB.destinationSE     = options.site
    jobB.prodSourceLabel   = 'panda'
    jobB.assignedPriority  = 2000
    jobB.computingSite     = options.site
    jobB.cloud             = Client.PandaSites[options.site]['cloud']
    # lib.tgz
    fileBO = FileSpec()
    fileBO.lfn               = '%s.lib.tgz' % jobB.destinationDBlock
    fileBO.type              = 'output'
    fileBO.dataset           = jobB.destinationDBlock
    fileBO.destinationSE     = jobB.destinationSE
    fileBO.destinationDBlock = jobB.destinationDBlock
    jobB.addFile(fileBO)
    fileBI = FileSpec()
    fileBI.lfn = archiveName
    fileBI.type = 'input'
    jobB.jobParameters     = '-i %s -o %s ' % (fileBI.lfn,fileBO.lfn)
    if options.bexec != '':
        jobB.jobParameters += '--bexec "%s" ' % urllib.quote(options.bexec)
        jobB.jobParameters += '-r %s ' % runDir
    # source URL
    matchURL = re.search("(http.*://[^/]+)/",Client.baseURLSSL)
    if matchURL != None:
        jobB.jobParameters += "--sourceURL %s " % matchURL.group(1)
    # log    
    file = FileSpec()
    file.lfn               = '%s.log.tgz' % jobB.destinationDBlock
    file.type              = 'log'
    file.dataset           = jobB.destinationDBlock
    file.destinationSE     = jobB.destinationSE
    file.destinationDBlock = jobB.destinationDBlock
    jobB.addFile(file)
    # set space token
    for file in jobB.Files:
        if file.type in ['output','log']:
            if options.spaceToken != '':
                file.destinationDBlockToken = options.spaceToken
            else:
                defaulttoken = Client.PandaSites[options.site]['defaulttoken']
                file.destinationDBlockToken = Client.getDefaultSpaceToken(vomsFQAN,defaulttoken)
    # append
    if options.verbose:    
        print jobB.jobParameters
    jobList.append(jobB)
else:
    # query files in lib dataset to reuse libraries
    print "query files in libDS:%s" % options.libDS
    tmpList = Client.queryFilesInDataset(options.libDS,options.verbose)
    tmpFileList = []
    tmpGUIDmap = {}
    for fileName in tmpList.keys():
        # ignore log file
        if len(re.findall('.log.tgz.\d+$',fileName)) or len(re.findall('.log.tgz$',fileName)):
            continue
        tmpFileList.append(fileName)
        tmpGUIDmap[fileName] = tmpList[fileName]['guid'] 
    # incomplete libDS
    if tmpFileList == []:
        print "ERROR : dataset %s is empty" % (options.libDS)
        sys.exit(EC_Dataset)
    # check file list                
    if len(tmpFileList) != 1:
        print "ERROR : dataset %s contains multiple lib files %s" % (options.libDS,tmpFileList)
        sys.exit(EC_Dataset)
    # instantiate FileSpec
    fileBO = FileSpec()
    fileBO.lfn = tmpFileList[0]
    fileBO.GUID = tmpGUIDmap[fileBO.lfn]
    fileBO.dataset = options.libDS
    fileBO.destinationDBlock = options.libDS
    fileBO.status = 'ready'


# get files in existing output dataset
existingFilesInOutDS = {}
if options.outputs != '' and outputDSexist:
    # query files in dataset from DDM
    print "query files in dataset:%s" % options.outDS
    existingFilesInOutDS = Client.queryFilesInDataset(options.outDS,options.verbose)
    # query files in dataset from Panda
    status,tmpMap = Client.queryLastFilesInDataset([options.outDS],options.verbose)
    for tmpLFN in tmpMap[options.outDS]:
        if not tmpLFN in existingFilesInOutDS:
            existingFilesInOutDS[tmpLFN] = None


# runJobs
indexOffsetMap = {}
for iJob in range(options.nJobs):
    jobR = JobSpec()
    jobR.jobDefinitionID   = jobDefinitionID
    jobR.jobName           = jobName
    jobR.metadata          = fullExecString
    if athenaVer != '':
        jobR.AtlasRelease  = athenaVer
        jobR.homepackage   = 'AnalysisTransforms'+cacheVer+nightVer
    jobR.transformation    = '%s/runGen-00-00-01' % Client.baseURLSUB
    jobR.destinationDBlock = options.outDS
    jobR.destinationSE     = options.site
    jobR.prodSourceLabel   = 'user'
    jobR.assignedPriority  = 1000
    jobR.computingSite     = options.site
    jobR.cloud             = Client.PandaSites[options.site]['cloud']
    jobR.jobParameters     = '-j "%s" ' % jobScript
    if options.jobParams != '':
        # random seed
        tmpJobO = options.jobParams
        for tmpItem in tmpJobO.split():
            match = re.search('%RNDM=(.+)$',tmpItem)
            if match:
                tmpRndmNum = int(match.group(1)) + iJob
                # replace parameters
                tmpJobO = tmpJobO.replace(match.group(0),'%s' % tmpRndmNum)
        jobR.jobParameters += '-p "%s" ' % urllib.quote(tmpJobO)
    # source files
    fileS = FileSpec()
    fileS.lfn            = fileBO.lfn
    fileS.GUID           = fileBO.GUID
    fileS.type           = 'input'
    fileS.status         = fileBO.status
    fileS.dataset        = fileBO.destinationDBlock
    fileS.dispatchDBlock = fileBO.destinationDBlock
    jobR.addFile(fileS)
    jobR.jobParameters  += '-l %s ' % fileS.lfn    
    # input files
    if options.inDS != '':
        totalSize = 0
        indexIn = iJob*options.nFilesPerJob
        inList = inputFileList[indexIn:indexIn+options.nFilesPerJob]
        if inList == []:
            break
        for tmpLFN in inList:
            vals = inputFileMap[tmpLFN]
            fileI = FileSpec()
            fileI.lfn        = tmpLFN
            fileI.GUID       = vals['guid']
	    fileI.fsize      = vals['fsize']
	    fileI.md5sum     = vals['md5sum']
            fileI.dataset    = options.inDS
            fileI.prodDBlock = options.inDS
            fileI.type       = 'input'
            fileI.status     = 'ready'
            fileI.dispatchDBlock = options.inDS
            jobR.addFile(fileI)
            try:
                totalSize += long(fileI.fsize)
            except:
                pass
        # size check    
        if totalSize > maxTotalSize:
            print "ERROR : A subjob has %s input files and requires %sMB of disk space." \
                  % (len(inList), (totalSize >> 20))
            print "        It must be less than %sMB to avoid overflowing the remote disk." \
                  % (maxTotalSize >> 20)
            print "        Please split the job using --nFilesPerJob."
            sys.exit(EC_Split)
        # set parameter
        jobR.jobParameters += '-i %s ' % AthenaUtils.convToCompact(inList)
    # output files
    outMap = {}
    if options.outputs != '':
        for tmpLFN in options.outputs.split(','):
            # set offset to 0 for fresh output dataset
            if existingFilesInOutDS == {}:
                indexOffsetMap[tmpLFN] = 0
            # offset is already obtained
            if indexOffsetMap.has_key(tmpLFN):
                idxOffset = indexOffsetMap[tmpLFN]
                getOffset = False
            else:
                idxOffset = 0
                getOffset = True
            # new LFN
            tmpNewLFN = '%s._%05d.%s' % (jobR.destinationDBlock,idxOffset+iJob+1,tmpLFN)
            # change * to XYZ and add .tgz
	    if tmpNewLFN.find('*') != -1:
		tmpNewLFN = tmpNewLFN.replace('*','XYZ')
		tmpNewLFN = '%s.tgz' % tmpNewLFN
            # get offset
            if getOffset:
                oldHead = '%s._%05d.' % (jobR.destinationDBlock,idxOffset+iJob+1)
                filePatt = tmpNewLFN.replace(oldHead,'%s\._(\d+)\.' % jobR.destinationDBlock)
                idxOffset = PsubUtils.getMaxIndex(existingFilesInOutDS,filePatt)
                # append
                existingFilesInOutDS[tmpLFN] = idxOffset
                # correct
                newHead = '%s._%05d.' % (jobR.destinationDBlock,idxOffset+iJob+1)
                tmpNewLFN = tmpNewLFN.replace(oldHead,newHead)
            # set file spec
            file = FileSpec()
            file.lfn               = tmpNewLFN              
            file.type              = 'output'
            file.dataset           = jobR.destinationDBlock        
            file.destinationSE     = jobR.destinationSE
            file.destinationDBlock = jobR.destinationDBlock
            jobR.addFile(file)
            outMap[tmpLFN] = file.lfn
        # set parameter
        jobR.jobParameters += '-o "%s" ' % str(outMap)
    # log
    file = FileSpec()
    file.lfn               = '%s._$PANDAID.log.tgz' % jobR.destinationDBlock
    file.type              = 'log'
    file.dataset           = jobR.destinationDBlock    
    file.destinationSE     = jobR.destinationSE
    file.destinationDBlock = jobR.destinationDBlock
    jobR.addFile(file)
    jobR.jobParameters += '-r %s ' % runDir
    # set space token
    for file in jobR.Files:
        if file.type in ['output','log']:
            if options.spaceToken != '':
                file.destinationDBlockToken = options.spaceToken
            else:
                defaulttoken = Client.PandaSites[options.site]['defaulttoken']
                file.destinationDBlockToken = Client.getDefaultSpaceToken(vomsFQAN,defaulttoken)
    # append
    if options.verbose:    
        print jobR.jobParameters
    jobList.append(jobR)


# no submit 
if options.nosubmit:
    sys.exit(0)


# upload proxy for glexec
if Client.PandaSites.has_key(options.site):
    # delegation
    delResult = PsubUtils.uploadProxy(options.site,options.myproxy,gridPassPhrase,options.verbose)
    if not delResult:
        print "ERROR : failed to upload proxy"
        sys.exit(EC_MyProxy)


# upload sources via HTTP POST
if options.libDS == '': 
    print "upload sources"
    status,out = Client.putFile(archiveName,options.verbose)
    if out != 'True':
	print out
	print "ERROR : %s" % status
	sys.exit(EC_Post)


# register output dataset
if not outputDSexist:
    Client.addDataset(options.outDS,options.verbose)

# register shadow dataset
if not outputDSexist:
    Client.addDataset("%s%s" % (options.outDS,suffixShadow),options.verbose)

# register libDS
if options.libDS == '':
    Client.addDataset(jobB.destinationDBlock,options.verbose)

# submit
print "submit"
status,out = Client.submitJobs(jobList,options.verbose)

# result
print '==================='
outstr   = ''
buildStr = ''
runStr   = ''
for index,o in enumerate(out):
    if o != None:
        if index==0:
            # set JobID
            jobID = o[1]
        if index==0 and options.libDS=='':
            outstr += "  > build\n"
            outstr += "    PandaID=%s\n" % o[0]
            buildStr = '%s' % o[0]            
        elif (index==1 and options.libDS=='') or \
             (index==0 and options.libDS!=''):
            outstr += "  > run\n"
            outstr += "    PandaID=%s" % o[0]
            runStr = '%s' % o[0]                        
        elif index+1==len(out):
            outstr += "-%s" % o[0]
            runStr += '-%s' % o[0]                                    
print ' JobID  : %s' % jobID
print ' Status : %d' % status
print outstr

# create dir for DB
dbdir = os.path.expanduser(os.environ['PANDA_CONFIG_ROOT'])
if not os.path.exists(dbdir):
    os.makedirs(dbdir)

# record jobID
tmpJobIdFile = open(jobid_file,'w')
tmpJobIdFile.write(str(jobID))
tmpJobIdFile.close()

# open DB
db = shelve.open('%s/job_record' % dbdir)

# create job record
jobRecord = {}
jobRecord['time' ] = datetime.datetime.now()
jobRecord['inDS' ] = options.inDS
jobRecord['outDS'] = options.outDS
if options.libDS=='':
    jobRecord['libDS'] = jobB.destinationDBlock
else:
    jobRecord['libDS'] = options.libDS
jobRecord['build'] = buildStr
jobRecord['run'  ] = runStr
jobRecord['jobO' ] = options.jobParams
jobRecord['site' ] = jobList[0].computingSite
jobRecord['jobParams' ] = PsubUtils.convSysArgv()

# record job into DB
db['%s' % jobID] = jobRecord

# close DB
db.close()
