#!/bin/bash

"exec" "python" "-u" "-Wignore" "$0" "$@"

import os
import re
import sys
import time
import copy
import shutil
import atexit
import commands
import optparse
import shelve
import datetime
import urllib
import random
import fcntl
import types
import traceback
import pickle

####################################################################

# error code
EC_Config    = 10
EC_CMT       = 20
EC_Extractor = 30
EC_Dataset   = 40
EC_Post      = 50
EC_Archive   = 60
EC_Split     = 70
EC_MyProxy   = 80
EC_Submit    = 90

#@ Number of events to skip in the file
nEventsToSkip=0
#@ Events blok counter per file
nSkips =0

# default cloud/site
defaultCloud = None

# max lookup for cross site option
maxCrossSite = 50

# default maxCpuCount
defaultMaxCpuCount = 12*60*60

usage = """%prog [options] <jobOption1.py> [<jobOption2.py> [...]]

'%prog --help' prints a summary of the options

  HowTo is available at https://twiki.cern.ch/twiki/bin/view/PanDA/PandaAthena"""


# command-line parameters
optP = optparse.OptionParser(usage=usage,conflict_handler="resolve")
# special options
optP.add_option('--version',action='store_const',const=True,dest='version',default=False,
                help='Displays version')
optP.add_option('--split', action='store', dest='split',  default=-1,
                type='int',    help='Number of sub-jobs to which a job is split')
optP.add_option('--nFilesPerJob', action='store', dest='nFilesPerJob',  default=-1, type='int', help='Number of files on which each sub-job runs')
optP.add_option('--nEventsPerJob', action='store', dest='nEventsPerJob',  default=-1,
                type='int',    help='Number of events per subjob. This info is used mainly for job splitting. If you run on MC datasets, the total number of subjobs is nEventsPerFile*nFiles/nEventsPerJob. For data, the number of events for each file is retrieved from AMI and subjobs are created accordingly. Note that if you run transformations you need to explicitly specify maxEvents or something in --trf to set the number of events processed in each subjob. If you run normal jobOption files, evtMax and skipEvents in appMgr are automatically set on WN.')
optP.add_option('--nEventsPerFile', action='store', dest='nEventsPerFile',  default=0,
                type='int',    help='Number of events per file')
optP.add_option('--nGBPerJob',action='store',dest='nGBPerJob',default=-1, help='Instantiate one sub job per NGBPERJOB GB of input files. --nGBPerJob=MAX sets the size to the default maximum value')
optP.add_option('--site', action='store', dest='site',  default="AUTO",
                type='string',    help='Site name where jobs are sent. If omitted, jobs are automatically sent to sites where input is available. A comma-separated list of sites can be specified (e.g. siteA,siteB,siteC), so that the best site(s) is chosen from the given site list. If AUTO is appended at the end of the list (e.g. siteA,siteB,siteC,AUTO), jobs are sent to any sites if input is not found in the previous sites')
optP.add_option('--athenaTag',action='store',dest='athenaTag',default='',type='string',
                help='Use differnet version of Athena on remote WN. By defualt the same version which you are locally using is set up on WN. e.g., --athenaTag=AtlasProduction,14.2.24.3')
optP.add_option('--inDS',  action='store', dest='inDS',  default='',
                type='string', help='Input dataset names. wildcard and/or comma can be used to concatenate multiple datasets')
optP.add_option('--inDsTxt',action='store',dest='inDsTxt',default='',
                type='string', help='a text file which contains the list of datasets to run over. newlines are replaced by commas and the result is set to --inDS. lines starting with # are ignored')
optP.add_option('--minDS',  action='store', dest='minDS',  default='',
                type='string', help='Dataset name for minimum bias stream')
optP.add_option('--nMin',  action='store', dest='nMin',  default=-1,
                type='int', help='Number of minimum bias files per one signal file')
optP.add_option('--nMinPerJob',  action='store', dest='nMinPerJob',  default=-1,
                type='int', help='Number of minimum bias files per sub job')
optP.add_option('--lowMinDS',  action='store', dest='lowMinDS',  default='',
                type='string', help='Dataset name for low pT minimum bias stream')
optP.add_option('--nLowMin',  action='store', dest='nLowMin',  default=-1,
                type='int', help='Number of low pT minimum bias files per one signal file')
optP.add_option('--nLowMinPerJob',  action='store', dest='nLowMinPerJob',  default=-1,
                type='int', help='Number of low pT minimum bias files per sub job')
optP.add_option('--highMinDS',  action='store', dest='highMinDS',  default='',
                type='string', help='Dataset name for high pT minimum bias stream')
optP.add_option('--nHighMin',  action='store', dest='nHighMin',  default=-1,
                type='int', help='Number of high pT minimum bias files per one signal file')
optP.add_option('--nHighMinPerJob',  action='store', dest='nHighMinPerJob',  default=-1,
                type='int', help='Number of high pT minimum bias files per sub job')
optP.add_option('--randomMin',action='store_const',const=True,dest='randomMin',default=False,
                help='randomize files in minimum bias dataset')
optP.add_option('--cavDS',  action='store', dest='cavDS',  default='',
                type='string', help='Dataset name for cavern stream')
optP.add_option('--nCav',  action='store', dest='nCav',  default=-1,
                type='int', help='Number of cavern files per one signal file')
optP.add_option('--nCavPerJob',  action='store', dest='nCavPerJob',  default=-1,
                type='int', help='Number of cavern files per sub job')
optP.add_option('--randomCav',action='store_const',const=True,dest='randomCav',default=False,
                help='randomize files in cavern dataset')
optP.add_option('--libDS', action='store', dest='libDS', default='',
                type='string', help='Name of a library dataset')
optP.add_option('--goodRunListXML', action='store', dest='goodRunListXML', default='',
                type='string', help='Good Run List XML which will be converted to datasets by AMI')
optP.add_option('--goodRunListDataType', action='store', dest='goodRunDataType', default='',
                type='string', help='specify data type when converting Good Run List XML to datasets, e.g, AOD (default)')
optP.add_option('--goodRunListProdStep', action='store', dest='goodRunProdStep', default='',
                type='string', help='specify production step when converting Good Run List to datasets, e.g, merge (default)')
optP.add_option('--goodRunListDS', action='store', dest='goodRunListDS', default='',
                type='string', help='A comma-separated list of pattern strings. Datasets which are converted from Good Run List XML will be used when they match with one of the pattern strings. Either \ or "" is required when a wild-card is used. If this option is omitted all datasets will be used')
optP.add_option('--eventPickEvtList',action='store',dest='eventPickEvtList',default='',
                type='string', help='a file name which contains a list of runs/events for event picking')
optP.add_option('--eventPickDataType',action='store',dest='eventPickDataType',default='',
                type='string', help='type of data for event picking. one of AOD,ESD,RAW')
optP.add_option('--eventPickStreamName',action='store',dest='eventPickStreamName',default='',
                type='string', help='stream name for event picking. e.g., physics_CosmicCaloEM')
optP.add_option('--eventPickDS',action='store',dest='eventPickDS',default='',
                type='string', help='A comma-separated list of pattern strings. Datasets which are converted from the run/event list will be used when they match with one of the pattern strings. Either \ or "" is required when a wild-card is used. e.g., data\*')
optP.add_option('--eventPickStagedDS',action='store',dest='eventPickStagedDS',default='',
                type='string', help='--eventPick options create a temporary dataset to stage-in interesting files when those files are available only on TAPE, and then a stage-in request is automatically sent to DaTRI. Once DaTRI transfers the dataset to DISK you can use the dataset as an input using this option')
optP.add_option('--eventPickAmiTag',action='store',dest='eventPickAmiTag',default='',
                type='string', help='AMI tag used to match TAG collections names. This option is required when you are interested in older data than the latest one. Either \ or "" is required when a wild-card is used. e.g., f2\*')
optP.add_option('--useNewTRF', action='store_const',const=True,dest='useNewTRF',default=True,
                help="Use the original filename with the attempt number for input in --trf when there is only one input, which follows the globbing scheme of new transformation framework")
optP.add_option('--useOldTRF', action='store_const',const=True,dest='useOldTRF',default=False,
                help="Remove the attempt number from the original filename for input in --trf when there is only one input")
optP.add_option('--useTagInTRF', action='store_const',const=True,dest='useTagInTRF',default=False,
                help="Set this option if you use TAG in --trf. If you run normal jobO this option is not required")
optP.add_option('--tagStreamRef',action='store',dest='tagStreamRef',default='',
                type='string', help='specify StreamRef of parent files when you use TAG in --trf. It must be one of StreamRAW,StreamESD,StreamAOD. E.g., if you want to read RAW files via TAGs, use --tagStreamRef=StreamRAW. If you run normal jobO, this option is ignored and EventSelector.RefName in your jobO is used')
optP.add_option('--tagQuery',action='store',dest='tagQuery',default='',
                type='string', help='specify Query for TAG preselection when you use TAG in --trf. If you run normal jobO, this option is ignored and EventSelector.Query in your jobO is used')
optP.add_option('--express', action='store_const',const=True,dest='express',default=False,
                help="Send the job using express quota to have higher priority. The number of express subjobs in the queue and the total execution time used by express subjobs are limited (a few subjobs and several hours per day, respectively). This option is intended to be used for quick tests before bulk submission. Note that buildXYZ is not included in quota calculation. If this option is used when quota has already exceeded, the panda server will ignore the option so that subjobs have normal priorities. Also, if you submit 1 buildXYZ and N runXYZ subjobs when you only have quota of M (M < N),  only the first M runXYZ subjobs will have higher priorities")
optP.add_option('--debugMode', action='store_const',const=True,dest='debugMode',default=False,
                help="Send the job with the debug mode on. If this option is specified the subjob will send stdout to the panda monitor every 5 min. The number of debug subjobs per user is limited. When this option is used and the quota has already exceeded, the panda server supresses the option so that subjobs will run without the debug mode. If you submit multiple subjobs in a single job, only the first subjob will set the debug mode on. Note that you can turn the debug mode on/off by using pbook after jobs are submitted" )
optP.add_option('--notUseTagLookup',action='store_const',const=True,dest='notUseTagLookup',default=False,
                help="don't use Event Lookup service to retrieve relation between TAG and parent datasets")
optP.add_option('--useContElementBoundary',action='store_const',const=True,dest='useContElementBoundary',default=False,
                help="Split job in such a way that sub jobs do not mix files of different datasets in the input container. See --useNthFieldForLFN too")
optP.add_option('--useNthFieldForLFN',action='store',dest='useNthFieldForLFN',default=2,type='int',
                help="all output files from a dataset in the input container have the same middle name in LFN, when --useContElementBoundary is used. The middle name is extracted from the dataset name. The second field is used by default. i.e., if the dataset name is data10_7TeV.00160387.physics_Muon..., 00160387 is used and LFN is something like user.hoge.JOBSETID.00160387.blha. One can change the extracted field using this option")
optP.add_option('--splitWithNthFiledOfLFN',action='store',dest='splitWithNthFiledOfLFN',default=-1,type='int',
                help="Split job in such a way that each sub job uses input files which have the same value in the Nth field of file names. This option would be useful to process a dataset which was produced with --useContElementBoundary, and in this case most likely that --splitWithNthFiledOfLFN=4")
optP.add_option('--buildInLastChunk',action='store_const',const=True,dest='buildInLastChunk',default=False,
                help="Produce lib.tgz in the last chunk when jobs are split to multiple chunks due to the limit on the number of files in each chunk or due to --useContElementBoundary/--loadXML")
optP.add_option('--useAMIEventLevelSplit',action='store_const',const=True,dest='useAMIEventLevelSplit',default=None,
                help="retrive the number of events per file from AMI to split the job using --nEventsPerJob")
optP.add_option('--appendStrToExtStream',action='store_const',const=True,dest='appendStrToExtStream',default=False,
                help='append the first part of filenames to extra stream names for --individualOutDS. E.g., if this option is used together with --individualOutDS, %OUT.AOD.pool.root will be contained in an EXT0_AOD dataset instead of an EXT0 dataset')
optP.add_option('--mergeOutput', action='store_const', const=True, dest='mergeOutput', default=False,
                help="merge output files")
optP.add_option('--mergeScript',action='store',dest='mergeScript',default='',type='string',
                help='Specify user-defied script or execution string for output merging')
optP.add_option('--useCommonHalo', action='store_const', const=False, dest='useCommonHalo',  default=True,
                help="use an integrated DS for BeamHalo")
optP.add_option('--beamHaloDS',  action='store', dest='beamHaloDS',  default='',
                type='string', help='Dataset name for beam halo')
optP.add_option('--beamHaloADS',  action='store', dest='beamHaloADS',  default='',
                type='string', help='Dataset name for beam halo A-side')
optP.add_option('--beamHaloCDS',  action='store', dest='beamHaloCDS',  default='',
                type='string', help='Dataset name for beam halo C-side')
optP.add_option('--nBeamHalo',  action='store', dest='nBeamHalo',  default=-1,
                type='int', help='Number of beam halo files per sub job')
optP.add_option('--nBeamHaloA',  action='store', dest='nBeamHaloA',  default=-1,
                type='int', help='Number of beam halo files for A-side per sub job')
optP.add_option('--nBeamHaloC',  action='store', dest='nBeamHaloC',  default=-1,
                type='int', help='Number of beam halo files for C-side per sub job')
optP.add_option('--useCommonGas', action='store_const', const=False, dest='useCommonGas',  default=True,
                help="use an integrated DS for BeamGas")
optP.add_option('--beamGasDS',  action='store', dest='beamGasDS',  default='',
                type='string', help='Dataset name for beam gas')
optP.add_option('--beamGasHDS',  action='store', dest='beamGasHDS',  default='',
                type='string', help='Dataset name for beam gas Hydrogen')
optP.add_option('--beamGasCDS',  action='store', dest='beamGasCDS',  default='',
                type='string', help='Dataset name for beam gas Carbon')
optP.add_option('--beamGasODS',  action='store', dest='beamGasODS',  default='',
                type='string', help='Dataset name for beam gas Oxygen')
optP.add_option('--nBeamGas',  action='store', dest='nBeamGas',  default=-1,
                type='int', help='Number of beam gas files per sub job')
optP.add_option('--nBeamGasH',  action='store', dest='nBeamGasH',  default=-1,
                type='int', help='Number of beam gas files for Hydrogen per sub job')
optP.add_option('--nBeamGasC',  action='store', dest='nBeamGasC',  default=-1,
                type='int', help='Number of beam gas files for Carbon per sub job')
optP.add_option('--nBeamGasO',  action='store', dest='nBeamGasO',  default=-1,
                type='int', help='Number of beam gas files for Oxygen per sub job')
optP.add_option('--parentDS', action='store', dest='parentDS', default='',
                type='string', help='Parent dataset names. The brokerage takes their locations into account for TAG-based analysis')
optP.add_option('--outDS', action='store', dest='outDS', default='',
                type='string', help='Name of an output dataset. OUTDS will contain all output files')
optP.add_option('--destSE',action='store', dest='destSE',default='',
                type='string', help='Destination strorage element')
optP.add_option('--nFiles', '--nfiles', action='store', dest='nfiles',  default=0,
                type='int',    help='Use an limited number of files in the input dataset')
optP.add_option('--nSkipFiles', action='store', dest='nSkipFiles',  default=0,
                type='int',    help='Skip N files in the input dataset')
optP.add_option('--provenanceID',action='store',dest='provenanceID',default=-1,type='int',
                help='provenanceID')
optP.add_option('--useSiteGroup',action='store',dest='useSiteGroup',default=-1,type='int',
                help='Use only site groups which have group numbers not higher than --siteGroup. Group 0: T1 or undefined, 1,2,3,4: alpha,bravo,charlie,delta which are defined based on site reliability')
optP.add_option('-v', action='store_const', const=True, dest='verbose',  default=False,
                help='Verbose')
optP.add_option('-l', '--long', action='store_const', const=True, dest='long',  default=False,
                help='Send job to a long queue')
optP.add_option('--blong', action='store_const', const=True, dest='blong',  default=False,
                help='Send build job to a long queue')
optP.add_option('--update', action='store_const', const=True, dest='update',  default=False,
                help='Update panda-client to the latest version')
optP.add_option('--cloud',action='store', dest='cloud',default=None,
                type='string', help='cloud where jobs are submitted. default is set according to your VOMS country group')
optP.add_option('--noBuild', action='store_const', const=True, dest='nobuild',  default=False,
                help='Skip buildJob')
optP.add_option('--noCompile', action='store_const',const=True,dest='noCompile',default=False,
                help='Just upload a tarball in the build step to avoid the tighter size limit imposed by --noBuild. The tarball contains binaries compiled on your local computer, so that compilation is skipped in the build step on remote WN')
optP.add_option('--noOutput', action='store_const', const=True, dest='noOutput',  default=False,
                help='Send job even if there is no output file')
optP.add_option('--individualOutDS', action='store_const', const=True, dest='individualOutDS',  default=False,
                help='Create individual output dataset for each data-type. By default, all output files are added to one output dataset')
optP.add_option('--transferredDS',action='store', dest='transferredDS',default='',type='string',
                help='Specify a comma-separated list of patterns so that only datasets which match the given patterns are transferred when --destSE is set. Either \ or "" is required when a wildcard is used. If omitted, all datasets are transferred')
optP.add_option('--noRandom', action='store_const', const=True, dest='norandom',  default=False,
                help='Enter random seeds manually')
optP.add_option('--useAMIAutoConf',action='store_const',const=True,dest='useAMIAutoConf',default=False,
                help='Use AMI for AutoConfiguration')
optP.add_option('--memory', action='store', dest='memory',  default=-1,
                type='int',    help='Required memory size in MB. e.g., for 1GB --memory 1024')
# FIXME
#optP.add_option('--maxCpuCount', action='store', dest='maxCpuCount', default=defaultMaxCpuCount, type='int',
optP.add_option('--maxCpuCount', action='store', dest='maxCpuCount', default=-1, type='int',
                help='Required CPU count in seconds. Mainly to extend time limit for looping detection')
optP.add_option('--official', action='store_const', const=True, dest='official',  default=False,
                help='Produce official dataset')
optP.add_option('--unlimitNumOutputs', action='store_const', const=True, dest='unlimitNumOutputs',  default=False,
                help='Remove the limit on the number of outputs. Note that having too many outputs per job causes a severe load on the system. You may be banned if you carelessly use this option') 
optP.add_option('--descriptionInLFN',action='store',dest='descriptionInLFN',default='',
                help='LFN is user.nickname.jobsetID.something (e.g. user.harumaki.12345.AOD._00001.pool) by default. This option allows users to put a description string into LFN. i.e., user.nickname.jobsetID.description.something')
optP.add_option('--extFile', action='store', dest='extFile',  default='',
                help='pathena exports files with some special extensions (.C, .dat, .py .xml) in the current directory. If you want to add other files, specify their names, e.g., data1.root,data2.doc')
optP.add_option('--excludeFile',action='store',dest='excludeFile',default='',
                help='specify a comma-separated string to exclude files and/or directories when gathering files in local working area. Either \ or "" is required when a wildcard is used. e.g., doc,\*.C')
optP.add_option('--extOutFile', action='store', dest='extOutFile',  default='',
                help='A comma-separated list of extra output files which cannot be extracted automatically. Either \ or "" is required when a wildcard is used. e.g., output1.txt,output2.dat,JiveXML_\*.xml')
optP.add_option('--supStream', action='store', dest='supStream',  default='',
                help='suppress some output streams. Either \ or "" is required when a wildcard is used. e.g., ESD,TAG,GLOBAL,StreamDESD\* ')
optP.add_option('--gluePackages', action='store', dest='gluePackages',  default='',
                help='list of glue packages which pathena cannot find due to empty i686-slc4-gcc34-opt. e.g., External/AtlasHepMC,External/Lhapdf')
optP.add_option('--excludedSite', action='append', dest='excludedSite',  default=[],
                help="list of sites which are not used for site section, e.g., ANALY_ABC,ANALY_XYZ")
optP.add_option('--noSubmit', action='store_const', const=True, dest='nosubmit',  default=False,
                help="Don't submit jobs")
optP.add_option('--prodSourceLabel', action='store', dest='prodSourceLabel',  default='',
                help="set prodSourceLabel")
optP.add_option('--processingType', action='store', dest='processingType',  default='pathena',
                help="set processingType")
optP.add_option('--seriesLabel', action='store', dest='seriesLabel',  default='',
                help="set seriesLabel")
optP.add_option('--workingGroup', action='store', dest='workingGroup',  default=None,
                help="set workingGroup")
optP.add_option('--generalInput', action='store_const', const=True, dest='generalInput',  default=False,
                help='Read input files with general format except POOL,ROOT,ByteStream')
optP.add_option('--crossSite',action='store',dest='crossSite',default=maxCrossSite,
                type='int',help='submit jobs to N sites at most when datasets in container split over many sites (N=%s by default)' % maxCrossSite)
optP.add_option('--tmpDir', action='store', dest='tmpDir', default='',
                type='string', help='Temporary directory in which an archive file is created')
optP.add_option('--shipInput', action='store_const', const=True, dest='shipinput',  default=False,
                help='Ship input files to remote WNs')
optP.add_option('--noLock', action='store_const', const=True, dest='nolock',  default=False,
                help="Don't create a lock for local database access")
optP.add_option('--disableAutoRetry',action='store_const',const=True,dest='disableAutoRetry',default=False,
                help='disable automatic job retry on the server side')
optP.add_option('--fileList', action='store', dest='filelist', default='',
                type='string', help='List of files in the input dataset to be run')
optP.add_option('--myproxy', action='store', dest='myproxy', default='myproxy.cern.ch',
                type='string', help='Name of the myproxy server')
optP.add_option('--dbRelease', action='store', dest='dbRelease', default='LATEST',
                type='string', help='DBRelease or CDRelease (DatasetName:FileName). e.g., ddo.000001.Atlas.Ideal.DBRelease.v050101:DBRelease-5.1.1.tar.gz. If --dbRelease=LATEST, the latest DBRelease is used')
optP.add_option('--dbRunNumber', action='store', dest='dbRunNumber', default='',
                type='string', help='RunNumber for DBRelease or CDRelease. If this option is used some redundant files are removed to save disk usage when unpacking DBRelease tarball. e.g., 0091890')
optP.add_option('--addPoolFC', action='store', dest='addPoolFC',  default='',
                help="file names to be inserted into PoolFileCatalog.xml except input files. e.g., MyCalib1.root,MyGeom2.root") 
optP.add_option('--skipScan', action='store_const', const=True, dest='skipScan', default=False,
                help='Skip LFC lookup at job submission')
optP.add_option('--inputFileList', action='store', dest='inputFileList', default='',
                type='string', help='name of file which contains a list of files to be run in the input dataset')
optP.add_option('--removeFileList', action='store', dest='removeFileList', default='',
                type='string', help='name of file which contains a list of files to be removed from the input dataset')
optP.add_option('--removedDS', action='store', dest='removedDS', default='',
                type='string', help="don't use datasets in the input dataset container")
optP.add_option('--corCheck', action='store_const', const=True, dest='corCheck',  default=False,
                help='Enable a checker to skip corrupted files')
optP.add_option('--prestage', action='store_const', const=True, dest='prestage',  default=False,
                help='EXPERIMENTAL : Enable prestager. Make sure that you are authorized')
optP.add_option('--voms', action='store', dest='vomsRoles',  default=None, type='string',
                help="generate proxy with paticular roles. e.g., atlas:/atlas/ca/Role=production,atlas:/atlas/fr/Role=pilot")
optP.add_option('--useNextEvent', action='store_const', const=True, dest='useNextEvent',  default=False,
                help="Set this option if your jobO uses theApp.nextEvent(), e.g. for G4. Note that this option is not required when you run transformations using --trf")
optP.add_option('--ara', action='store_const', const=True, dest='ara',  default=False,
                help='obsolete. Please use prun instead')
optP.add_option('--ares', action='store_const', const=True, dest='ares',  default=False,
                help='obsolete. Please use prun instead')
optP.add_option('--araOutFile', action='store', dest='araOutFile',  default='',
                help='define output files for ARA, e.g., output1.root,output2.root')
optP.add_option('--trf', action='store', dest='trf',  default=False,
                help='run transformation, e.g. --trf "csc_atlfast_trf.py %IN %OUT.AOD.root %OUT.ntuple.root -1 0"')
optP.add_option('--spaceToken', action='store', dest='spaceToken', default='',
                type='string', help='spacetoken for outputs. e.g., ATLASLOCALGROUPDISK')
optP.add_option('--notSkipMissing', action='store_const', const=True, dest='notSkipMissing',  default=False,
                help='If input files are not read from SE, they will be skipped by default. This option disables the functionality')
optP.add_option('--burstSubmit', action='store', dest='burstSubmit', default='',
                type='string', help="Please don't use this option. Only for site validation by experts")
optP.add_option('--removeBurstLimit', action='store_const', const=True, dest='removeBurstLimit', default=False,
                help="Please don't use this option. Only for site validation by experts")
optP.add_option('--useShortLivedReplicas', action='store_const', const=True, dest='useShortLivedReplicas', default=False,
                help="Use replicas even if they have very sort lifetime")
optP.add_option('--useDirectIOSites', action='store_const', const=True, dest='useDirectIOSites', default=False,
                help="Use only sites which use directIO to read input files")
optP.add_option('--devSrv', action='store_const', const=True, dest='devSrv',  default=False,
                help="Please don't use this option. Only for developers to use the dev panda server")
optP.add_option('--useAIDA', action='store_const', const=True, dest='useAIDA',  default=False,
                help="use AIDA")
optP.add_option('--inputType', action='store', dest='inputType', default='',
                type='string', help='File type in input dataset which contains multiple file types')
optP.add_option('--outTarBall', action='store', dest='outTarBall', default='',
                type='string', help='Save a gzipped tarball of local files which is the input to buildXYZ')
optP.add_option('--inTarBall', action='store', dest='inTarBall', default='',
                type='string', help='Use a gzipped tarball of local files as input to buildXYZ. Generall the tarball is created by using --outTarBall')
optP.add_option('--outRunConfig', action='store', dest='outRunConfig', default='',
                type='string', help='Save extracted config information to a local file')
optP.add_option('--inRunConfig', action='store', dest='inRunConfig', default='',
                type='string', help='Use a saved config information to skip config extraction')
optP.add_option('--mcData', action='store', dest='mcData', default='',
                type='string', help='Create a symlink with linkName to .dat which is contained in input file')
optP.add_option('--pfnList', action='store', dest='pfnList', default='',
                type='string', help='Name of file which contains a list of input PFNs. Those files can be un-registered in DDM')
optP.add_option('--outputPath',action='store',dest='outputPath', default='./',
                type='string', help='Physical path of output directory relative to a root path')
optP.add_option('--useExperimental', action='store_const', const=True, dest='useExperimental',  default=False,
                help='use experimental features')
optP.add_option('--useOldStyleOutput',action='store_const',const=True,dest='useOldStyleOutput',default=False,
                help="use output dataset and long LFN instead of output dataset container and short LFN")
optP.add_option('--disableRebrokerage',action='store_const',const=True,dest='disableRebrokerage',default=False,
                help="disable auto-rebrokerage")
optP.add_option('--useChirpServer',action='store',dest='useChirpServer', default='',
                type='string', help='The CHIRP server where output files are written to. e.g., --useChirpServer voatlas92.cern.ch')
optP.add_option('--useGOForOutput',action='store',dest='useGOForOutput',default='',metavar='GOENDPOINT',
                type='string', help='The Globus Online server where output files are written to. e.g., --useGOForOutput voatlas92.cern.ch')
optP.add_option('--enableJEM',action='store_const',const=True,dest='enableJEM',default=False,
                help="enable JEM")
optP.add_option('--configJEM', action='store', dest='configJEM', default='',
                type='string', help='configration parameters for JEM')
optP.add_option('--cmtConfig', action='store', dest='cmtConfig', default=None,
                type='string', help='CMTCONFIG=i686-slc5-gcc43-opt is used on remote worker-node by default even if you use another CMTCONFIG locally. This option allows you to use another CMTCONFIG remotely. e.g., --cmtConfig x86_64-slc5-gcc43-opt. If you use --libDS together with this option, make sure that the libDS was compiled with the same CMTCONFIG, in order to avoid failures due to inconsistency in binary files')
# athena options
optP.add_option('-c',action='store',dest='singleLine',type='string',default='',metavar='COMMAND',
                help='One-liner, runs before any jobOs')
optP.add_option('-p',action='store',dest='preConfig',type='string',default='',metavar='BOOTSTRAP',
                help='location of bootstrap file')
optP.add_option('-s',action='store_const',const=True,dest='codeTrace',default=False,
                help='show printout of included files')
optP.add_option('--queueData', action='store', dest='queueData', default='',
                type='string', help="Please don't use this option. Only for developers")
# internal parameters
optP.add_option('--panda_srvURL', action='store', dest='panda_srvURL', default='',
                type='string', help='internal parameter')
optP.add_option('--panda_cacheSrvURL', action='store', dest='panda_cacheSrvURL', default='',
                type='string', help='internal parameter')
optP.add_option('--panda_runConfig', action='store', dest='panda_runConfig', default='',
                type='string', help='internal parameter')
optP.add_option('--panda_srcName', action='store', dest='panda_srcName', default='',
                type='string', help='internal parameter')
optP.add_option('--panda_inDS', action='store', dest='panda_inDS', default='',
                type='string', help='internal parameter')
optP.add_option('--panda_inDSForEP', action='store', dest='panda_inDSForEP', default='',
                type='string', help='internal parameter')
optP.add_option('--panda_origFullExecString', action='store', dest='panda_origFullExecString', default='',
                type='string', help='internal parameter')
optP.add_option('--panda_jobsetID',action='store',dest='panda_jobsetID',default=-1,
                type='int', help='internal parameter for jobsetID')
optP.add_option('--panda_parentJobsetID',action='store',dest='panda_parentJobsetID',default=-1,
                type='int', help='internal parameter for jobsetID')
optP.add_option('--panda_dbRelease', action='store', dest='panda_dbRelease', default='',
                type='string', help='internal parameter')
optP.add_option('--panda_singleLine', action='store', dest='panda_singleLine', default='',
                type='string', help='internal parameter')
optP.add_option('--panda_trf', action='store', dest='panda_trf', default='',
                type='string', help='internal parameter')
optP.add_option('--panda_eventPickRunEvtDat', action='store', dest='panda_eventPickRunEvtDat', default='',
                type='string', help='internal parameter')
optP.add_option('--panda_devidedByGUID',action='store_const',const=True,dest='panda_devidedByGUID',default=False,
                help='internal parameter')
optP.add_option('--panda_suppressMsg',action='store_const',const=True,dest='panda_suppressMsg',default=False,
                help='internal parameter')
optP.add_option('--panda_fullPathJobOs',action='store', dest='panda_fullPathJobOs', default='',
                type='string', help='internal parameter')
optP.add_option('--panda_tagParentFile',action='store', dest='panda_tagParentFile', default='',
                type='string', help='internal parameter')

# parse options
options,args = optP.parse_args()
if options.verbose:
    print options
    print

# display version
from pandatools import PandaToolsPkgInfo
if options.version:
    print "Version: %s" % PandaToolsPkgInfo.release_version
    sys.exit(0)

from pandatools import Client
from pandatools import PsubUtils
from pandatools import AthenaUtils
from pandatools import GlobalConfig
from pandatools import AppConfig
from pandatools import MiscUtils 
from pandatools import PLogger

# update panda-client
if options.update:
    res = PsubUtils.updatePackage(options.verbose)
    if res:
	sys.exit(0)
    else:
	sys.exit(1)

# full execution string
fullExecString = PsubUtils.convSysArgv()

# max size per job
maxTotalSize = Client.maxTotalSize
safetySize   = Client.safetySize

# set grid source file
globalConf = GlobalConfig.getConfig()
if globalConf.grid_src != '' and not os.environ.has_key('PATHENA_GRID_SETUP_SH'):
    os.environ['PATHENA_GRID_SETUP_SH'] = globalConf.grid_src

# get logger
tmpLog = PLogger.getPandaLogger()

# set default
appConf = AppConfig.AppConfig('pathena')
for tmpAppConfKey,tmpAppConfVal in appConf.getConfig().iteritems():
    # check lower characters just in case
    tmpAppConfKeys = [tmpAppConfKey,tmpAppConfKey.lower()]
    for tmpKey in tmpAppConfKeys:
        if hasattr(options,tmpKey):
            tmpSetAttFlag = False
	    if getattr(options,tmpKey) in [-1,0,None,'','AUTO',False]:
		setattr(options,tmpKey,tmpAppConfVal)
		tmpSetAttFlag = True
            elif getattr(options,tmpKey) == []:
                tmpSetValue = tmpAppConfVal.split(',')
                if '' in tmpSetValue:
                    tmpSetValue.remove('')
                setattr(options,tmpKey,tmpSetValue)
                tmpSetAttFlag = True
            if tmpSetAttFlag:
                # append parameter to metadata
                if isinstance(tmpAppConfVal,types.BooleanType):
                    fullExecString += ' --%s' % tmpKey
                else:
                    fullExecString += ' --%s=%s' % (tmpKey,tmpAppConfVal)
                if options.verbose:
                    tmpLog.debug("Use default option in panda.cfg %s=%s" % (tmpKey,getattr(options,tmpKey)))
            break

# use dev server
if options.devSrv:
    Client.useDevServer()

# set server
if options.panda_srvURL != '':
    Client.setServer(options.panda_srvURL)
if options.panda_cacheSrvURL != '':
    Client.setCacheServer(options.panda_cacheSrvURL)

# version check
PsubUtils.checkPandaClientVer(options.verbose)

# use old trf parameterization
if options.useOldTRF:
    options.useNewTRF = False

# noCompile uses noBuild stuff
if options.noCompile:
    if options.nobuild:
        tmpLog.error("--noBuild and --noCompile cannot be used simultaneously")
        sys.exit(EC_Config)
    options.nobuild = True

# files to be deleted
delFilesOnExit = []
                            
# suffix for shadow dataset
suffixShadow = Client.suffixShadow

# save current dir
currentDir = os.path.realpath(os.getcwd())

brokerageLogs  = []
userBrokerLogs = []

# exclude sites
if options.excludedSite != []:
    Client.excludeSite(options.excludedSite,options.panda_origFullExecString,userBrokerLogs)

# use certain sites
useRandomCloud = []
if re.search(',',options.site) != None:
    options.site,useRandomCloud = Client.useCertainSites(options.site)

# site specified
siteSpecified = True
if options.site == 'AUTO':
    siteSpecified = False

# cloud specified
if options.cloud != defaultCloud:
    # add logging info
    userBrokerLogs = PsubUtils.getUserBrokerageInfo(options.cloud,'cloud',userBrokerLogs)

# use outputPath as outDS
if Client.isDQ2free(options.site):
    if options.outDS != '':
	options.outputPath = options.outDS
    else:
	options.outputPath = './'
    options.outDS = options.outputPath
else:
    # enforce to use output dataset container
    if not options.useOldStyleOutput and not options.outDS.endswith('/'):
        options.outDS = options.outDS + '/'

# keep original outDS
original_outDS_Name = options.outDS
     
# reset crossSite unless container is used for output 
if not original_outDS_Name.endswith('/'):
    options.crossSite = 0
    options.panda_jobsetID = None

usingContainerForOut = original_outDS_Name.endswith('/')

# read datasets from file
if options.inDsTxt != '':
    options.inDS = PsubUtils.readDsFromFile(options.inDsTxt)

# set inDS for recursive goodRunListXML
orig_inDS = options.inDS
if options.panda_inDS != '':
    options.inDS = options.panda_inDS
    options.goodRunListXML = ''

# disable expiring file check
if options.useShortLivedReplicas:
    Client.useExpiringFiles()
    
# use staged dataset for event picking
if options.eventPickStagedDS != '':
    if options.panda_inDSForEP == '':
        options.panda_inDSForEP = options.eventPickStagedDS
        options.panda_eventPickRunEvtDat = '%s/ep_%s.dat' % (currentDir,MiscUtils.wrappedUuidGen())
        evI = open(options.eventPickEvtList)
        evO = open(options.panda_eventPickRunEvtDat,'w')
        evO.write(evI.read())
        # close
        evI.close()
        evO.close()
        # add to be deleted on exit
        delFilesOnExit.append(options.panda_eventPickRunEvtDat)


# set inDS for recursive event picking
orig_eventPickEvtList = options.eventPickEvtList
eventPickRunEvtDat = ''
if options.panda_inDSForEP != '':
    options.inDS = options.panda_inDSForEP
    options.eventPickEvtList = ''
    eventPickRunEvtDat = options.panda_eventPickRunEvtDat

# error
if options.outDS == '':
    tmpLog.error("no outDS is given\n pathena [--inDS input] --outDS output myJobO.py")
    sys.exit(EC_Config)
if options.split < -1 :
    tmpLog.error("Number of jobs should be a positive integer")
    sys.exit(EC_Config)
if options.shipinput and options.inDS != '' and options.pfnList != '':
    tmpLog.error("--shipInput, --pfnList and --inDS cannot be used at the same time")
    sys.exit(EC_Config)

# libDS
libds_file = '%s/libds_pathena.dat' % os.environ['PANDA_CONFIG_ROOT']
if options.libDS == 'LAST':
    if not os.path.exists(libds_file):
        tmpLog.error("LAST cannot be used until you submit at least one job without --libDS")
        sys.exit(EC_Config)
    # read line
    tmpFile = open(libds_file)
    tmpLibDS = tmpFile.readline()
    tmpFile.close()
    # remove \n
    tmpLibDS = tmpLibDS.replace('\n','')
    # set
    options.libDS = tmpLibDS

# absolute path for PFN list
if options.pfnList != '':
    options.pfnList = os.path.realpath(options.pfnList)

# burst submission
if options.burstSubmit != '':
    # don't scan LRC/LFC
    options.skipScan = True
    # reset cloud/site. They will be overwritten at submission
    options.cloud = None
    options.site  = None
    # disable individual output
    options.individualOutDS = False
    # check libDS stuff
    if options.libDS != '' or (options.nobuild and not options.noCompile):
        tmpLog.error("--libDS or --nobuild cannot be used together with --burstSubmit")
        sys.exit(EC_Config)
        

# split options are mutually exclusive
if (options.nFilesPerJob > 0 and options.nEventsPerJob > 0 and options.nGBPerJob != -1):
    tmpLog.error("split by files and split by events and split by file size can not be used simultaneously")
    sys.exit(EC_Config)

# split options are mutually exclusive
if (options.nEventsPerJob > 0 and options.nGBPerJob != -1):
    tmpLog.error("split by events and split by file size can not be used simultaneously")
    sys.exit(EC_Config)

# check nGBPerJob
if options.nGBPerJob != -1:
    # convert to int
    try:
        if options.nGBPerJob != 'MAX':
            options.nGBPerJob = int(options.nGBPerJob)
    except:
        tmpLog.error("nGBPerJob must be an integer or MAX")
        sys.exit(EC_Config)
    # check negative    
    if options.nGBPerJob <= 0:
        tmpLog.error("nGBPerJob must be positive")
        sys.exit(EC_Config)
    # convert MAX to default max value
    if options.nGBPerJob == 'MAX':
        options.nGBPerJob = maxTotalSize
    else:
        # convert to Bytes
        options.nGBPerJob = long(int(options.nGBPerJob)*1024*1024*1024)
        # reset max size per job
        maxTotalSize = options.nGBPerJob
    
# trf parameter
if options.panda_trf != '':
    options.trf = urllib.unquote(options.panda_trf)
if options.trf == False:
    orig_trfStr = ''
else:
    orig_trfStr = options.trf

# one liner
if options.panda_singleLine != '':
    options.singleLine = urllib.unquote(options.panda_singleLine)

# AMI event-level split
if options.useAMIEventLevelSplit == None:
    if options.inDS.startswith('data') or options.goodRunListXML != '':
        # use AMI for real data since the number of events per file is not uniform
        options.useAMIEventLevelSplit = True
    else:
        options.useAMIEventLevelSplit = False

# check DBRelease
if options.dbRelease != '' and (options.dbRelease.find(':') == -1 and options.dbRelease !='LATEST'):
    tmpLog.error("invalid argument for --dbRelease. Must be DatasetName:FileName or LATEST")  
    sys.exit(EC_Config)

# Good Run List
if options.goodRunListXML != '' and options.inDS != '':
    tmpLog.error("cannnot use --goodRunListXML and --inDS at the same time")
    sys.exit(EC_Config)

# event picking
if options.eventPickEvtList != '' and options.inDS != '':
    tmpLog.error("cannnot use --eventPickEvtList and --inDS at the same time")
    sys.exit(EC_Config)

# param check for event picking
if options.eventPickEvtList != '':
    if options.eventPickDataType == '':
        tmpLog.error("--eventPickDataType must be specified")
        sys.exit(EC_Config)
    if options.trf != False:
        tmpLog.error("--eventPickEvtList doesn't work with --trf until official transformations support event picking")
        sys.exit(EC_Config)
        
    
# additinal files
options.extFile = options.extFile.split(',')
try:
    options.extFile.remove('')
except:
    pass
options.extOutFile = re.sub(' ','',options.extOutFile)
options.extOutFile = options.extOutFile.split(',')
try:
    options.extOutFile.remove('')
except:
    pass

# user-specified merging script
if options.mergeScript != '':
    # enable merging
    options.mergeOutput = True
    # add it to extFile
    if not options.mergeScript in options.extFile:
        options.extFile.append(options.mergeScript)

# removed datasets
if options.removedDS == '':
    options.removedDS = []
else:
    options.removedDS = options.removedDS.split(',')
    
# glue packages
options.gluePackages = options.gluePackages.split(',')
try:
    options.gluePackages.remove('')
except:
    pass

# set excludeFile
if options.excludeFile != '':
    AthenaUtils.setExcludeFile(options.excludeFile)

# mapping for extra stream names
if options.appendStrToExtStream:
    AthenaUtils.enableExtendedExtStreamName()

# set ara on when ares is used
if options.ares:
    options.ara = True

# output files for ARA
if options.ara and options.araOutFile == '':
    tmpLog.error("--araOutFile is needed when ARA (--ara) is used")
    sys.exit(EC_Config)
for tmpName in options.araOutFile.split(','):
    if tmpName != '':
        options.extOutFile.append(tmpName)

# file list
tmpList = options.filelist.split(',')
options.filelist = []
for tmpItem in tmpList:
    if tmpItem == '':
        continue
    # wild card
    tmpItem = tmpItem.replace('*','.*')
    # append
    options.filelist.append(tmpItem) 
# read file list from file
if options.inputFileList != '':
    rFile = open(options.inputFileList)
    for line in rFile:
        line = re.sub('\n','',line)
        line = line.strip()
        if line != '':
            options.filelist.append(line)
    rFile.close()

# removed files
if options.removeFileList == '':
    # empty
    options.removeFileList = []
else:
    # read from file
    rList = []
    rFile = open(options.removeFileList)
    for line in rFile:
        line = re.sub('\n','',line)        
        rList.append(line)
    rFile.close()
    options.removeFileList = rList

# file type
options.inputType = options.inputType.split(',')
try:
    options.inputType.remove('')
except:
    pass

# suppressed streams
options.supStream = options.supStream.upper().split(',')
try:
    options.supStream.remove('')
except:
    pass

# set nFilesPerJob for MC data
if options.mcData != '':
    options.nFilesPerJob = 1
    
# set nfiles
if options.nFilesPerJob > 0 and options.nfiles == 0 and options.split > 0:
    options.nfiles = options.nFilesPerJob * options.split

# check grid-proxy
gridPassPhrase,vomsFQAN = PsubUtils.checkGridProxy('',False,options.verbose,options.vomsRoles)

# add allowed sites# 
if (not siteSpecified) and options.burstSubmit == '':
    tmpSt = Client.addAllowedSites(options.verbose)
    if not tmpSt:
        tmpLog.error("Failed to get allowed site list")
        sys.exit(EC_Config)

# set cloud according to country FQAN
expCloudFlag = False
if options.cloud == None and options.burstSubmit == '':
    options.cloud = PsubUtils.getCloudUsingFQAN(defaultCloud,options.verbose,useRandomCloud)
elif options.cloud != None:
    # use cloud explicitly
    expCloudFlag = True

# correct site
if options.site != 'AUTO' and options.burstSubmit == '':
    origSite = options.site
    # patch for BNL
    if options.site in ['BNL',"ANALY_BNL"]:
        options.site = "ANALY_BNL_SHORT"
    # patch for CERN
    if options.site in ['CERN']:
        options.site = "ANALY_CERN_XROOTD"        
    # try to convert DQ2ID to PandaID
    pID = PsubUtils.convertDQ2toPandaID(options.site)
    if pID != '':
        options.site = pID
    # add ANALY
    if not options.site.startswith('ANALY_'):
        options.site = 'ANALY_%s' % options.site
    # check
    if not Client.PandaSites.has_key(options.site):
        tmpLog.error("unknown siteID:%s" % origSite)
        sys.exit(EC_Config)
    # add logging info
    userBrokerLogs = PsubUtils.getUserBrokerageInfo(options.site,'site',userBrokerLogs)
    # set cloud
    options.cloud = Client.PandaSites[options.site]['cloud']

# check cloud
if options.burstSubmit == '':
    foundCloud = False
    for tmpID,spec in Client.PandaSites.iteritems():
        if options.cloud == spec['cloud']:
            foundCloud = True
            break
    if not foundCloud:
        tmpLog.error("unsupported cloud:%s" % options.cloud)
        sys.exit(EC_Config)

# get DN
distinguishedName = PsubUtils.getDN()

# get nickname
nickName = PsubUtils.getNickname()

if nickName == '':
    sys.exit(EC_Config)

# set Rucio accounting
PsubUtils.setRucioAccount(nickName,'pathena',True)

# check outDS format
if not PsubUtils.checkOutDsName(options.outDS,distinguishedName,options.official,nickName,
                                options.site,vomsFQAN,options.mergeOutput):
    tmpLog.error("invalid output datasetname:%s" % options.outDS)
    sys.exit(EC_Config)

# check destSE
if options.destSE != '':
    if not PsubUtils.checkDestSE(options.destSE,options.outDS,options.verbose):
        sys.exit(EC_Config)

# convert in/outTarBall to full path
if options.inTarBall != '':
    options.inTarBall = os.path.abspath(os.path.expanduser(options.inTarBall))
if options.outTarBall != '':
    options.outTarBall = os.path.abspath(os.path.expanduser(options.outTarBall))

# convert n/outRunConfig to full path
if options.inRunConfig != '':
    options.inRunConfig = os.path.abspath(os.path.expanduser(options.inRunConfig))
if options.outRunConfig != '':
    options.outRunConfig = os.path.abspath(os.path.expanduser(options.outRunConfig))

# check maxCpuCount 
if options.maxCpuCount > Client.maxCpuCountLimit:
    tmpLog.error("too large maxCpuCount. Must be less than %s" % Client.maxCpuCountLimit)
    sys.exit(EC_Config)
    
# give warning for maxCpuCount
# FIXME
#PsubUtils.giveWarningForMaxCpuCount(defaultMaxCpuCount,options.maxCpuCount,tmpLog)

# create tmp dir
if options.tmpDir == '':
    tmpDir = '%s/%s' % (currentDir,MiscUtils.wrappedUuidGen())
else:
    tmpDir = '%s/%s' % (os.path.abspath(options.tmpDir),MiscUtils.wrappedUuidGen())    
os.makedirs(tmpDir)

# set tmp dir in Client
Client.setGlobalTmpDir(tmpDir)

# exit action
def _onExit(dir,files):
    for tmpFile in files:
        commands.getoutput('rm -rf %s' % tmpFile)        
    commands.getoutput('rm -rf %s' % dir)
atexit.register(_onExit,tmpDir,delFilesOnExit)


# get Athena versions
stA,retA = AthenaUtils.getAthenaVer()
# failed
if not stA:
    sys.exit(EC_CMT)
workArea  = retA['workArea'] 
athenaVer = retA['athenaVer'] 
groupArea = retA['groupArea'] 
cacheVer  = retA['cacheVer'] 
nightVer  = retA['nightVer']

# overwrite with athenaTag
if options.athenaTag != '':
    athenaVer = ''
    cacheVer  = ''
    nightVer  = ''
    # get list of Athena projects
    listProjects = Client.getCachePrefixes(options.verbose)
    items = options.athenaTag.split(',')
    usingNightlies = False
    for item in items:
        # releases
        match = re.search('^(\d+\.\d+\.\d+)',item)
        if match != None:
            athenaVer = match.group(1)
            # cache
	    cmatch = re.search('^(\d+\.\d+\.\d+\.\d+\.*\d*)$',item)
	    if cmatch != None:
		cacheVer += '_%s' % cmatch.group(1)
        else:
            # nightlies
            match = re.search('^(\d+\.\d+\.X|\d+\.X\.\d+)$',item)
            if match != None:
                athenaVer = 'Atlas-%s' % match.group(1)
        # project
        if item.startswith('Atlas') or item in listProjects:
            # ignore AtlasOffline
            if item in ['AtlasOffline']:
                continue
            cacheVer = '-'+item+cacheVer
        # nightlies    
        if item.startswith('rel_'):
            usingNightlies = True
            if 'dev' in items:
                athenaVer = 'Atlas-dev'
            elif 'devval' in items:
                athenaVer = 'Atlas-devval'
            cacheVer  = '-AtlasOffline_%s' % item
	# CMTCONFIG
        if item == '64':
            options.cmtConfig = 'x86_64-slc5-gcc43-opt'
        if item == '32':
            options.cmtConfig = 'i686-slc5-gcc43-opt'
    # check cache
    if re.search('^-.+_.+$',cacheVer) == None:
        if re.search('^_\d+\.\d+\.\d+\.\d+$',cacheVer) != None:
            # use AtlasProduction
            cacheVer = '-AtlasProduction'+cacheVer
        else:
            # unknown
            cacheVer = ''
    # use dev nightlies
    if usingNightlies and athenaVer == '':
        athenaVer = 'Atlas-dev'

# set CMTCONFIG
options.cmtConfig = AthenaUtils.getCmtConfig(athenaVer,cacheVer,nightVer,options.cmtConfig,verbose=options.verbose)

# check CMTCONFIG
if not AthenaUtils.checkCmtConfig(retA['cmtConfig'],options.cmtConfig,options.nobuild):
    sys.exit(EC_CMT)

tmpLog.info('using CMTCONFIG=%s' % options.cmtConfig)

# get run directory
# remove special characters                    
sString=re.sub('[\+]','.',workArea)
runDir = re.sub('^%s' % sString, '', currentDir)
if runDir == currentDir:
    errMsg  = "You need to run pathena in a directory under %s. " % workArea
    errMsg += "If '%s' is a read-only directory, perhaps you did setup Athena without --testarea or the 'here' tag of asetup." % workArea
    tmpLog.error(errMsg)
    sys.exit(EC_Config)
elif runDir == '':
    runDir = '.'
elif runDir.startswith('/'):
    runDir = runDir[1:]
runDir = runDir+'/'

# check unmerge dataset
PsubUtils.checkUnmergedDataset(options.inDS,options.parentDS)

# good run list
if options.goodRunListXML != '':
    # look for pyAMI
    status,options.inDS,tmpAmiFileList = AthenaUtils.convertGoodRunListXMLtoDS(options.goodRunListXML,
                                                                               options.goodRunDataType,
                                                                               options.goodRunProdStep,
                                                                               options.goodRunListDS,
                                                                               options.verbose)
    if not status:
        tmpLog.error("failed to convert GoodRunListXML")
        sys.exit(EC_Config)
    if options.inDS == '':
        tmpLog.error("no datasets were extracted from AMI using %s" % options.goodRunListXML)
        sys.exit(EC_Config)
    if options.filelist == []:    
        options.filelist = tmpAmiFileList
        
# event picking
if options.eventPickEvtList != '':
    # convert run/evt list to dataset/LFN list
    epDsLFNs,epGuidEvtMap = PsubUtils.getDSsFilesByRunsEvents(currentDir,
                                                              options.eventPickEvtList,
                                                              options.eventPickDataType,
                                                              options.eventPickStreamName,
                                                              options.eventPickDS,
                                                              options.verbose,
                                                              options.eventPickAmiTag)
    # set param
    options.inDS = ''
    options.filelist = []
    tmpDsNameList = []
    tmpLFNList = []    
    for tmpGUID,tmpDsLFNs in epDsLFNs.iteritems():
        tmpDsName,tmpLFN = tmpDsLFNs
        # set filelist
        if not tmpLFN in tmpLFNList:
            tmpLFNList.append(tmpLFN)
            options.filelist.append(tmpLFN)
        # set inDS    
        if not tmpDsName in tmpDsNameList:
            tmpDsNameList.append(tmpDsName)
            options.inDS += '%s,' % tmpDsName
    options.inDS = options.inDS[:-1]
    # make run/event list
    eventPickRunEvtDat = '%s/ep_%s.dat' % (currentDir,MiscUtils.wrappedUuidGen())
    evFH = open(eventPickRunEvtDat,'w') 
    for tmpGUID,tmpRunEvtList in epGuidEvtMap.iteritems():
        for tmpRunNr,rmpEvtNr in tmpRunEvtList:
            evFH.write('%s %s\n' % (tmpRunNr,rmpEvtNr))
    # close        
    evFH.close()
    # add to be deleted on exit
    delFilesOnExit.append(eventPickRunEvtDat)
    
# get job options
jobO = ''
if options.trf:
    # replace : to = for backward compatibility
    for optArg in ['DB','RNDM']:
        options.trf = re.sub('%'+optArg+':','%'+optArg+'=',options.trf)
    # use trf's parameters
    jobO = options.trf
else:
    # get jobOs from command-line
    if options.preConfig != '':
        jobO += '-p %s ' % options.preConfig
    if options.singleLine != '':
        options.singleLine = options.singleLine.replace('"','\'')
        jobO += '-c "%s" ' % options.singleLine
    for arg in args:
        jobO += ' %s' % arg
if jobO == "":
    tmpLog.error("no jobOptions is given\n   pathena [--inDS input] --outDS output myJobO.py")
    sys.exit(EC_Config)

# ARA uses trf I/F
if options.ara:
    if options.ares:
        jobO = "athena.py " + jobO        
    elif jobO.endswith(".C"):
        jobO = "root -l " + jobO
    else:
        jobO = "python " + jobO        
    options.trf = jobO


if options.panda_runConfig == '' and options.inRunConfig == '':
    # extract run configuration    
    tmpLog.info('extracting run configuration')
    # run ConfigExtractor for normal jobO 
    ret,runConfig = AthenaUtils.extractRunConfig(jobO,options.supStream,options.useAIDA,options.shipinput,
                                                 options.trf,verbose=options.verbose,
						 useAMI=options.useAMIAutoConf,inDS=options.inDS,
						 tmpDir=tmpDir)
    # save runconfig
    if options.outRunConfig != '':
        cFile = open(options.outRunConfig,'w')
        pickle.dump(runConfig,cFile)
        cFile.close()
else:
    # use a saved file
    if options.panda_runConfig == '':
        options.panda_runConfig = options.inRunConfig
    # load from file
    ret = True
    tmpRunConfFile = open(options.panda_runConfig)
    runConfig = pickle.load(tmpRunConfFile)
    tmpRunConfFile.close()
if not options.trf:
    # extractor failed
    if not ret:
        sys.exit(EC_Extractor)
    # shipped files
    if runConfig.other.inputFiles:
        for fileName in runConfig.other.inputFiles:
            # append .root for tag files
            if runConfig.other.inColl:
                match = re.search('\.root(\.\d+)*$',fileName)
                if match == None:
                    fileName = '%s.root' % fileName
            # check ship files in the current dir
            if not os.path.exists(fileName):
                tmpLog.error("%s needs exist in the current directory when --shipInput is used" % fileName)
                sys.exit(EC_Extractor)
            # append to extFile
            options.extFile.append(fileName)
            if not runConfig.input.shipFiles:
                runConfig.input['shipFiles'] = []
            if not fileName in runConfig.input['shipFiles']:    
                runConfig.input['shipFiles'].append(fileName)
    # generator files
    if runConfig.other.rndmGenFile:
        # append to extFile
        for fileName in runConfig.other.rndmGenFile:
            options.extFile.append(fileName)
    # Condition file
    if runConfig.other.condInput:
        # append to extFile
        for fileName in runConfig.other.condInput:
            if options.addPoolFC == "":
                options.addPoolFC = fileName
            else:
                options.addPoolFC += ",%s" % fileName
    # set default ref name
    if not runConfig.input.collRefName:
        runConfig.input.collRefName = 'Token'
    # check dupication in extOutFile
    if runConfig.output.alloutputs != False:
        if options.verbose:
            tmpLog.debug("output files : %s" % str(runConfig.output.alloutputs)) 
        for tmpExtOutFile in tuple(options.extOutFile):
            if tmpExtOutFile in runConfig.output.alloutputs:
                if not options.panda_suppressMsg:
                    tmpLog.warning("removed %s from extOutFile since it is automatically extracted from Athena. You don't need to specify it in extOutFile"
                                   % tmpExtOutFile)
                options.extOutFile.remove(tmpExtOutFile)
else:
    # parse parameters for trf
    # AMI tag
    newJobO = ''
    for tmpString in jobO.split(';'):
        match = re.search(' AMI=',tmpString)
        if match == None:
            # use original command
            newJobO += (tmpString + ';')
        else:
            tmpLog.info('getting configration from AMI')
            # get configration using GetCommand.py
            com = 'GetCommand.py ' + re.sub('^[^ ]+ ','',tmpString.strip())
            if options.verbose:
                tmpLog.debug(com)
            amiSt,amiOut = commands.getstatusoutput(com)
            amiSt %= 255
            if amiSt != 0:
                tmpLog.error(amiOut)
                errSt =  'Failed to get configuration from AMI. '
                errSt += 'Using AMI=tag in --trf is disallowed since it may overload the AMI server. '
                errSt += 'Please use explicit configuration parameters in --trf'
                tmpLog.error(errSt)
                sys.exit(EC_Config)
            # get full command string
            fullCommand = ''
            for amiStr in amiOut.split('\n'):
                if amiStr != '' and not amiStr.startswith('#') and not amiStr.startswith('*'):
                    fullCommand = amiStr
            # failed to extract configration        
            if fullCommand == '':
                tmpLog.error(amiOut)
                errSt =  "Failed to extract configuration from AMI's output"
                tmpLog.error(errSt)
                sys.exit(EC_Config)
            # replace
            newJobO += (fullCommand + ';')
    # remove redundant ;
    newJobO = newJobO[:-1]
    # replace
    if newJobO != '':
        jobO = newJobO
        if options.verbose:
            tmpLog.debug('new jobO : '+jobO)
    # output                
    oneOut = False
    # replace ; for job sequence
    tmpString = re.sub(';',' ',jobO)
    # look for %OUT
    for tmpItem in tmpString.split():
        match = re.search('\%OUT\.([^ \"\',]+)',tmpItem)
        if match:
            # append basenames to extOutFile
            tmpOutName = match.group(1)
            if not tmpOutName in options.extOutFile:
                options.extOutFile.append(tmpOutName)
                oneOut = True
    # warning if no output
    if not oneOut:
        if not options.ara:
            tmpLog.warning("argument of --trf doesn't contain any %OUT")

# no output jobs
tmpOutKeys = runConfig.output.keys()
for tmpIgnorKey in ['outUserData','alloutputs']:
    try:
        tmpOutKeys.remove(tmpIgnorKey)
    except:
        pass
if tmpOutKeys == [] and options.extOutFile == [] and not options.noOutput:
    errStr  = "No output stream was extracted from jobOs or --trf. "
    if not options.trf:
	errStr += "If your job defines an output without Athena framework "
	errStr += "(e.g., using ROOT.TFile.Open instead of THistSvc) "
	errStr += "please specify the output filename by using --extOutFile. "
	errStr += "Or if you define the output with a relatively new mechanism "
	errStr += "please report it to Savannah to update the automatic extractor. " 
    errStr += "If you are sure that your job doesn't produce any output file "
    errStr += "(e.g., HelloWorldOptions.py) please use --noOutput. " 
    tmpLog.error(errStr)  
    sys.exit(EC_Extractor)

# set extOutFile to runConfig
if options.extOutFile != []:
    runConfig.output['extOutFile'] = options.extOutFile

# check ship files in the current dir
if not runConfig.input.shipFiles:
    runConfig.input.shipFiles = []
for file in runConfig.input.shipFiles:
    if not os.path.exists(file):
        tmpLog.error("%s needs exist in the current directory when using --shipInput" % file)
        sys.exit(EC_Extractor)

# get random number
runConfig.other['rndmNumbers'] = []
if not runConfig.other.rndmStream:
    runConfig.other.rndmStream = []
if len(runConfig.other.rndmStream) != 0:
    if options.norandom:
        print
        print "Initial random seeds need to be defined."
        print "Enter two numbers for each random stream."
        print "  e.g., PYTHIA : 4789899 989240512"
        print
    for stream in runConfig.other.rndmStream:
        if options.norandom:
            # enter manually
            while True:
                randStr = raw_input("%s : " % stream)
                num = randStr.split()
                if len(num) == 2:
                    break
                print " Two numbers are needed"
            runConfig.other.rndmNumbers.append([int(num[0]),int(num[1])])
        else:
            # automatic
            runConfig.other.rndmNumbers.append([random.randint(1,5000000),random.randint(1,5000000)])
    if options.norandom:
        print
if runConfig.other.G4RandomSeeds == True:
    if options.norandom:
        print
        print "Initial G4 random seeds need to be defined."
        print "Enter one positive number."
        print
        # enter manually
        while True:
            num = raw_input("SimFlags.SeedsG4=")
            try:
                num = int(num)
                if num > 0:
                    runConfig.other.G4RandomSeeds = num
                    break
            except:
                pass
        print    
    else:
        # automatic
        runConfig.other.G4RandomSeeds = random.randint(1,10000)
else:
    # set -1 to disable G4 Random Seeds
    runConfig.other.G4RandomSeeds = -1

# RefName and Query for TAG
if options.trf and AthenaUtils.checkUseTagInTrf(jobO,options.useTagInTRF):
    if options.tagStreamRef == '':
        tmpLog.error("--tagStreamRef is required when you use TAG in --trf")
        sys.exit(EC_Extractor)
    runConfig.input.collRefName = options.tagStreamRef
    if not options.tagStreamRef.endswith('_ref'):
        runConfig.input.collRefName += '_ref'
    # query
    if options.tagQuery != '':
        runConfig.input.tagQuery = options.tagQuery

#####################################################################
# archive sources and send it to HTTP-reachable location

if options.panda_srcName != '':
    # reuse src
    if options.verbose:
        tmpLog.debug('reuse source files')
    archiveName = options.panda_srcName
    # go to tmp dir
    os.chdir(tmpDir)
    # set jobOs with fullpath
    if options.panda_fullPathJobOs != '':
        AthenaUtils.fullPathJobOs = AthenaUtils.convStrToFullPathJobOs(options.panda_fullPathJobOs)
else:
    if options.inTarBall == '':
        # extract jobOs with full pathnames
        for tmpItem in jobO.split():
            if re.search('^/.*\.py$',tmpItem) != None:
                # set random name to avoid overwriting
                tmpName = tmpItem.split('/')[-1]
                tmpName = '%s_%s' % (MiscUtils.wrappedUuidGen(),tmpName)
                # set
                AthenaUtils.fullPathJobOs[tmpItem] = tmpName

        # copy some athena specific files
        AthenaUtils.copyAthenaStuff(currentDir)

        # set extFile
        AthenaUtils.setExtFile(options.extFile)

        archiveName = ""
        if options.libDS == '' and not (options.nobuild and not options.noCompile):
            # archive sources
            archiveName,archiveFullName = AthenaUtils.archiveSourceFiles(workArea,runDir,currentDir,tmpDir,
                                                                         options.verbose,options.gluePackages) 
        else:
            # archive jobO
            archiveName,archiveFullName = AthenaUtils.archiveJobOFiles(workArea,runDir,currentDir,
                                                                       tmpDir,options.verbose)

        # archive InstallArea
        if options.libDS == '':
            AthenaUtils.archiveInstallArea(workArea,groupArea,archiveName,archiveFullName,
                                           tmpDir,options.nobuild,options.verbose)

        # back to tmp dir        
        os.chdir(tmpDir)

        # remove some athena specific files
        AthenaUtils.deleteAthenaStuff(currentDir)

        # compress
        status,out = commands.getstatusoutput('gzip %s' % archiveName)
        archiveName += '.gz'
        if status != 0 or options.verbose:
            print out

        # check archive
        status,out = commands.getstatusoutput('ls -l %s' % archiveName)
        if status != 0:
            print out
            tmpLog.error("Failed to archive working area.\n        If you see 'Disk quota exceeded', try '--tmpDir /tmp'") 
            sys.exit(EC_Archive)

        # check symlinks
        tmpLog.info("checking symbolic links")
        status,out = commands.getstatusoutput('tar tvfz %s' % archiveName)
        if status != 0:
            tmpLog.error("Failed to expand archive")
            sys.exit(EC_Archive)
        symlinks = []    
        for line in out.split('\n'):
            items = line.split()
            if items[0].startswith('l') and items[-1].startswith('/'):
                symlinks.append(line)
        if symlinks != []:
            tmpStr  = "Found some unresolved symlinks which may cause a problem\n"
            tmpStr += "     See, e.g., http://savannah.cern.ch/bugs/?43885\n"
            tmpStr += "   Please ignore if you believe they are harmless"
            tmpLog.warning(tmpStr)
            for symlink in symlinks:
                print "  %s" % symlink
    else:
        # go to tmp dir
        os.chdir(tmpDir)
        # use a saved copy
        if options.libDS == '' and not (options.nobuild and not options.noCompile):
            archiveName     = 'sources.%s.tar' % MiscUtils.wrappedUuidGen()
            archiveFullName = "%s/%s" % (tmpDir,archiveName)
        else:
            archiveName     = 'jobO.%s.tar' % MiscUtils.wrappedUuidGen()
            archiveFullName = "%s/%s" % (tmpDir,archiveName)
        # make copy to avoid name duplication
        shutil.copy(options.inTarBall,archiveFullName)
        
    # save
    if options.outTarBall != '':
        shutil.copy(archiveName,options.outTarBall)

    # put sources/jobO via HTTP POST
    if not options.nosubmit:
        tmpLog.info("uploading source/jobO files")
        status,out = Client.putFile(archiveName,options.verbose,useCacheSrv=True,reuseSandbox=True)
	if out.startswith('NewFileName:'):
	    # found the same input sandbox to reuse 
	    archiveName = out.split(':')[-1]
        elif out != 'True':
            # failed
            print out
            tmpLog.error("Failed with %s" % status)
            sys.exit(EC_Post)

####################################################################
# datasets 

# override if %DB is specified in --trf
if options.dbRelease == 'LATEST' and re.search('%DB=[^ :]+:[^ ]',jobO) != None:
    options.dbRelease = ''
    
# get the latest of DBRelease
if options.dbRelease == 'LATEST' or "%DB=LATEST" in jobO:
    if options.panda_dbRelease == '':
        options.dbRelease = Client.getLatestDBRelease(options.verbose)
        options.panda_dbRelease = options.dbRelease
    else:
        options.dbRelease = options.panda_dbRelease
    # replace
    if "%DB=LATEST" in jobO:
        jobO = jobO.replace("%DB=LATEST","%DB=" + options.dbRelease) 

# check if output dataset is unique
outputDSexist = False
outputContExist = False
outputIndvDSlist = {}
if not Client.isDQ2free(options.site):
    tmpDatasets = Client.getDatasets(options.outDS,options.verbose)
    if len(tmpDatasets) != 0:
        if original_outDS_Name.endswith('/'):
            outputContExist = True
        else:
            outputDSexist = True
        # get real datasetname for case sensitivity    
        options.outDS = PsubUtils.getRealDatasetName(options.outDS,tmpDatasets)
# get exsisting individualOutDS
if (outputDSexist or outputContExist) and options.individualOutDS:
    outputIndvDSlist = Client.getDatasets("%s*" % re.sub('/$','',options.outDS),options.verbose,True)    

# check if shadow dataset exists
shadowDSexist = False
"""
# disable shadow stuff
if not Client.isDQ2free(options.site):
    tmpDatasets = Client.getDatasets("%s%s" % (options.outDS,suffixShadow),options.verbose)
    if len(tmpDatasets) != 0:
        shadowDSexist = True
"""

# set location when outDS or libDS already exists
checkLibDS = True
if outputDSexist and options.destSE == '':
    checkLibDS = False
    if options.verbose:
        tmpLog.debug("get locations for outDS:%s" % options.outDS)
    outDSlocations = Client.getLocations(options.outDS,[],options.cloud,True,options.verbose)
    if outDSlocations == []:
        tmpLog.error("cannot find locations for existing output dataset:%s" % options.outDS)
        sys.exit(EC_Dataset)
    # keep origianl site
    if Client.PandaSites.has_key(options.site) and \
       (Client.PandaSites[options.site]['ddm'] in outDSlocations or \
        Client.convSrmV2ID(Client.PandaSites[options.site]['ddm']) in outDSlocations):
        pass
    else:
        # convert DQ2ID to Panda siteID
	tmpConvIDs = []
        for outDSlocation in outDSlocations:
            tmpConvIDs += Client.convertDQ2toPandaIDList(outDSlocation)
        # not found
        if tmpConvIDs == []:
	    checkLibDS = True
	    options.destSE = outDSlocations[0]
	    tmpLog.info("set destSE:%s because outDS:%s already exists there" % \
		(options.destSE,options.outDS))
	# chose one
	if len(tmpConvIDs) == 1:
	    convID = tmpConvIDs[0]
        else:
            # run brokerage to be sent to free site
            convID,brokerageLogs = PsubUtils.runBrokerageForCompSite(tmpConvIDs,'Atlas-%s' % athenaVer,cacheVer,
                                                                     options.verbose,options.cmtConfig,
                                                                     memorySize=options.memory,
                                                                     useDirectIO=options.useDirectIOSites,
                                                                     siteGroup=options.useSiteGroup,
                                                                     maxCpuCount=options.maxCpuCount)
        # set    
        options.site  = convID
        options.cloud = Client.PandaSites[convID]['cloud']
        tmpLog.info("set site:%s cloud:%s because outDS:%s already exists at %s" % \
                    (options.site,options.cloud,options.outDS,outDSlocations))
        # add logging info
        userBrokerLogs = PsubUtils.getUserBrokerageInfo(options.site,'outDS',userBrokerLogs)

if options.libDS != '' and not Client.isDQ2free(options.site):
    # get real datasetname for case sensitivity    
    tmpDatasets = Client.getDatasets(options.libDS,options.verbose)
    options.libDS = PsubUtils.getRealDatasetName(options.libDS,tmpDatasets)
    # get location for libDS
    if options.verbose:
        tmpLog.debug("get locations for libDS:%s" % options.libDS)
    libDSlocations = Client.getLocations(options.libDS,[],options.cloud,True,options.verbose)
    if libDSlocations == []:
        tmpLog.error("cannot find locations for existing lib dataset:%s" % options.libDS)
        sys.exit(EC_Dataset)
    # check consistency with outDS location    
    if not checkLibDS:
        PsubUtils.checkLocationConsistency(outDSlocations,libDSlocations)
    # keep origianl site
    if Client.PandaSites.has_key(options.site) and \
       (Client.PandaSites[options.site]['ddm'] in libDSlocations or \
        Client.convSrmV2ID(Client.PandaSites[options.site]['ddm']) in libDSlocations):
        pass
    else:
        # convert DQ2ID to Panda siteID
	tmpConvIDs = []
        for libDSlocation in libDSlocations:
            tmpConvIDs += Client.convertDQ2toPandaIDList(libDSlocation)
        # not found
        if tmpConvIDs == []:
            tmpLog.error("cannot find supported sites for existing lib datasete:%s" % options.libDS)
            sys.exit(EC_Dataset)
	# chose one
	if len(tmpConvIDs) == 1:
	    convID = tmpConvIDs[0]
        else:
            # run brokerage to be sent to free site
            convID,brokerageLogs = PsubUtils.runBrokerageForCompSite(tmpConvIDs,'Atlas-%s' % athenaVer,cacheVer,
                                                                     options.verbose,options.cmtConfig,
                                                                     memorySize=options.memory,
                                                                     useDirectIO=options.useDirectIOSites,
                                                                     siteGroup=options.useSiteGroup,
                                                                     maxCpuCount=options.maxCpuCount)
        # set    
        options.site  = convID
        options.cloud = Client.PandaSites[convID]['cloud']
        tmpLog.info("set site:%s cloud:%s because libDS:%s exists at %s" % \
                    (options.site,options.cloud,options.libDS,libDSlocations))
        # add logging info
        userBrokerLogs = PsubUtils.getUserBrokerageInfo(options.site,'libDS',userBrokerLogs)

# get parent datasets for TAG
useTagParentLookup = False
tagParentInfo = {}
parentLfnToTagMap = {}
if options.panda_tagParentFile != '':
    useTagParentLookup = True
    # load parent info
    tagParentInfo,parentLfnToTagMap = PsubUtils.loadTagParentInfo(options.panda_tagParentFile)
elif (runConfig.input.inColl or (options.trf and AthenaUtils.checkUseTagInTrf(jobO,options.useTagInTRF))) \
         and options.parentDS == '' and options.inDS != '' and not options.shipinput \
       and not options.notUseTagLookup:
    useTagParentLookup = True
    # get parent info from TAG DB
    tagParentInfo,parentLfnToTagMap = PsubUtils.getTagParentInfoUsingTagQuery(options.inDS,runConfig.input.tagQuery,
                                                                                 runConfig.input.collRefName,options.verbose)
    
# set TAG datasets to parentDS to reuse the framework
if useTagParentLookup:
    # set parent datasets to inDS since job splitting uses parent file boundary
    tmpItems = tagParentInfo.keys()
    tmpItems.sort()
    options.inDS = ''
    newFileList = []
    for tmpItem in tmpItems:
        # check when input files are specified
        if options.filelist != []:
            matchInput = False
            for tmpOptInFileName in options.filelist:
                if tagParentInfo[tmpItem]['tagToParentLFNmap'].has_key(tmpOptInFileName):
                    matchInput = True
                    for tmpParentLFN in tagParentInfo[tmpItem]['tagToParentLFNmap'][tmpOptInFileName]:
                        if not tmpParentLFN in newFileList:
                            newFileList.append(tmpParentLFN)
            if not matchInput:
                continue
        else:
            for tmpParentLFNs in tagParentInfo[tmpItem]['tagToParentLFNmap'].values():
                for tmpParentLFN in tmpParentLFNs:
                    if not tmpParentLFN in newFileList:
                        newFileList.append(tmpParentLFN)
        options.inDS += '%s,' % tmpItem
    options.inDS = options.inDS[:-1]
    # set input file list
    options.filelist = newFileList
    tmpTagDsList = []
    options.parentDS = ''
    for tmpParentDS,tmpTagInfo in tagParentInfo.iteritems():
        if not tmpTagInfo['tagElementDS'] in tmpTagDsList:
            tmpTagDsList.append(tmpTagInfo['tagElementDS'])
            options.parentDS += '%s,' % tmpTagInfo['tagElementDS']
    options.parentDS = options.parentDS[:-1]
    # get file map in TAG datasets
    tagFilesMap = Client.queryFilesInDataset(options.parentDS,options.verbose)
    
    
# parent datasets
parentDSlocations = None
if options.parentDS != '':
    parentDSlocations = Client.getLocations(options.parentDS,[],options.cloud,False,options.verbose,
                                            getDQ2IDs=True,useOutContainer=usingContainerForOut)
    if parentDSlocations == []:
        tmpLog.error("could not find replica locations for %s" % options.parentDS)
        sys.exit(EC_Dataset)


# get DB datasets
dbrFiles  = {}
dbrDsList = []
dbrDsSize = 0
if options.trf or options.dbRelease != '':
    if options.trf:
        # parse jobO for TRF
        tmpItems = jobO.split()
    else:
	tmpItems = []
    if options.dbRelease != '':    
        # mimic a trf parameter to reuse following algorithm
        tmpItems += ['%DB='+options.dbRelease]
    # look for DBRelease
    for tmpItem in tmpItems:
        match = re.search('%DB=([^:]+):(.+)$',tmpItem)
        if match:
            tmpDbrDS  = match.group(1)
            tmpDbrLFN = match.group(2)
            # get files in the dataset
            if not tmpDbrDS in dbrDsList:
                tmpVerboseFlag = options.crossSite in [0,maxCrossSite] or options.verbose
                if tmpVerboseFlag and not options.panda_suppressMsg:
                    tmpLog.info("query files in %s" % tmpDbrDS)
                tmpList = Client.queryFilesInDataset(tmpDbrDS,options.verbose)
                # append
                for tmpLFN,tmpVal in tmpList.iteritems():
                    dbrFiles[tmpLFN] = tmpVal
                dbrDsList.append(tmpDbrDS)
            # check
            if not dbrFiles.has_key(tmpDbrLFN):
                tmpLog.error("%s is not in %s" % (tmpDbrLFN,tmpDbrDS))
		sys.exit(EC_Dataset)
            # DBR size
            dbrDsSize += long(dbrFiles[tmpDbrLFN]['fsize'])	


# input datasets
inputGroupFilesMap = {}
amiFileMetadataMap = {}
tagFileList = {}
tagToParentMap = {}
if options.inDS != '' or options.shipinput or options.pfnList != '':
    # minimum bias dataset
    if options.trf and jobO.find('%MININ') != -1:
        runConfig.input.inMinBias = True
    # set dataset name    
    if runConfig.input.inMinBias:
        if options.minDS == "":
            # read from stdin   
            print
            print "This job uses Minimum-Bias stream"
            while True:
                minbiasDataset = raw_input("Enter dataset name for Minimum Bias : ")
                minbiasDataset = minbiasDataset.strip()
                if minbiasDataset != "":
                    break
        else:
            minbiasDataset = options.minDS
    # low pT minimum bias dataset
    if options.trf and jobO.find('%LOMBIN') != -1:
        runConfig.input.inLoMinBias = True
    # set dataset name    
    if runConfig.input.inLoMinBias:
        if options.lowMinDS == "":
            # read from stdin   
            print
            print "This job uses Low pt Minimum-Bias stream"
            while True:
                options.lowMinDS = raw_input("Enter dataset name for Low pT Minimum Bias : ")
                options.lowMinDS = options.lowMinDS.strip()
                if options.lowMinDS != "":
                    break
    # high pT minimum bias dataset
    if options.trf and jobO.find('%HIMBIN') != -1:
        runConfig.input.inHiMinBias = True
    # set dataset name    
    if runConfig.input.inHiMinBias:
        if options.highMinDS == "":
            # read from stdin   
            print
            print "This job uses High pt Minimum-Bias stream"
            while True:
                options.highMinDS = raw_input("Enter dataset name for High pT Minimum Bias : ")
                options.highMinDS = options.highMinDS.strip()
                if options.highMinDS != "":
                    break
    # cavern dataset
    if options.trf and jobO.find('%CAVIN') != -1:
        runConfig.input.inCavern = True
    if runConfig.input.inCavern:
        if options.cavDS == "":
            # read from stdin                  
            print
            print "This job uses Cavern stream"
            while True:
                cavernDataset = raw_input("Enter dataset name for Cavern : ")
                cavernDataset = cavernDataset.strip()
                if cavernDataset != "":
                    break
        else:
            cavernDataset = options.cavDS
    # beam halo dataset
    if options.trf and jobO.find('%BHIN') != -1:
        runConfig.input.inBeamHalo = True 
    if runConfig.input.inBeamHalo:
	# use common DS
	if options.useCommonHalo:
	    if options.beamHaloDS == "":
                # read from stdin                  
                print
                print "This job uses BeamHalo stream"
                while True:
                    beamHaloDataset = raw_input("Enter dataset name for BeamHalo : ")
                    beamHaloDataset = beamHaloDataset.strip()
                    if beamHaloDataset != "":
                        break
            else:
                beamHaloDataset = options.beamHaloDS
	else:	
            # get DS for A-side        
            if options.beamHaloADS == "":
                # read from stdin                  
                print
                print "This job uses BeamHalo stream"
                while True:
                    beamHaloAdataset = raw_input("Enter dataset name for BeamHalo A-side : ")
                    beamHaloAdataset = beamHaloAdataset.strip()
                    if beamHaloAdataset != "":
                        break
            else:
                beamHaloAdataset = options.beamHaloADS
            # get DS for C-side
            if options.beamHaloCDS == "":
                # read from stdin                  
                while True:
                    beamHaloCdataset = raw_input("Enter dataset name for BeamHalo C-side : ")
                    beamHaloCdataset = beamHaloCdataset.strip()
                    if beamHaloCdataset != "":
                        break
            else:
                beamHaloCdataset = options.beamHaloCDS
    # beam gas dataset
    if options.trf and jobO.find('%BGIN') != -1:
        runConfig.input.inBeamGas = True  
    if runConfig.input.inBeamGas: 
	# use common DS
	if options.useCommonGas:
            # get BeamGas DS
            if options.beamGasDS == "":
                # read from stdin                  
                print
                print "This job uses BeamGas stream"
                while True:
                    beamGasDataset = raw_input("Enter dataset name for BeamGas : ")
                    beamGasDataset = beamGasDataset.strip()
                    if beamGasDataset != "":
                        break
            else:
                beamGasDataset = options.beamGasDS
        else:
            # get DS for H
            if options.beamGasHDS == "":
                # read from stdin                  
                print
                print "This job uses BeamGas stream"
                while True:
                    beamGasHdataset = raw_input("Enter dataset name for BeamGas Hydrogen : ")
                    beamGasHdataset = beamGasHdataset.strip()
                    if beamGasHdataset != "":
                        break
            else:
                beamGasHdataset = options.beamGasHDS
            # get DS for C
            if options.beamGasCDS == "":
                # read from stdin                  
                while True:
                    beamGasCdataset = raw_input("Enter dataset name for BeamGas Carbon : ")
                    beamGasCdataset = beamGasCdataset.strip()
                    if beamGasCdataset != "":
                        break
            else:
                beamGasCdataset = options.beamGasCDS
            # get DS for O
            if options.beamGasODS == "":
                # read from stdin                  
                while True:
                    beamGasOdataset = raw_input("Enter dataset name for BeamGas Oxygen : ")
                    beamGasOdataset = beamGasOdataset.strip()
                    if beamGasOdataset != "":
                        break
            else:
                beamGasOdataset = options.beamGasODS
    # input dataset        
    if options.inDS != '':
        # query files in shadow dataset. this list is used mainly to reduce the number of LFC lookup
        shadowList = []
        if shadowDSexist and not outputContExist:
	    shadowList = Client.getFilesInShadowDatasetOld(options.outDS,suffixShadow,options.verbose)
        elif outputContExist:
            shadowList = Client.getFilesInShadowDataset(original_outDS_Name,suffixShadow,options.verbose)
        # query files in dataset
        if options.crossSite in [0,maxCrossSite] or options.verbose:
            tmpLog.info("query files in %s" % options.inDS)
        else:
            tmpLog.info("query files with DQ2")
        inputFileMap,inputDsString = Client.queryFilesInDataset(options.inDS,options.verbose,getDsString=True)
        # remove files
        for tmpKey in options.removeFileList:
            if inputFileMap.has_key(tmpKey):
                del inputFileMap[tmpKey]
        # separate normal input filelist and wildcard list
        normalInPatList   = []
        wildCardInPatList = []
        for pattern in options.filelist:
            if re.search('\*',pattern) != None:
                wildCardInPatList.append(pattern)
            else:
                normalInPatList.append(pattern)
        # check normal input filelist since doing the next loop is time consuming
        if normalInPatList != []:
            tmpInputFileMap = {}
            for tmpPat in normalInPatList:
                if inputFileMap.has_key(tmpPat):
                    tmpInputFileMap[tmpPat] = inputFileMap[tmpPat]
            inputFileMap = tmpInputFileMap        
        # remove log, and check matching
        for fileName in inputFileMap.keys():
            # ignore log file
            if re.search('\.log(\.\d+)*(\.tgz)*$',fileName) != None or \
                   re.search('\.log(\.tgz)*(\.\d+)*$',fileName) != None:
                del inputFileMap[fileName]
                continue
            # check type
            if options.inputType != []:
                matchType = False
                for tmpType in options.inputType:
                    if tmpType in fileName:
                        matchType = True
                        break
                if not matchType:
                    del inputFileMap[fileName]                    
                    continue
            # filename matching with wild card       
            if wildCardInPatList != []:
                # check matching    
                matchFlag = False
                for pattern in wildCardInPatList:
                    # wildcard matching
                    if re.search(pattern,fileName) != None:
                        matchFlag = True
                        break
                # doesn't match
                if not matchFlag:
                    del inputFileMap[fileName]
        # no files in filelist are available
        if options.filelist != [] and inputFileMap == {}:
            if options.inputFileList != '':
                errStr =  "No files in %s are available in %s. " % (options.inputFileList,options.inDS)
            else:
                errStr =  "%s are not available in %s. " % (options.filelist,options.inDS)
            errStr += "Make sure that you specify correct LFNs"
            tmpLog.error(errStr)
            sys.exit(EC_Config)
        # skip files
        tmpFileKeys = inputFileMap.keys()
        tmpFileKeys.sort()
        if options.nSkipFiles > len(tmpFileKeys):
            tmpStr  = "the number of files in %s is less than nSkipFiles. " % options.inDS
            tmpStr += " N of files=%s : nSkipFiles=%s" % (len(tmpFileKeys),options.nSkipFiles)
            tmpLog.error(tmpStr)
            sys.exit(EC_Dataset)
        for tmpKey in tmpFileKeys[:options.nSkipFiles]:
            if inputFileMap.has_key(tmpKey):
                del inputFileMap[tmpKey]
        # empty
        if inputFileMap == {}:
            errStr = "No files were found in %s (log.tgz was ignored). " % options.inDS
            if options.inputType != []:
                errStr += "Make sure that you specify correct types in --inputType"
            else:
		errStr += "The dataset is empty"
            tmpLog.error(errStr)
            sys.exit(EC_Config)
        # check if all files were used
        unUsedFlag = False
        for tmpLFN in inputFileMap.keys():
            if not tmpLFN in shadowList:
                unUsedFlag = True
                break
        if not unUsedFlag:
            # exit
            tmpMsg  = "Done. No jobs to be submitted since all input files were (or are being) processed for the outDS. "
            tmpMsg += "The intention is to avoid duplication in the outDS. " 
            tmpMsg += "If you need to run again on the input dataset please use new outDS"
            tmpLog.info(tmpMsg)
            sys.exit(0)
        # get the number of events per file from AMI
        if options.nEventsPerJob > 0 and options.nEventsPerFile == 0 and options.useAMIEventLevelSplit:
            if options.crossSite in [0,maxCrossSite] or options.verbose:
                tmpLog.info("getting metadata for %s from AMI" % options.inDS)
            else:
                tmpLog.info("getting metadata from AMI")
            if inputDsString == '':
                # normal dataset
                amiFileMetadataMap = AthenaUtils.listFilesUsingAMI(options.inDS,options.verbose)
            else:
                # wildcard is used
                amiFileMetadataMap = AthenaUtils.listFilesUsingAMI(inputDsString,options.verbose)
        # get locations when site==AUTO
        newUsedDsList = []
        if options.site == "AUTO":
            if inputDsString == '':
                # not using wildcard
                inputDsString = options.inDS
            # take MinBias/Cavern datasets into account
            secondaryDsStrList = []
            if runConfig.input.inMinBias:
                secondaryDsStrList.append(minbiasDataset)
            if runConfig.input.inLoMinBias:
                secondaryDsStrList.append(options.lowMinDS)
            if runConfig.input.inHiMinBias:
                secondaryDsStrList.append(options.highMinDS)
            if runConfig.input.inCavern:
                secondaryDsStrList.append(cavernDataset)
            # take beamgas/halo datasets into account
            if runConfig.input.inBeamHalo:
                if options.useCommonHalo:
                    secondaryDsStrList.append(beamHaloDataset)
                else:
                    secondaryDsStrList.append(beamHaloAdataset)
                    secondaryDsStrList.append(beamHaloCdataset)
            if runConfig.input.inBeamGas:
                if options.useCommonGas:
                    secondaryDsStrList.append(beamGasDataset)
                else:
                    secondaryDsStrList.append(beamGasHdataset)
                    secondaryDsStrList.append(beamGasCdataset)
                    secondaryDsStrList.append(beamGasOdataset)
            # DBRelease
            dbrDsString = ''
            if options.trf or options.dbRelease != '':
                dbrDsList = []
                if options.trf:
                    # parse jobO for TRF
                    tmpItems = jobO.split()
                else:
                    tmpItems = []
                if options.dbRelease != '':
                    # mimic a trf parameter to reuse following algorithm
                    tmpItems += ['%DB='+options.dbRelease]
                # look for DBRelease
                for tmpItem in tmpItems:
                    match = re.search('%DB=([^:]+):(.+)$',tmpItem)
                    if match:
                        tmpDbrDS  = match.group(1)
                        if not tmpDbrDS in dbrDsList:
                            if dbrDsString == '':
                                dbrDsString = tmpDbrDS
                            else:
                                dbrDsString = dbrDsString + ',' + tmpDbrDS
                            dbrDsList.append(tmpDbrDS)
            # get location        
            dsLocationMap,dsLocationMapBack,dsTapeSites,dsUsedDsMap = Client.getLocations(inputDsString,inputFileMap,options.cloud,
                                                                                          False,options.verbose,
                                                                                          expCloud=expCloudFlag,getReserved=True,
                                                                                          getTapeSites=True,
                                                                                          locCandidates=parentDSlocations,
                                                                                          removeDS=True,
                                                                                          removedDatasets=options.removedDS,
                                                                                          useOutContainer=usingContainerForOut)
            # get locations for DBR
            dbrMissingSites = []
            if dbrDsString != '':
                dbrDsLocationMap,dbrDsLocationMapBack,dbrDsTapeSites = Client.getLocations(dbrDsString,{},options.cloud,
                                                                                           False,options.verbose,
                                                                                           expCloud=expCloudFlag,getReserved=True,
                                                                                           getTapeSites=True,
                                                                                           locCandidates=parentDSlocations,
                                                                                           useCVMFS=True)
                # remove if DBR is unavailable
                tmpSiteKeys = dsLocationMap.keys()
                for tmpSite in tmpSiteKeys:
                    if not tmpSite in dbrDsLocationMap and not tmpSite in dbrDsLocationMapBack:
                        for tmpItem in dsLocationMap[tmpSite]:
                            if not tmpItem in dbrMissingSites:
                                dbrMissingSites.append(tmpItem)
                        del dsLocationMap[tmpSite]
                tmpSiteKeys = dsLocationMapBack.keys()
                for tmpSite in tmpSiteKeys:
                    if not tmpSite in dbrDsLocationMap and not tmpSite in dbrDsLocationMapBack:
                        for tmpItem in dsLocationMap[tmpSite]:
                            if not tmpItem in dbrMissingSites:
                                dbrMissingSites.append(tmpItem)
                        del dsLocationMapBack[tmpSite]
            # get location for secondary DS
            secMissingSites = {}
            if secondaryDsStrList != []:
                for secDsString in secondaryDsStrList:
                    secMissingSites[secDsString] = []
                    secDsLocationMap,secDsLocationMapBack,secDsTapeSites = Client.getLocations(secDsString,{},options.cloud,
                                                                                               False,options.verbose,
                                                                                               expCloud=expCloudFlag,getReserved=True,
                                                                                               getTapeSites=True,
                                                                                               locCandidates=parentDSlocations)
                    # remove if DBR is unavailable
                    tmpSiteKeys = dsLocationMap.keys()
                    for tmpSite in tmpSiteKeys:
                        if not tmpSite in secDsLocationMap and not tmpSite in secDsLocationMapBack:
                            for tmpItem in dsLocationMap[tmpSite]:
                                if not tmpItem in secMissingSites[secDsString]:
                                    secMissingSites[secDsString].append(tmpItem)
                            del dsLocationMap[tmpSite]
                    tmpSiteKeys = dsLocationMapBack.keys()
                    for tmpSite in tmpSiteKeys:
                        if not tmpSite in secDsLocationMap and not tmpSite in secDsLocationMapBack:
                            for tmpItem in dsLocationMap[tmpSite]:
                                if not tmpItem in secMissingSites[secDsString]:
                                    secMissingSites[secDsString].append(tmpItem)
                            del dsLocationMapBack[tmpSite]
	    # no location
            if dsLocationMap == {} and dsLocationMapBack == {}:
		if dsTapeSites != {}:
		    if orig_eventPickEvtList == '':
			errStr = "Tape sites hold datasets as follows:\n\n"
			for tmpKey,tmpVal in dsTapeSites.iteritems():
			    errStr += "  %s:%s\n" % (tmpKey,str(tmpVal))
			errStr += "\nPlease request subscription to disk area first if needed. "
			errStr += "This error could happen if your excluded sites have DISK replicas, "
			errStr += "or the sites where DISK replicas are available have been blacklisted by DDM (usually downtime) or HammerCloud, "
			errStr += "or there is no DISK replica."                        
			tmpLog.error(errStr)
                    elif options.eventPickStagedDS != '':
                        errStr = 'Some files in %s are still on TAPE. Please wait until you get a notification from DaTRI' % options.eventPickStagedDS
                        tmpLog.error(errStr)
                        sys.exit(EC_Dataset)
		    else:
                        # get command line parameters
                        if options.panda_origFullExecString == '':
                            epParams = fullExecString
                        else:
                            epParams = urllib.unquote(options.panda_origFullExecString)
                        # request event picking on the server side
                        tmpMsg  = 'Some input files are available only at TAPE sites. '
                        tmpMsg += 'Sending a request to stage-in those files to DISK'
                        print
                        tmpLog.warning(tmpMsg)
                        print
                        if not options.nosubmit:
			    epLockedBy = 'pathena'
                            epStat,epOutput = Client.requestEventPicking(eventPickRunEvtDat,
                                                                         options.eventPickDataType,
                                                                         options.eventPickStreamName,
                                                                         options.eventPickDS,
                                                                         options.eventPickAmiTag,
                                                                         options.filelist,
                                                                         '',
                                                                         options.outDS,
                                                                         epLockedBy,epParams,
                                                                         options.verbose)
                            tmpMsg  = 'The request has been registered. '
                            tmpMsg += "Those files will be contained in '%s' to be transferred to DISK via DaTRI. " % epOutput
                            tmpMsg += 'Once the transfer completed DaTRI will send a notification '
                            tmpMsg += 'and then you can use the dataset as an input by using --eventPickStagedDS=%s, i.e.,' % epOutput
                            tmpMsg += '\n\n%s %s --eventPickStagedDS=%s' % (epLockedBy,epParams,epOutput)
                            tmpLog.info(tmpMsg)
                        sys.exit(EC_Dataset)                            
                if dbrMissingSites != []:
                    tmpLog.error("%s is missing at %s although input dataset is available" % (dbrDsString,str(dbrMissingSites)))
                for secDsString,secMissingSites in secMissingSites.iteritems():
                    if secMissingSites != []:
                        tmpLog.error("%s is missing at %s although input dataset is available" % (secDsString,str(secMissingSites)))
                if options.crossSite in [0,maxCrossSite] or options.verbose:
                    if expCloudFlag:
                        errStr = "could not find supported/online sites in the %s cloud for %s" % (options.cloud,options.inDS)
                    else:
                        errStr = "could not find supported/online sites for %s" % options.inDS
                    if secondaryDsStrList != []:
                        for secondaryDsString in secondaryDsStrList:
                            errStr += ' and %s' % secondaryDsString
                    if options.parentDS != '':
                        errStr += ' and %s' % options.parentDS
                    if dbrDsString != '':
                        errStr += ' and %s' % dbrDsString
                    tmpLog.error(errStr)
                else:
                    errStr = "could not find supported/online sites for the missing files"
                    tmpLog.error(errStr)
                # print missing files
                if options.verbose:    
                    tmpMsgStr = '=== missing file list ===\n'
                    tmpInFileList = inputFileMap.keys()
                    tmpInFileList.sort()
                    for tmpFile in tmpInFileList:
                        tmpMsgStr += '%s\n' % tmpFile
                    tmpLog.debug(tmpMsgStr)    
                sys.exit(EC_Dataset)
            # run brorage
            tmpDsLocationMapList = [dsLocationMap]
            if dsLocationMapBack != {}:
                tmpDsLocationMapList.append(dsLocationMapBack)
            tmpBrokerErr = ''   
            for idxDsLocationMap,tmpDsLocationMap in enumerate(tmpDsLocationMapList):    
                tmpSites = []
                for tmpItems in tmpDsLocationMap.values():
                    for tmpItem in tmpItems:
                        # convert to long
                        if options.long:
                            tmpItem = Client.convertToLong(tmpItem)
                        tmpSites.append(tmpItem)
		if tmpSites == []:
                    continue
                status,outMap = Client.runBrokerage(tmpSites,'Atlas-%s' % athenaVer,verbose=options.verbose,trustIS=True,cacheVer=cacheVer,
                                                    loggingFlag=True,cmtConfig=options.cmtConfig,memorySize=options.memory,
                                                    useDirectIO=options.useDirectIOSites,siteGroup=options.useSiteGroup,
                                                    maxCpuCount=options.maxCpuCount)
                if status != 0:
                    tmpLog.error('failed to run brokerage for automatic assignment: %s' % outMap)
                    sys.exit(EC_Config)
                out = outMap['site']
                brokerageLogs = outMap['logInfo']
                if out.startswith('ERROR :'):
                    tmpCheckedDsList = PsubUtils.getDsListCheckedForBrokerage(dsUsedDsMap)
                    tmpLog.info('tried to find candidate sites for untouched datasets : %s' % tmpCheckedDsList)
		    if idxDsLocationMap == 0:
			tmpBrokerErr += out
                        if expCloudFlag:
                            tmpBrokerErr += " Could not find sites in the %s cloud.\n" % options.cloud
                        else:
                            tmpBrokerErr += " Could not find sites.\n"
		    else:
			if tmpBrokerErr != '':
			    tmpBrokerErr += out
			    tmpBrokerErr += " Could not find sites in other clouds.\n"
			else:
			    tmpBrokerErr += out
			    tmpBrokerErr += " Could not find sites.\n"
                    if idxDsLocationMap+1 >= len(tmpDsLocationMapList):
                        tmpLog.error('brokerage failed')
                        print tmpBrokerErr[:-1]
                        sys.exit(EC_Config)
                    continue    
                if not Client.PandaSites.has_key(out):
                    tmpLog.error('brokerage gave wrong PandaSiteID:%s' % out)
                    sys.exit(EC_Config)
                break    
	    # set site/cloud
            options.site  = out
            options.cloud = Client.PandaSites[options.site]['cloud']
            if options.verbose:
                tmpLog.debug("chosen site=%s" % options.site)
            # get expiring files
            expFilesMap = Client.getExpiringFiles(options.inDS,options.removedDS,options.site,options.verbose)
            # update the list of used datasets
            for tmpDsUsedDsMapKey,tmpDsUsedDsVal in dsUsedDsMap.iteritems():
                if options.site in [tmpDsUsedDsMapKey,Client.convertToLong(tmpDsUsedDsMapKey)]:
                    for tmpDsUsedDsValItem in tmpDsUsedDsVal:
                        # ignore expiring datasets
                        if not tmpDsUsedDsValItem in expFilesMap['datasets']:
                            options.removedDS.append(tmpDsUsedDsValItem)
                            newUsedDsList.append(tmpDsUsedDsValItem) 
                    break
        # reset max input size
        if not Client.PandaSites[options.site]['maxinputsize'] in [0,'']:
            maxTotalSize = Client.PandaSites[options.site]['maxinputsize'] * 1024*1024
        # scan local replica catalog
        dsLocation = Client.PandaSites[options.site]['ddm']
        if Client.getLFC(dsLocation) != None:
            # LFC
            if options.nfiles == 0:
                missList = Client.getMissLFNsFromLFC(inputFileMap,options.site,True,options.verbose,
                                                     dsStr=options.inDS,removedDS=options.removedDS,
                                                     skipScan=options.skipScan)
            else:
                missList = Client.getMissLFNsFromLFC(inputFileMap,options.site,True,options.verbose,
                                                     options.nfiles,shadowList,
                                                     dsStr=options.inDS,removedDS=options.removedDS,
                                                     skipScan=options.skipScan)
        elif Client.getLRC(dsLocation) != None:
            tmpLog.info("scanning LRC %s for %s" % (Client.getLRC(dsLocation),options.site))
            # LRC
            missList = Client.getMissLFNsFromLRC(inputFileMap,Client.getLRC(dsLocation),options.verbose,
                                                 options.nfiles)
        else:
            missList = []
        # remove datasets from removedDS if they are incomplete in LFC
        tmpInconDsList = Client.getInconsistentDS(missList,newUsedDsList)
        for tmpInconDS in tmpInconDsList:
            if tmpInconDS in options.removedDS:
                options.removedDS.remove(tmpInconDS)
        # choose min missList
        if options.verbose:
            tmpLog.debug("%s holds %s files" % (dsLocation,len(inputFileMap)-len(missList)))
        # check availability of TAG files
        if useTagParentLookup:
            tmpLog.info("checking files in %s" % orig_inDS)
            for tmpInputFileName,tmpInputFileVal in inputFileMap.iteritems():
                # ignore missing or shadow files
                if tmpInputFileName in missList or tmpInputFileName in shadowList:
                    continue
                # collect TAG files
                tmpTagFileName = parentLfnToTagMap[tmpInputFileName]['lfn']
                tagFileList[tmpTagFileName] = tagFilesMap[tmpTagFileName]
                # keep TAG to parent LFC map
                if not tagToParentMap.has_key(tmpTagFileName):
                    tagToParentMap[tmpTagFileName] = []
                if not tmpInputFileName in tagToParentMap[tmpTagFileName]:
                    tagToParentMap[tmpTagFileName].append(tmpInputFileName)
            # check with LFC
            tagMissFiles = Client.getMissLFNsFromLFC(tagFileList,options.site,True,options.verbose,
                                                     dsStr=options.parentDS,skipScan=options.skipScan)
            for tmpTagMissFile in tagMissFiles:
                # remove missing TAG files
                del tagFileList[tmpTagMissFile]
                # add parent files to missing list when corresponding TAG is unavailable
                for tmpParentFileName in tagToParentMap[tmpTagMissFile]:
                    if not tmpParentFileName in missList:
                        missList.append(tmpParentFileName)
        # No files available
        if len(inputFileMap) == len(missList) and shadowList == []:
            tmpStr = "No files available on disk at %s" % options.site
            if options.crossSite == 0:
                tmpLog.error(tmpStr)
                sys.exit(EC_Dataset)
            elif options.crossSite == 1:
                tmpLog.warning(tmpStr)
            else:
                tmpLog.info(tmpStr)
        # remove missing
        for fileName in inputFileMap.keys():
            # missing at the site
            if fileName in missList:
                del inputFileMap[fileName]                    
                continue
        # update shadow just in case since LFC scan may take long time
        if not options.skipScan:
            if shadowDSexist and not outputContExist:
		# using += just in case
                shadowList += Client.getFilesInShadowDatasetOld(options.outDS,suffixShadow,options.verbose)
            elif outputContExist:
		# using += just in case
                shadowList += Client.getFilesInShadowDataset(original_outDS_Name,suffixShadow,options.verbose)
        # remove shadow
        for fileName in shadowList:
            if inputFileMap.has_key(fileName):
                del inputFileMap[fileName]
        # no input
        if len(inputFileMap) == 0:
            tmpStr  = "all input files at %s had already been used or remaining files are unavailable on disk. " % options.site
	    tmpStr += "pathena runs on files which were failed or were not used in "
	    tmpStr += "previous submissions if it runs with the same inDS and outDS"
            if options.crossSite == 0:
		tmpLog.error(tmpStr)
		sys.exit(EC_Dataset)
            else:
                # go back to current dir
                os.chdir(currentDir)
                # try another site if input files remain
		options.crossSite -= 1
                if options.crossSite > 0 and options.inDS != '' and not siteSpecified:
                    if missList != []:
                        PsubUtils.runPathenaRec(runConfig,missList,tmpDir,fullExecString,options.nfiles,inputFileMap,
                                                options.site,options.crossSite,archiveName,options.removedDS,
                                                options.inDS,options.goodRunListXML,options.eventPickEvtList,
                                                options.panda_devidedByGUID,options.panda_dbRelease,
                                                options.panda_jobsetID,orig_trfStr,options.singleLine,
                                                True,eventPickRunEvtDat,useTagParentLookup,options.verbose)
                # exit        
                sys.exit(0)
        # make list
        inputFileList = inputFileMap.keys()
        inputFileList.sort()
    elif options.pfnList != '':
        # read PFNs from a file
        devidedByGUID = False 
        rFile = open(options.pfnList)
        inputFileList = []
        for line in rFile:
            line = re.sub('\n','',line)
            line.strip()
            if line != '' and not line.startswith('#'):
                inputFileList.append(line)
        rFile.close()
        inputFileList.sort()
        inputFileMap = {}
    else:
        # ship input files
        devidedByGUID = False 
        # extract GUIDs
        guidCollMap,guidCollList = AthenaUtils.getGUIDfromColl(athenaVer,runConfig.input.shipFiles,
                                                               currentDir,
                                                               runConfig.input.collRefName,
                                                               options.verbose)
        # if works
        if guidCollList != []:
            # use GUIDs for looping
            inputFileList = guidCollList
            # use GUID boundaries
            devidedByGUID = True
            # set inDS and missList to invoke runPathenaRec
            options.inDS = ''
            missList = []
            # set parentDS
            if options.parentDS == '':
                # get DSs using GUIDs
		tmpLog.info("extracting parent datasets from TAG")
                tmpRetMap,tmpAllMap = Client.listDatasetsByGUIDs(guidCollList,options.eventPickDS,options.verbose,True)
                for tmpGUID in guidCollList:
                    if not tmpRetMap.has_key(tmpGUID):
                        # no dataset for the GUID
                        errStr = "cannot get dataset for GUID=%s from DQ2. " % tmpGUID
                        if options.eventPickDS != '':
                            errStr += "--eventPickDS doesn't match with any of %s" % str(tmpAllMap[tmpGUID])
                        tmpLog.error(errStr)
                        sys.exit(EC_Dataset)
                    # get dataset name    
                    tmpCollDS,tmpCollLFN = tmpRetMap[tmpGUID]
                    # append
                    if not (tmpCollDS+',') in options.inDS:
                        options.inDS += (tmpCollDS+',')
                    if not tmpCollLFN in missList:
                        missList.append(tmpCollLFN)
                options.inDS = options.inDS[:-1]
                if options.inDS == '':
                    tmpLog.error("GUIDs in TAG cannot be converted to datasets. Please specify --parentDS")
                    sys.exit(EC_Dataset)                    
                else:
                    tmpLog.info("parent datasets : %s" % options.inDS)
            else:
                tmpLog.info("checking GUIDs in TAG and parentDS")
                # get files in parent datasets
                tmpParentFileList = Client.queryFilesInDataset(options.parentDS,options.verbose)
                # set parameters for recursive execution
                options.inDS = options.parentDS
                tmpParentGuidLLfnMap = {}
                for tmpParentLFN,tmpParentVal in tmpParentFileList.iteritems():
                    tmpParentGuidLLfnMap[tmpParentVal['guid']] = tmpParentLFN
                # get files 
                missList = []
                for tmpGUID in guidCollList:
                    if not tmpParentGuidLLfnMap.has_key(tmpGUID):
			errStr = "GUID=%s in TAG is not found in %s. Please check if you specify correct --parentDS" \
                                 % (tmpGUID,options.parentDS)
                        tmpLog.error(errStr)
                        sys.exit(EC_Dataset)
                    # append
                    if not tmpParentGuidLLfnMap[tmpGUID] in missList:
                        missList.append(tmpParentGuidLLfnMap[tmpGUID])
            # execute pathena            
            os.chdir(currentDir)
            # use devidedByGUID in invoked process
            options.panda_devidedByGUID = True
            # set a dummy string to set inDS in invoked process
            options.goodRunListXML = 'dummy'
            # invoke pathena
            PsubUtils.runPathenaRec(runConfig,missList,tmpDir,fullExecString,options.nfiles,{},
                                    options.site,options.crossSite,archiveName,options.removedDS,
                                    options.inDS,options.goodRunListXML,options.eventPickEvtList,
                                    options.panda_devidedByGUID,options.panda_dbRelease,
                                    options.panda_jobsetID,orig_trfStr,options.singleLine,
                                    False,eventPickRunEvtDat,useTagParentLookup,options.verbose)
            # exit        
            sys.exit(0)
        else:
            # use input collections for looping
            inputFileList = runConfig.input.shipFiles
    # set devidedByGUID which was originaly set by runPathenaRec
    if options.panda_devidedByGUID:
        devidedByGUID = True
    # use limited number of files
    if options.nfiles > 0:
        inputFileList = inputFileList[:options.nfiles]
    # group files by dataset
    if options.useContElementBoundary:
        tmpLog.info("checking boundaries of container elements")
        inputGroupFilesMap = PsubUtils.groupFilesByDataset(options.inDS,inputDsString,
                                                           inputFileList,options.verbose)
    # set # of events for shipInput
    if options.shipinput and options.nFilesPerJob == -1 and options.nEventsPerJob == -1:
	# non GUID boundaries
	if not devidedByGUID:
	    options.nEventsPerJob = 2000

    # input stream for minimum bias
    if runConfig.input.inMinBias:
        # query files in dataset
        tmpLog.info("query files in %s" % minbiasDataset)
        tmpList = Client.queryFilesInDataset(minbiasDataset,options.verbose)
	tmpMissList = Client.getMissLFNsFromLFC(tmpList,options.site,True,options.verbose,dsStr=minbiasDataset,
                                                skipScan=options.skipScan)
	minbiasList = []
        for item in tmpList.keys():
            # remove log
            if re.search('log(\.tgz)*(\.\d+)*$',item) != None or \
                   re.search('\.log(\.\d+)*(\.tgz)*$',item) != None:
                continue
            # remove missing files
            if item in tmpMissList:
                continue
            minbiasList.append((item,tmpList[item]))
        # sort
        minbiasList.sort()
        # randomize
        if options.randomMin:
            random.shuffle(minbiasList)
        # number of files per one signal
        if options.nMin < 0 and options.nMinPerJob < 0:
            while True:
                tmpStr = raw_input("Enter the number of Minimum-Bias files per one signal file : ")
                try:
                    options.nMin = int(tmpStr)
                    break
                except:
                    pass
        # check # of files
        if len(minbiasList) < max(options.nMin,options.nMinPerJob):
            if options.nMin > 0:
                tmpLog.error("only %s files in %s are available at %s which is less than nMin=%s" % \
                             (len(minbiasList),minbiasDataset,options.site,options.nMin))
            else:
                tmpLog.error("only %s files in %s are available at %s which is less than nMinPerJob=%s" % \
                             (len(minbiasList),minbiasDataset,options.site,options.nMinPerJob))
            sys.exit(EC_Dataset)
    # input stream for low pT minimum bias
    if runConfig.input.inLoMinBias:
        # query files in dataset
        tmpLog.info("query files in %s" % options.lowMinDS)
        tmpList = Client.queryFilesInDataset(options.lowMinDS,options.verbose)
	tmpMissList = Client.getMissLFNsFromLFC(tmpList,options.site,True,options.verbose,dsStr=options.lowMinDS,
                                                skipScan=options.skipScan)
	lowminbiasList = []
        for item in tmpList.keys():
            # remove log
            if re.search('log(\.tgz)*(\.\d+)*$',item) != None or \
                   re.search('\.log(\.\d+)*(\.tgz)*$',item) != None:
                continue
            # remove missing files
            if item in tmpMissList:
                continue
            lowminbiasList.append((item,tmpList[item]))
        # sort
        lowminbiasList.sort()
        # randomize
        if options.randomMin:
            random.shuffle(lowminbiasList)
        # number of files per one signal
        if options.nLowMin < 0 and options.nLowMinPerJob < 0:
            while True:
                tmpStr = raw_input("Enter the number of Low pT Minimum-Bias files per one signal file : ")
                try:
                    options.nLowMin = int(tmpStr)
                    break
                except:
                    pass
        # check # of files
        if len(lowminbiasList) < max(options.nLowMin,options.nLowMinPerJob):
            if options.nLowMin > 0:
                tmpLog.error("only %s files in %s are available at %s which is less than nLowMin=%s" % \
                             (len(lowminbiasList),options.lowMinDS,options.site,options.nLowMin))
            else:
                tmpLog.error("only %s files in %s are available at %s which is less than nLowMinPerJob=%s" % \
                             (len(lowminbiasList),options.lowMinDS,options.site,options.nLowMinPerJob))
            sys.exit(EC_Dataset)
    # input stream for high pT minimum bias
    if runConfig.input.inHiMinBias:
        # query files in dataset
        tmpLog.info("query files in %s" % options.highMinDS)
        tmpList = Client.queryFilesInDataset(options.highMinDS,options.verbose)
	tmpMissList = Client.getMissLFNsFromLFC(tmpList,options.site,True,options.verbose,dsStr=options.highMinDS,
                                                skipScan=options.skipScan)
	highminbiasList = []
        for item in tmpList.keys():
            # remove log
            if re.search('log(\.tgz)*(\.\d+)*$',item) != None or \
                   re.search('\.log(\.\d+)*(\.tgz)*$',item) != None:
                continue
            # remove missing files
            if item in tmpMissList:
                continue
            highminbiasList.append((item,tmpList[item]))
        # sort
        highminbiasList.sort()
        # randomize
        if options.randomMin:
            random.shuffle(highminbiasList)
        # number of files per one signal
        if options.nHighMin < 0 and options.nHighMinPerJob < 0:
            while True:
                tmpStr = raw_input("Enter the number of High pT Minimum-Bias files per one signal file : ")
                try:
                    options.nHighMin = int(tmpStr)
                    break
                except:
                    pass
        # check # of files
        if len(highminbiasList) < max(options.nHighMin,options.nHighMinPerJob):
            if options.nHighMin > 0:
                tmpLog.error("only %s files in %s are available at %s which is less than nHighMin=%s" % \
                             (len(highminbiasList),options.highMinDS,options.site,options.nHighMin))
            else:
                tmpLog.error("only %s files in %s are available at %s which is less than nHighMinPerJob=%s" % \
                             (len(highminbiasList),options.highMinDS,options.site,options.nHighMinPerJob))
            sys.exit(EC_Dataset)
    # input stream for cavern
    if runConfig.input.inCavern:
        # query files in dataset
        tmpLog.info("query files in %s" % cavernDataset)
        tmpList = Client.queryFilesInDataset(cavernDataset,options.verbose)
        tmpMissList = Client.getMissLFNsFromLFC(tmpList,options.site,True,options.verbose,dsStr=cavernDataset,
                                                skipScan=options.skipScan)
	cavernList = []
        for item in tmpList.keys():
            # remove log
            if re.search('\.log(\.\d+)*(\.tgz)*$',item) != None or \
                   re.search('\.log(\.tgz)*(\.\d+)*$',item) != None:
                continue
            # remove missing files
            if item in tmpMissList:
                continue
            cavernList.append((item,tmpList[item]))
        # sort
        cavernList.sort()
        # randomize
        if options.randomCav:
            random.shuffle(cavernList)
        # number of files per one signal
        if options.nCav < 0 and options.nCavPerJob < 0:
            while True:
                tmpStr = raw_input("Enter the number of Cavern files per one signal file : ")
                try:
                    options.nCav = int(tmpStr)
                    break
                except:
                    pass
        # check # of files
        if len(cavernList) < max(options.nCav,options.nCavPerJob):
            if options.nCav > 0:
                tmpLog.error("only %s files in %s are available at %s which is less than nCav=%s" % \
                             (len(cavernList),cavernDataset,options.site,options.nCav))
            else:
                tmpLog.error("only %s files in %s are available at %s which is less than nCavPerJob=%s" % \
                             (len(cavernList),cavernDataset,options.site,options.nCavPerJob))
            sys.exit(EC_Dataset)
    # input stream for beam halo
    if runConfig.input.inBeamHalo:
	# use common DS
	if options.useCommonHalo:
            # query files in dataset
            tmpLog.info("query files in %s" % beamHaloDataset)
            tmpList = Client.queryFilesInDataset(beamHaloDataset,options.verbose)
            tmpMissList = Client.getMissLFNsFromLFC(tmpList,options.site,True,options.verbose,dsStr=beamHaloDataset,
                                                    skipScan=options.skipScan)
	    beamHaloList = []
            for item in tmpList.keys():
                # remove log
                if re.search('\.log(\.\d+)*(\.tgz)*$',item) != None or \
                       re.search('\.log(\.tgz)*(\.\d+)*$',item) != None:
                    continue
                # remove missing files
                if item in tmpMissList:
                    continue
                beamHaloList.append((item,tmpList[item]))
            # sort
            beamHaloList.sort()
            # number of files per one sub job
            if options.nBeamHalo < 0:
                while True:
                    tmpStr = raw_input("Enter the number of BeamHalo files per one sub job : ")
                    try:
                        options.nBeamHalo = int(tmpStr)
                        break
                    except:
                        pass
            # check # of files
            if len(beamHaloList) < options.nBeamHalo:
                tmpLog.error("only %s files in %s are available at %s which is less than nBeamHalo=%s" % \
                             (len(beamHaloList),beamHaloDataset,options.site,options.nBeamHalo))                
                sys.exit(EC_Dataset)
	else:	
            # query files in dataset
            tmpLog.info("query files in %s" % beamHaloAdataset)
            tmpList = Client.queryFilesInDataset(beamHaloAdataset,options.verbose)
            tmpMissList = Client.getMissLFNsFromLFC(tmpList,options.site,True,options.verbose,dsStr=beamHaloAdataset,
                                                    skipScan=options.skipScan)
	    beamHaloAList = []
            for item in tmpList.keys():
                # remove log
                if re.search('\.log(\.\d+)*(\.tgz)*$',item) != None or \
                       re.search('\.log(\.tgz)*(\.\d+)*$',item) != None:
                    continue
                # remove missing files
                if item in tmpMissList:
                    continue
                beamHaloAList.append((item,tmpList[item]))
            tmpLog.info("query files in %s" % beamHaloCdataset)
            tmpList = Client.queryFilesInDataset(beamHaloCdataset,options.verbose)
            tmpMissList = Client.getMissLFNsFromLFC(tmpList,options.site,True,options.verbose,dsStr=beamHaloCdataset,
                                                    skipScan=options.skipScan)
	    beamHaloCList = []
            for item in tmpList.keys():
                # remove log
                if re.search('\.log(\.\d+)*(\.tgz)*$',item) != None or \
                       re.search('\.log(\.tgz)*(\.\d+)*$',item) != None:
                    continue
                # remove missing files
                if item in tmpMissList:
                    continue
                beamHaloCList.append((item,tmpList[item]))
            # sort
            beamHaloAList.sort()
            beamHaloCList.sort()
            # number of files per one sub job
            if options.nBeamHaloA < 0:
                while True:
                    tmpStr = raw_input("Enter the number of BeamHalo files for A-side per one sub job : ")
                    try:
                        options.nBeamHaloA = int(tmpStr)
                        break
                    except:
                        pass
            if options.nBeamHaloC < 0:
                # use default ratio
                options.nBeamHaloC = int(0.02/1.02*options.nBeamHaloA)
            # check # of files
            if len(beamHaloAList) < options.nBeamHaloA:
                tmpLog.error("only %s files in %s are available at %s which is less than nBeamHaloA=%s" % \
                             (len(beamHaloAList),beamHaloAdataset,options.site,options.nBeamHaloA))                
                sys.exit(EC_Dataset)
            if len(beamHaloCList) < options.nBeamHaloC:
                tmpLog.error("only %s files in %s are available at %s which is less than nBeamHaloC=%s" % \
                             (len(beamHaloCList),beamHaloCdataset,options.site,options.nBeamHaloC))
                sys.exit(EC_Dataset)
    # beam gas dataset
    if runConfig.input.inBeamGas: 
	# use common DS
	if options.useCommonGas:
            # query files in dataset
            tmpLog.info("query files in %s" % beamGasDataset)
            tmpList = Client.queryFilesInDataset(beamGasDataset,options.verbose)
            tmpMissList = Client.getMissLFNsFromLFC(tmpList,options.site,True,options.verbose,dsStr=beamGasDataset,
                                                    skipScan=options.skipScan)
	    beamGasList = []
            for item in tmpList.keys():
                # remove log
                if re.search('\.log(\.\d+)*(\.tgz)*$',item) != None or \
                       re.search('\.log(\.tgz)*(\.\d+)*$',item) != None:
                    continue
                # remove missing files
                if item in tmpMissList:
                    continue
                beamGasList.append((item,tmpList[item]))
            # sort
            beamGasList.sort()
            # number of files per one sub job
            if options.nBeamGas < 0:
                while True:
                    tmpStr = raw_input("Enter the number of BeamGas files per one sub job : ")
                    try:
                        options.nBeamGas = int(tmpStr)
                        break
                    except:
                        pass
            # check # of files
            if len(beamGasList) < options.nBeamGas:
                tmpLog.error("only %s files in %s are available at %s which is less than nBeamGas=%s" % \
                             (len(beamGasList),beamGasDataset,options.site,options.nBeamGas))
                sys.exit(EC_Dataset)
        else:
            # query files in dataset
            tmpLog.info("query files in %s" % beamGasHdataset)
            tmpList = Client.queryFilesInDataset(beamGasHdataset,options.verbose)
            tmpMissList = Client.getMissLFNsFromLFC(tmpList,options.site,True,options.verbose,dsStr=beamGasHdataset,
                                                    skipScan=options.skipScan)
	    beamGasHList = []
            for item in tmpList.keys():
                # remove log
                if re.search('\.log(\.\d+)*(\.tgz)*$',item) != None or \
                       re.search('\.log(\.tgz)*(\.\d+)*$',item) != None:
                    continue
                # remove missing files
                if item in tmpMissList:
                    continue
                beamGasHList.append((item,tmpList[item]))
            tmpLog.info("query files in %s" % beamGasCdataset)
            tmpList = Client.queryFilesInDataset(beamGasCdataset,options.verbose)
            tmpMissList = Client.getMissLFNsFromLFC(tmpList,options.site,True,options.verbose,dsStr=beamGasCdataset,
                                                    skipScan=options.skipScan)
	    beamGasCList = []
            for item in tmpList.keys():
                # remove log
                if re.search('\.log(\.\d+)*(\.tgz)*$',item) != None or \
                       re.search('\.log(\.tgz)*(\.\d+)*$',item) != None:
                    continue
                # remove missing files
                if item in tmpMissList:
                    continue
                beamGasCList.append((item,tmpList[item]))
            tmpLog.info("query files in %s" % beamGasOdataset)
            tmpList = Client.queryFilesInDataset(beamGasOdataset,options.verbose)
            tmpMissList = Client.getMissLFNsFromLFC(tmpList,options.site,True,options.verbose,dsStr=beamGasOdataset,
                                                    skipScan=options.skipScan)
	    beamGasOList = []
            for item in tmpList.keys():
                # remove log
                if re.search('\.log(\.\d+)*(\.tgz)*$',item) != None or \
                       re.search('\.log(\.tgz)*(\.\d+)*$',item) != None:
                    continue
                # remove missing files
                if item in tmpMissList:
                    continue
                beamGasOList.append((item,tmpList[item]))
            # sort
            beamGasHList.sort()
            beamGasCList.sort()
            beamGasOList.sort()        
            # number of files per one sub job
            if options.nBeamGasH < 0:
                while True:
                    tmpStr = raw_input("Enter the number of BeamGas files for Hydrogen per one sub job : ")
                    try:
                        options.nBeamGasH = int(tmpStr)
                        break
                    except:
                        pass
            if options.nBeamGasC < 0:
                # use default ratio
                options.nBeamGasC = int(options.nBeamGasH*7/90)
            if options.nBeamGasO < 0:
                # use default ratio
                options.nBeamGasO = int(options.nBeamGasH*3/90)
            # check # of files
            if len(beamGasHList) < options.nBeamGasH:
                tmpLog.error("only %s files in %s are available at %s which is less than nBeamGasH=%s" % \
                             (len(beamGasHList),beamGasHdataset,options.site,options.nBeamGasH))
                sys.exit(EC_Dataset)
            if len(beamGasCList) < options.nBeamGasC:
                tmpLog.error("only %s files in %s are available at %s which is less than nBeamGasC=%s" % \
                             (len(beamGasCList),beamGasCdataset,options.site,options.nBeamGasC))
                sys.exit(EC_Dataset)
            if len(beamGasOList) < options.nBeamGasO:
                tmpLog.error("only %s files in %s are available at %s which is less than nBeamGasO=%s" % \
                             (len(beamGasOList),beamGasOdataset,options.site,options.nBeamGasO))
                sys.exit(EC_Dataset)
else:
    if options.split <= 0:
        options.split = 1


# choose site automatically when it is still AUTO
if options.site == "AUTO":
    # convert candidates for SRM v2
    if options.parentDS != '':
        parentDSlocationsSrmV2 = []
        for locTmp in parentDSlocations:
            parentDSlocationsSrmV2.append(Client.convSrmV2ID(locTmp))
    # get DBR locations
    if dbrDsList != []:
	dbrDsString = ''
        for tmpDS in dbrDsList:
            dbrDsString += (tmpDS + ',')
        dbrDsString = dbrDsString[:-1]    
        dbrDsLocationMap,dbrDsLocationMapBack,dbrDsTapeSites = Client.getLocations(dbrDsString,{},options.cloud,
                                                                                   False,options.verbose,
                                                                                   expCloud=expCloudFlag,getReserved=True,
                                                                                   getTapeSites=True,
                                                                                   locCandidates=parentDSlocations)
    # get sites belonging to a cloud and others
    tmpPriSites = []
    tmpSecSites = []
    for tmpID,spec in Client.PandaSites.iteritems():
        if spec['status']=='online':
            # exclude long,xrootd,local queues
            if Client.isExcudedSite(tmpID):
                continue
            # check DDM for parent datasets
            if options.parentDS != '':            
                if not Client.convSrmV2ID(spec['ddm']) in parentDSlocationsSrmV2:
                    continue
            # check DBR locations
            if dbrDsList != []:
                if not Client.convSrmV2ID(spec['ddm']) in dbrDsLocationMap.keys()+dbrDsLocationMapBack.keys():
                    continue
	    # convert to long
            if options.long:
                tmpID = Client.convertToLong(tmpID)
            # check cloud if it is specified   
            if spec['cloud']==options.cloud or (not expCloudFlag):
                tmpPriSites.append(tmpID)
            elif not expCloudFlag:
                tmpSecSites.append(tmpID)
    tmpSitesList = [tmpPriSites]
    if not expCloudFlag:
        tmpSitesList.append(tmpSecSites)
    # run brokerage
    tmpBrokerErr = ''
    for idxTmpSites,tmpSites in enumerate(tmpSitesList):
        if tmpSites != []:
            status,outMap = Client.runBrokerage(tmpSites,'Atlas-%s' % athenaVer,verbose=options.verbose,trustIS=True,cacheVer=cacheVer,
                                                loggingFlag=True,cmtConfig=options.cmtConfig,memorySize=options.memory,
                                                useDirectIO=options.useDirectIOSites,siteGroup=options.useSiteGroup,
                                                maxCpuCount=options.maxCpuCount)
        else:
            status,outMap = 0,{'site':'ERROR : site list is empty.','logInfo':[]} 
        if status != 0:
            tmpLog.error('failed to run brokerage for automatic assignment: %s' % outMap)  
            sys.exit(EC_Config)
        out = outMap['site']
        brokerageLogs = outMap['logInfo']
        if out.startswith('ERROR :'):
            tmpBrokerErr += out
            if idxTmpSites == 0:
                tmpBrokerErr += " Could not find sites in the %s cloud.\n" % options.cloud
            else:
                tmpBrokerErr += " Could not find sites in other clouds.\n"
            if idxTmpSites+1 >= len(tmpSitesList):
                tmpLog.error('brokerage failed')
                print tmpBrokerErr[:-1]
                sys.exit(EC_Config)
            continue    
        if not Client.PandaSites.has_key(out):
            tmpLog.error('brokerage gave wrong PandaSiteID:%s' % out)
            sys.exit(EC_Config)
        break    
    # set site
    options.site = out

# long queue
if options.long and not options.site.startswith('ANALY_LONG_'):
    options.site = Client.convertToLong(options.site)
        
# modify outDS name when container is used for output
if original_outDS_Name.endswith('/'):
    options.outDS = re.sub('/$','.%s' % time.strftime('%y%m%d%H%M%S'),options.outDS)
    # check outDS
    tmpDatasets = Client.getDatasets(options.outDS,options.verbose)
    if len(tmpDatasets) != 0:
        outputDSexist = True

# special handling
specialHandling = ''
if options.express:
    specialHandling += 'express,'
if options.debugMode:
    specialHandling += 'debug,'
specialHandling = specialHandling[:-1]

# check permission
if not Client.checkSiteAccessPermission(options.site,options.workingGroup,options.verbose):
    sys.exit(EC_Config)

# reset destSE if it is redundant
if options.burstSubmit == '':
    tmpOutDsLocation = Client.PandaSites[options.site]['ddm']
    if options.spaceToken != '':
	if Client.PandaSites[options.site]['setokens'].has_key(options.spaceToken):
	    tmpOutDsLocation = Client.PandaSites[options.site]['setokens'][options.spaceToken]
    if options.destSE == tmpOutDsLocation:
	options.destSE = ''

if options.verbose:
    print "== parameters =="
    print "Site       : %s" % options.site
    print "Athena     : %s" % athenaVer
    if groupArea != '':
        print "Group Area : %s" % groupArea
    if cacheVer != '':
        print "ProdCache  : %s" % cacheVer[1:]
    if nightVer != '':
        print "Nightly    : %s" % nightVer[1:]        
    print "cmtConfig  : %s" % AthenaUtils.getCmtConfig(athenaVer,cacheVer,nightVer,options.cmtConfig)	
    print "RunDir     : %s" % runDir
    print "jobO       : %s" % jobO.lstrip()


####################################################################3
# submit jobs

# read jobID
jobDefinitionID = 1
jobid_file = '%s/pjobid.dat' % os.environ['PANDA_CONFIG_ROOT']
if os.path.exists(jobid_file):
    try:
        # read line
        tmpJobIdFile = open(jobid_file)
        tmpID = tmpJobIdFile.readline()
        tmpJobIdFile.close()
        # remove \n
        tmpID = tmpID.replace('\n','')
        # convert to int
        jobDefinitionID = long(tmpID) + 1
    except:
        pass

# look for pandatools package
for path in sys.path:
    if path == '':
        path = curDir
    if os.path.exists(path) and os.path.isdir(path) and 'pandatools' in os.listdir(path):
        # make symlink for module name.
        os.symlink('%s/pandatools' % path,'taskbuffer')
        break

# append tmpdir to import taskbuffer module
sys.path = [tmpDir]+sys.path
from taskbuffer.JobSpec  import JobSpec
from taskbuffer.FileSpec import FileSpec

jobList = []

# job name
jobName = MiscUtils.wrappedUuidGen()

# build job
if options.nobuild and not options.noCompile:
    pass
elif options.libDS == '': 
    jobB = JobSpec()
    jobB.jobDefinitionID   = jobDefinitionID
    if not options.panda_jobsetID in [None,'NULL']:
        jobB.jobsetID      = options.panda_jobsetID
    if options.panda_parentJobsetID != -1:
        jobB.sourceSite    = options.panda_parentJobsetID
    jobB.jobName           = jobName
    jobB.lockedby          = 'panda-client-%s' % PandaToolsPkgInfo.release_version 
    jobB.AtlasRelease      = 'Atlas-%s' % athenaVer
    jobB.homepackage       = 'AnalysisTransforms'+cacheVer+nightVer
    jobB.transformation    = '%s/buildJob-00-00-03' % Client.baseURLSUB
    jobB.specialHandling   = specialHandling
    jobB.cmtConfig         = AthenaUtils.getCmtConfig(athenaVer,cacheVer,nightVer,options.cmtConfig)
    if options.provenanceID != -1:
        jobB.jobExecutionID = options.provenanceID
    hostName = commands.getoutput('hostname').split('.')[0]
    if nickName == '':
        # old convention
        tmpDsPrefix = 'user%s.%s' % (time.strftime('%y',time.gmtime()),distinguishedName)
    else:
        tmpDsPrefix = 'user.%s' % nickName
    libDsName = '%s.%s.%s.lib._%06d' % (tmpDsPrefix,
                                        time.strftime('%m%d%H%M%S',time.gmtime()),
                                        datetime.datetime.utcnow().microsecond,
                                        jobDefinitionID)
    if Client.isDQ2free(options.site):
        # user specified output path
        jobB.destinationDBlock = re.sub('/+$','',options.outputPath)+'/%s' % libDsName
    else:
        jobB.destinationDBlock = libDsName
    if Client.isDQ2free(options.site):
        # flag to use DQ2-free output
	jobB.destinationSE = 'local'
    else:
	jobB.destinationSE = options.site
    if options.prodSourceLabel != '':
        jobB.prodSourceLabel = options.prodSourceLabel
    else:
        jobB.prodSourceLabel = 'panda'        
    if options.processingType != '':    
        jobB.processingType = options.processingType
    if options.seriesLabel != '':    
        jobB.prodSeriesLabel = options.seriesLabel
    jobB.workingGroup      = options.workingGroup    
    jobB.assignedPriority  = 2000
    jobB.computingSite     = options.site
    if options.burstSubmit == '':
        jobB.cloud = Client.PandaSites[options.site]['cloud']
    if options.panda_origFullExecString == '':    
        jobB.metadata = fullExecString
    else:
        jobB.metadata = urllib.unquote(options.panda_origFullExecString)
    fileBO = FileSpec()
    fileBO.lfn = '%s.lib.tgz' % libDsName
    fileBO.type = 'output'
    fileBO.dataset = jobB.destinationDBlock
    fileBO.destinationDBlock = jobB.destinationDBlock
    fileBO.destinationSE = jobB.destinationSE
    jobB.addFile(fileBO)
    fileBI = FileSpec()
    fileBI.lfn = archiveName
    fileBI.type = 'input'
    jobB.jobParameters     = '-i %s -o %s' % (fileBI.lfn,fileBO.lfn)
    # source URL
    matchURL = re.search("(http.*://[^/]+)/",Client.baseURLCSRVSSL)
    if matchURL != None:
        jobB.jobParameters += " --sourceURL %s " % matchURL.group(1)
    # debug parameters
    if options.queueData != '':
        jobB.jobParameters += "--overwriteQueuedata=%s " % options.queueData
    # no compile
    if options.noCompile:
        jobB.jobParameters += "--noCompile "
    # log    
    file = FileSpec()
    file.lfn  = '%s.log.tgz' % libDsName
    file.type = 'log'
    file.dataset = jobB.destinationDBlock
    file.destinationDBlock = jobB.destinationDBlock
    file.destinationSE = jobB.destinationSE
    jobB.addFile(file)
    # set space token
    for file in jobB.Files:
        if file.type in ['output','log']:
            if options.spaceToken != '':
		if Client.PandaSites[options.site]['setokens'].has_key(options.spaceToken):
		    file.destinationDBlockToken = options.spaceToken
            else:
                if options.burstSubmit == '':
                    defaulttoken = Client.PandaSites[options.site]['defaulttoken']
                    file.destinationDBlockToken = Client.getDefaultSpaceToken(vomsFQAN,defaulttoken)
    # append
    jobList.append(jobB)
else:
    # query files in lib dataset to reuse libraries
    if not Client.isDQ2free(options.site):
        tmpLog.info("query files in %s" % options.libDS)
        tmpList = Client.queryFilesInDataset(options.libDS,options.verbose)
    else:
        # don't check for DQ2 free sites
        tmpList = {}
    tmpFileList = []
    tmpGUIDmap = {}
    tmpMD5Sum  = None
    tmpFSize   = None
    tmpScope   = None
    for fileName in tmpList.keys():
        # ignore log file
        if re.search('\.log(\.\d+)*(\.tgz)*$',fileName) != None or \
               re.search('\.log(\.tgz)*(\.\d+)*$',fileName) != None:
            continue
        tmpFileList.append(fileName)
        tmpGUIDmap[fileName] = tmpList[fileName]['guid']
        tmpMD5Sum  = tmpList[fileName]['md5sum']
        tmpFSize   = tmpList[fileName]['fsize']
	tmpScope   = tmpList[fileName]['scope']
    # incomplete libDS
    if tmpFileList == []:
        # query files in dataset from Panda
	status,tmpMap = Client.queryLastFilesInDataset([options.libDS],options.verbose)
        # look for lib.tgz
	for fileName in tmpMap[options.libDS]:
            # ignore log file
            if re.search('\.log(\.\d+)*(\.tgz)*$',fileName) != None or \
                   re.search('\.log(\.tgz)*(\.\d+)*$',fileName) != None:
                continue
            tmpFileList.append(fileName)
            tmpGUIDmap[fileName] = None
    # incomplete libDS
    if tmpFileList == []:
        if not Client.isDQ2free(options.site):
            tmpLog.error("lib dataset %s is empty" % options.libDS)
            sys.exit(EC_Dataset)
        else:
            # try lib.tgz for DQ2 free sites as this might be in ArchivedDB 
            fileName = "%s.lib.tgz" % options.libDS
            tmpFileList.append(fileName)
            tmpGUIDmap[fileName] = MiscUtils.wrappedUuidGen()
    # check file list                
    if len(tmpFileList) != 1:
        tmpLog.error("dataset %s contains multiple lib.tgz files : %s" % (options.libDS,tmpFileList))
        sys.exit(EC_Dataset)
    # instantiate FileSpec
    fileBO = FileSpec()
    fileBO.lfn = tmpFileList[0]
    fileBO.GUID = tmpGUIDmap[fileBO.lfn]
    fileBO.dataset = options.libDS
    fileBO.destinationDBlock = options.libDS
    if fileBO.GUID != 'NULL':
        fileBO.status = 'ready'
    if tmpMD5Sum != None:
        fileBO.md5sum = tmpMD5Sum
    if tmpFSize != None:
        fileBO.fsize = tmpFSize    
    if tmpScope != None:
        fileBO.scope = tmpScope

# run athena            

if options.inDS != '' and options.burstSubmit == '':
    if len(missList) == 0:
        tmpLog.info("all files are available at %s" % options.site)
    else:
        tmpLog.info("%s files are missing or unchecked at %s" % (len(missList),options.site))
    if options.nEventsPerJob != -1:
        pass
    elif not useTagParentLookup:    
        tmpLog.info("use %s files" % len(inputFileList))
    else:
        tmpParentType = 'parent'
        try:
            if runConfig.input.collRefName.startswith('Stream'):
                tmpParentType = re.sub('^Stream','',runConfig.input.collRefName)
                tmpParentType = re.sub('_ref','',tmpParentType)
        except:
            pass
        tmpLog.info("use %s %s files" % (len(inputFileList),tmpParentType))

# disp datasetname to identify inputs
commonDispName = 'user_disp.%s' % MiscUtils.wrappedUuidGen()

# check access method
isDirectAccess = PsubUtils.isDirectAccess(options.site,runConfig.input.inBS,options.trf,options.ara)

# create single submission block when ContElementBoundary is not used
if inputGroupFilesMap=={} or not options.useContElementBoundary:
    if options.inDS != '' or options.shipinput or options.pfnList != '':
        inputGroupFilesMap[options.inDS] = inputFileList
    else:
        inputGroupFilesMap['singleblock'] = []

# split with Nth field of LFN
new_inputGroupFilesMap = {}
if options.splitWithNthFiledOfLFN > 0:
    # loop over all LFN list
    for b_inputDatasetName,b_inputFileList in inputGroupFilesMap.iteritems():
        # make a map between field value and LFNs
        tmpFileldFileMap = {}
        for tmpFileName in b_inputFileList:
            try:
                tmpField = tmpFileName.split('.')[options.splitWithNthFiledOfLFN-1]
            except:
                tmpField = None
            # append to list
            if not tmpFileldFileMap.has_key(tmpField):
                tmpFileldFileMap[tmpField] = []
            tmpFileldFileMap[tmpField].append(tmpFileName)
        # append to map
        for tmpField,tmpFileldFileList in tmpFileldFileMap.iteritems():
            new_inputGroupFilesMap[(b_inputDatasetName,tmpField)] = tmpFileldFileList
else:
    # just set dummy key
    for b_inputDatasetName,b_inputFileList in inputGroupFilesMap.iteritems():
        new_inputGroupFilesMap[(b_inputDatasetName,None)] = b_inputFileList
inputGroupFilesMap = new_inputGroupFilesMap


# error messages
givenErrorMessages = {}

# loop over all blocks
for b_dsFiledKey in inputGroupFilesMap.keys():
    b_inputDatasetName,b_fileFieldValue = b_dsFiledKey
    b_inputFileList  = inputGroupFilesMap[b_dsFiledKey]
    b_nEventsPerFile = options.nEventsPerFile
    b_nFilesPerJob   = options.nFilesPerJob
    b_nEventsPerJob  = options.nEventsPerJob
    b_split          = options.split
    # set # of split
    if b_nEventsPerJob != -1:
        if (not useTagParentLookup) and amiFileMetadataMap == {}:
            #  number of events is avaliable in panda mon
            b_nEventsPerFile,b_nFilesPerJob,b_nEventsPerJob = PsubUtils.calculateNumSplitEvent(b_nEventsPerJob,
                                                                                               b_inputFileList,
                                                                                               options.shipinput,currentDir,
                                                                                               b_nEventsPerFile,
                                                                                               b_nFilesPerJob,
                                                                                               b_inputDatasetName,
                                                                                               options.verbose)
        else:
            # the number of events is avaliable via TAG DB
            if b_split < 0:
                if useTagParentLookup:
                    b_split = PsubUtils.calculateNumSplitTAG(b_nEventsPerJob,b_inputFileList,parentLfnToTagMap,b_nFilesPerJob)
                else:
                    b_split = PsubUtils.calculateNumSplitTAG(b_nEventsPerJob,b_inputFileList,amiFileMetadataMap,b_nFilesPerJob)
    if b_split == -1:
        # calculate # of subjobs using all files
        b_split = PsubUtils.calculateNumSplit(b_nFilesPerJob,options.nGBPerJob,b_nEventsPerJob,b_nEventsPerFile,
                                              maxTotalSize,dbrDsSize,safetySize,useTagParentLookup,
                                              b_inputFileList,inputFileMap,tagFileList,parentLfnToTagMap)
    # LFN description
    b_descriptionInLFN = options.descriptionInLFN
    if options.splitWithNthFiledOfLFN > 0 and b_fileFieldValue != None:
        tmpDescription = b_fileFieldValue
    elif options.useContElementBoundary:
        tmpDescription = PsubUtils.extractNthFieldFromDS(b_inputDatasetName,options.useNthFieldForLFN)
    else:
        tmpDescription = ''
    if b_descriptionInLFN != '':
        if tmpDescription != '':
            b_descriptionInLFN += '.%s' % tmpDescription
    else:
        b_descriptionInLFN = tmpDescription            
    # index of input
    indexFiles   = 0
    indexCavern  = 0
    indexMin     = 0
    indexLowMin  = 0
    indexHighMin = 0
    indexBHalo   = 0
    indexBHaloA  = 0
    indexBHaloC  = 0
    indexBGas    = 0
    indexBGasH   = 0
    indexBGasC   = 0
    indexBGasO   = 0
    # set index of outputs
    if outputDSexist or outputContExist:
        AthenaUtils.setInitOutputIndex(runConfig,original_outDS_Name,options.individualOutDS,
                                       options.extOutFile,outputIndvDSlist,
                                       options.verbose,b_descriptionInLFN)
    # split job
    nUsedByPreviousLoop = 0
    #@ Loop for jobs here
    for iSubJob in range(b_split):
        # terminate condition: no remaining files
        if (options.inDS != '' or options.shipinput or options.pfnList != '') and indexFiles >= len(b_inputFileList):
            break
        # instantiate sub-job
        jobR = JobSpec()
        jobR.jobDefinitionID   = jobDefinitionID
        if not options.panda_jobsetID in [None,'NULL']:
            jobR.jobsetID      = options.panda_jobsetID
        if options.panda_parentJobsetID != -1:
            jobR.sourceSite    = options.panda_parentJobsetID
        jobR.jobName           = jobName
        jobR.lockedby          = 'panda-client-%s' % PandaToolsPkgInfo.release_version     
        jobR.AtlasRelease      = 'Atlas-%s' % athenaVer
        jobR.homepackage       = 'AnalysisTransforms'+cacheVer+nightVer
        jobR.transformation    = '%s/runAthena-00-00-11' % Client.baseURLSUB
        jobR.specialHandling   = specialHandling
        jobR.cmtConfig         = AthenaUtils.getCmtConfig(athenaVer,cacheVer,nightVer,options.cmtConfig)
        # disable automatic job retry on the server side
        if options.disableAutoRetry:
            jobR.maxAttempt = -1
        jobR.destinationDBlock = options.outDS
        if Client.isDQ2free(options.site):
            # flag to use DQ2-free output 
            jobR.destinationSE = 'local'
        elif options.destSE != '':
            # write outputs to destSE
            jobR.destinationSE = options.destSE
        else:
            jobR.destinationSE = options.site
        if options.provenanceID != -1:
            jobR.jobExecutionID = options.provenanceID
        if options.prodSourceLabel != '':
            jobR.prodSourceLabel = options.prodSourceLabel
        else:
            jobR.prodSourceLabel   = 'user'        
        if options.processingType != '':
            jobR.processingType = options.processingType
        if options.seriesLabel != '':    
            jobR.prodSeriesLabel = options.seriesLabel
        jobR.workingGroup      = options.workingGroup            
        jobR.assignedPriority  = 1000
        if options.panda_origFullExecString == '':
            jobR.metadata = fullExecString
        else:
            jobR.metadata = urllib.unquote(options.panda_origFullExecString)
        # memory
        if options.memory != -1:
            jobR.minRamCount = options.memory
        # CPU count
        if options.maxCpuCount != -1:
            jobR.maxCpuCount = options.maxCpuCount
        jobR.computingSite = options.site
        if options.burstSubmit == '':
            jobR.cloud = Client.PandaSites[options.site]['cloud']
        # source files
        if not (options.nobuild and not options.noCompile):
            fileS = FileSpec()
            fileS.lfn     = fileBO.lfn
            fileS.GUID    = fileBO.GUID
	    fileS.md5sum  = fileBO.md5sum
	    fileS.fsize   = fileBO.fsize
            fileS.scope   = fileBO.scope
            fileS.type    = 'input'
            fileS.status  = fileBO.status
            fileS.dataset = fileBO.destinationDBlock
            fileS.dispatchDBlock = fileBO.destinationDBlock
            jobR.addFile(fileS)
        # input files
        inList       = []
        minList      = []
        cavList      = []
        bhaloList    = []
        bgasList     = []
        guidBoundary = []
        lowminList   = []
        highminList  = []                
        if options.inDS != '' or options.shipinput or options.pfnList != '':
            # calculate N files
            if options.nGBPerJob == -1:
                (divF,modF) = divmod(len(b_inputFileList),b_split)
                if modF != 0:
                    divF += 1
                # if split by files was specified
                if b_nFilesPerJob > 0:
                    divF = b_nFilesPerJob 
                # if split by events was specified then
                if b_nEventsPerJob > 0:
                    if (not useTagParentLookup) and amiFileMetadataMap == {}:    
                        #@Calculate how many events to skip
                        nEventsToSkip = nSkips*b_nEventsPerJob
                        # @Increment number of skipped blocks
                        nSkips = nSkips + 1
                        #Take just one file
                        divF = 1
                        # @ If splitting of file per event is complete then take the next file
                        if nEventsToSkip >= b_nEventsPerFile :
                            nEventsToSkip = 0
                            nSkips        = 1
                            indexFiles   += divF
                    else:
                        # the number of events is available via TAG or AMI
                        divF = 1
                        nEventsToSkip = nUsedByPreviousLoop
                        tmpSubTotalEvents = 0 - nUsedByPreviousLoop
                        nUsedFilesForEventSplit = 0
                        if useTagParentLookup:
                            # use TAG for metadata source
                            fileMetadataMapForEventSplit = parentLfnToTagMap
                        else:
                            # use AMI for metadata source                            
                            fileMetadataMapForEventSplit = amiFileMetadataMap
                        for fileName in b_inputFileList[indexFiles:]:
                            if (tmpSubTotalEvents + fileMetadataMapForEventSplit[fileName]['nEvents']) > b_nEventsPerJob:
                                # set the number of used events
				if tmpSubTotalEvents > 0:
				    nUsedByPreviousLoop += (b_nEventsPerJob - tmpSubTotalEvents)
                                else:
                                    nUsedByPreviousLoop += b_nEventsPerJob
                                break    
                            else:
                                tmpSubTotalEvents += fileMetadataMapForEventSplit[fileName]['nEvents']
                                # increment file count since all events should be used
                                nUsedFilesForEventSplit += 1
                                # reset
                                nUsedByPreviousLoop = 0
                                # next file is not used
                                if tmpSubTotalEvents == b_nEventsPerJob:
                                    break
				# the number of input files is specified    
				if b_nFilesPerJob > 0 and nUsedFilesForEventSplit >= b_nFilesPerJob:
                                    break
                                # increment the number of files used by the subjob 
                                divF += 1
            else:
                # calculate number of files for nGBPerJob
                divF = 0
                tmpSubTotal = 0
                tmpSubNumFiles = 0
                tmpTagList = []
                tmpTagSize = 0
                for fileName in b_inputFileList[indexFiles:]:
                    vals = inputFileMap[fileName]
                    tmpSize = long(vals['fsize'])
                    tmpSubNumFiles += 1
                    # take TAG into account
                    if useTagParentLookup:
                        tmpTagFileName = parentLfnToTagMap[fileName]['lfn']
                        if not tmpTagFileName in tmpTagList:
                            tmpTagList.append(tmpTagFileName)
                            tmpTagSize += long(tagFileList[tmpTagFileName]['fsize'])
                    # check    
                    if tmpSubTotal+tmpSize+tmpTagSize > options.nGBPerJob-dbrDsSize-safetySize \
                           or tmpSubNumFiles+len(tmpTagList) > PsubUtils.limit_maxNumInputs:
                        break
                    divF += 1
                    tmpSubTotal += tmpSize
                # avoid zero-divide    
                if divF == 0:
                    divF = 1
            # File Selector
            totalSize = 0
            numInputFiles = 0
            totalTagSize = 0
            tmpList = b_inputFileList[indexFiles:indexFiles+divF]
            if options.inDS != '' and not options.shipinput:
                tmpTagList = []
                inTagListStr = ''
                numInputFiles = len(tmpList)
                for fileName in tmpList:
                    vals = inputFileMap[fileName]
                    # instantiate  FileSpec
                    file = FileSpec()
                    file.lfn            = fileName
                    file.GUID           = vals['guid']
                    file.fsize          = vals['fsize']
                    file.md5sum         = vals['md5sum']
                    file.scope          = vals['scope']
                    if vals.has_key('dataset'):
                        file.dataset    = vals['dataset']
                    else:
                        file.dataset    = options.inDS                    
                    file.prodDBlock     = file.dataset
                    file.dispatchDBlock = commonDispName
                    file.type           = 'input'
                    file.status         = 'ready'
                    # use local access for TRF and BS
                    if (options.trf or runConfig.input.inBS) and not isDirectAccess:
                        file.prodDBlockToken = 'local'
                    jobR.addFile(file)
                    inList.append(fileName)
                    try:
                        totalSize += long(file.fsize)
                    except:
                        pass
                    # TAG
                    if useTagParentLookup:
                        # collect parent GUIDs
                        guidBoundary.append(file.GUID)
                        # get TAG spec
                        tmpTagFileName = parentLfnToTagMap[fileName]['lfn']
                        if not tmpTagFileName in tmpTagList:
                            tmpTagList.append(tmpTagFileName)
                            inTagListStr += '%s,' % tmpTagFileName
                            vals = tagFileList[tmpTagFileName]
                            # instantiate FileSpec for TAG                   
                            file = FileSpec()
                            file.lfn            = tmpTagFileName
                            file.GUID           = vals['guid']
                            file.fsize          = vals['fsize']
                            file.md5sum         = vals['md5sum']
                            file.scope          = vals['scope']
                            file.dataset        = parentLfnToTagMap[fileName]['tagDS']
                            file.prodDBlock     = file.dataset
                            file.dispatchDBlock = commonDispName
                            file.type           = 'input'
                            file.status         = 'ready'
                            if options.trf and not isDirectAccess:
                                file.prodDBlockToken = 'local'
                            # set noshadow since single TAG may point to multiple parent files
                            file.destinationDBlockToken = 'noshadow'
                            jobR.addFile(file)
                            inList.append(tmpTagFileName)
                            totalTagSize += long(file.fsize)
                inTagListStr = inTagListStr[:-1]
            else:        
                for fileName in tmpList:
                    # use GUID boundaries or not
                    if options.panda_devidedByGUID:
                        vals = inputFileMap[fileName]
                        # collect GUID/LFNs
                        guidBoundary.append(vals['guid'])
                    elif devidedByGUID:
                        # collect GUIDs
                        guidBoundary.append(fileName)
                        # fileName is a GUID in this case 
                        realFileName = guidCollMap[fileName]
                        if not realFileName in inList:
                            inList.append(realFileName)
                    else:
                        inList.append(fileName)
                # use full list since mapping between input and coll GUIDs is lost by runPathenaRec
                if options.panda_devidedByGUID:
                    for tmpShipFile in runConfig.input.shipFiles:
                        if not tmpShipFile in inList:
                            inList.append(tmpShipFile)
            # Minimum Bias
            totalMinSize = 0
            numMinFiles  = 0
            if runConfig.input.inMinBias:
                if (options.nMin > 0 and indexMin+options.nMin*divF > len(minbiasList)) or \
                       (options.nMinPerJob > 0 and indexMin+options.nMinPerJob > len(minbiasList)):
                    # re-use files when Minimum-Bias files are not enough   
                    indexMin = 0
                if options.nMin > 0:    
                    tmpList = minbiasList[indexMin:indexMin+options.nMin*divF]
                    indexMin += options.nMin*divF
                else:
                    tmpList = minbiasList[indexMin:indexMin+options.nMinPerJob]
                    indexMin += options.nMinPerJob
                numMinFiles = len(tmpList)
                for fileName,vals in tmpList:
                    # instantiate  FileSpec
                    file = FileSpec()
                    file.lfn            = fileName
                    file.GUID           = vals['guid']
                    file.fsize          = vals['fsize']
                    file.md5sum         = vals['md5sum']
                    file.scope          = vals['scope']
                    file.dataset        = minbiasDataset
                    file.prodDBlock     = minbiasDataset
                    file.dispatchDBlock = commonDispName
                    file.type       = 'input'
                    file.status     = 'ready'
                    file.destinationDBlockToken = 'noshadow'                
                    if options.trf and not isDirectAccess:
                        file.prodDBlockToken = 'local'
                    jobR.addFile(file)
                    minList.append(fileName)
                    totalMinSize += long(file.fsize)
            # low pT Minimum Bias        
            if runConfig.input.inLoMinBias:
                if (options.nLowMin > 0 and indexLowMin+options.nLowMin*divF > len(lowminbiasList)) or \
                       (options.nLowMinPerJob > 0 and indexLowMin+options.nLowMinPerJob > len(lowminbiasList)):
                    # re-use files when Low pT Minimum-Bias files are not enough   
                    indexLowMin = 0
                if options.nLowMin > 0:    
                    tmpList = lowminbiasList[indexLowMin:indexLowMin+options.nLowMin*divF]
                    indexLowMin += options.nLowMin*divF
                else:
                    tmpList = lowminbiasList[indexLowMin:indexLowMin+options.nLowMinPerJob]
                    indexLowMin += options.nLowMinPerJob
                numMinFiles += len(tmpList)
                for fileName,vals in tmpList:
                    # instantiate  FileSpec
                    file = FileSpec()
                    file.lfn            = fileName
                    file.GUID           = vals['guid']
                    file.fsize          = vals['fsize']
                    file.md5sum         = vals['md5sum']
                    file.scope          = vals['scope']
                    file.dataset        = options.lowMinDS
                    file.prodDBlock     = options.lowMinDS
                    file.dispatchDBlock = commonDispName
                    file.type       = 'input'
                    file.status     = 'ready'
                    file.destinationDBlockToken = 'noshadow'                
                    if options.trf and not isDirectAccess:
                        file.prodDBlockToken = 'local'
                    jobR.addFile(file)
                    lowminList.append(fileName)
                    totalMinSize += long(file.fsize)
            # high pT Minimum Bias        
            if runConfig.input.inHiMinBias:
                if (options.nHighMin > 0 and indexHighMin+options.nHighMin*divF > len(highminbiasList)) or \
                       (options.nHighMinPerJob > 0 and indexHighMin+options.nHighMinPerJob > len(highminbiasList)):
                    # re-use files when High pT Minimum-Bias files are not enough   
                    indexHighMin = 0
                if options.nHighMin > 0:    
                    tmpList = highminbiasList[indexHighMin:indexHighMin+options.nHighMin*divF]
                    indexHighMin += options.nHighMin*divF
                else:
                    tmpList = highminbiasList[indexHighMin:indexHighMin+options.nHighMinPerJob]
                    indexHighMin += options.nHighMinPerJob
                numMinFiles += len(tmpList)
                for fileName,vals in tmpList:
                    # instantiate  FileSpec
                    file = FileSpec()
                    file.lfn            = fileName
                    file.GUID           = vals['guid']
                    file.fsize          = vals['fsize']
                    file.md5sum         = vals['md5sum']
                    file.scope          = vals['scope']
                    file.dataset        = options.highMinDS
                    file.prodDBlock     = options.highMinDS
                    file.dispatchDBlock = commonDispName
                    file.type       = 'input'
                    file.status     = 'ready'
                    file.destinationDBlockToken = 'noshadow'                
                    if options.trf and not isDirectAccess:
                        file.prodDBlockToken = 'local'
                    jobR.addFile(file)
                    highminList.append(fileName)
                    totalMinSize += long(file.fsize)
            # Cavern
            totalCavSize = 0
            numCavFiles  = 0
            if runConfig.input.inCavern:
                if (options.nCav > 0 and indexCavern+options.nCav*divF > len(cavernList)) or \
                       (options.nCavPerJob > 0 and indexCavern+options.nCavPerJob > len(cavernList)):
                    # re-use files when Cavern files are not enough   
                    indexCavern = 0
                if options.nCav > 0:    
                    tmpList = cavernList[indexCavern:indexCavern+options.nCav*divF]
                    indexCavern += options.nCav*divF
                else:
                    tmpList = cavernList[indexCavern:indexCavern+options.nCavPerJob]
                    indexCavern += options.nCavPerJob
                numCavFiles = len(tmpList)
                for fileName,vals in tmpList:
                    # instantiate  FileSpec
                    file = FileSpec()
                    file.lfn            = fileName
                    file.GUID           = vals['guid']
                    file.fsize          = vals['fsize']
                    file.md5sum         = vals['md5sum']
                    file.scope          = vals['scope']
                    file.dataset        = cavernDataset
                    file.prodDBlock     = cavernDataset
                    file.dispatchDBlock = commonDispName
                    file.type           = 'input'
                    file.status         = 'ready'
                    file.destinationDBlockToken = 'noshadow'                
                    if options.trf and not isDirectAccess:
                        file.prodDBlockToken = 'local'
                    jobR.addFile(file)
                    cavList.append(fileName)
                    totalCavSize += long(file.fsize)
            # BeamHalo
            totalBeamHaloSize = 0
            numBeamHaloFiles  = 0
            if runConfig.input.inBeamHalo:
                if options.useCommonHalo:
                    # integrated dataset
                    if indexBHalo+options.nBeamHalo > len(beamHaloList):
                        # re-use files when BeamHalo files are not enough   
                        indexBHalo = 0   
                    tmpList = beamHaloList[indexBHalo:indexBHalo+options.nBeamHalo]
                    indexBHalo += options.nBeamHalo
                else:
                    # separate datasets
                    if indexBHaloA+options.nBeamHaloA > len(beamHaloAList):
                        # re-use files when BeamHalo files are not enough   
                        indexBHaloA = 0   
                    if indexBHaloC+options.nBeamHaloC > len(beamHaloCList):
                        # re-use files when BeamHalo files are not enough   
                        indexBHaloC = 0   
                    tmpList = beamHaloAList[indexBHaloA:indexBHaloA+options.nBeamHaloA] + \
                              beamHaloCList[indexBHaloC:indexBHaloC+options.nBeamHaloC]
                    indexBHaloA += options.nBeamHaloA
                    indexBHaloC += options.nBeamHaloC
                tmpIndex = 0
                numBeamHaloFiles = len(tmpList)
                for fileName,vals in tmpList:
                    # instantiate  FileSpec
                    file = FileSpec()
                    file.lfn            = fileName
                    file.GUID           = vals['guid']
                    file.fsize          = vals['fsize']
                    file.md5sum         = vals['md5sum']
                    file.scope          = vals['scope']
                    if options.useCommonHalo:
                        # integrated dataset
                        file.dataset        = beamHaloDataset
                        file.prodDBlock     = beamHaloDataset
                        file.dispatchDBlock = commonDispName
                    else:
                        # separate datasets
                        if tmpIndex < options.nBeamHaloA:
                            file.dataset        = beamHaloAdataset
                            file.prodDBlock     = beamHaloAdataset
                            file.dispatchDBlock = commonDispName
                        else:
                            file.dataset        = beamHaloCdataset
                            file.prodDBlock     = beamHaloCdataset
                            file.dispatchDBlock = commonDispName
                    file.type           = 'input'
                    file.status         = 'ready'
                    file.destinationDBlockToken = 'noshadow'                
                    if options.trf and not isDirectAccess:
                        file.prodDBlockToken = 'local'
                    jobR.addFile(file)
                    bhaloList.append(fileName)
                    tmpIndex += 1
                    totalBeamHaloSize += long(file.fsize) 
            # BeamGas
            totalBeamGasSize = 0
            numBeamGasFiles  = 0
            if runConfig.input.inBeamGas:        
                if options.useCommonGas:
                    # integrated dataset
                    if indexBGas+options.nBeamGas > len(beamGasList):
                        # re-use files when BeamGas files are not enough   
                        indexBGas = 0   
                    tmpList = beamGasList[indexBGas:indexBGas+options.nBeamGas]
                    indexBGas += options.nBeamGas
                else:
                    # separate dataset
                    if indexBGasH+options.nBeamGasH > len(beamGasHList):
                        # re-use files when BeamGas files are not enough   
                        indexBGasH = 0   
                    if indexBGasC+options.nBeamGasC > len(beamGasCList):
                        # re-use files when BeamGas files are not enough   
                        indexBGasC = 0   
                    if indexBGasO+options.nBeamGasO > len(beamGasOList):
                        # re-use files when BeamGas files are not enough   
                        indexBGasO = 0   
                    tmpList = beamGasHList[indexBGasH:indexBGasH+options.nBeamGasH] + \
                              beamGasCList[indexBGasC:indexBGasC+options.nBeamGasC] + \
                              beamGasOList[indexBGasO:indexBGasO+options.nBeamGasO]
                    indexBGasH += options.nBeamGasH
                    indexBGasC += options.nBeamGasC
                    indexBGasO += options.nBeamGasO
                tmpIndex = 0
                numBeamGasFiles = len(tmpList)
                for fileName,vals in tmpList:
                    # instantiate  FileSpec
                    file = FileSpec()
                    file.lfn            = fileName
                    file.GUID           = vals['guid']
                    file.fsize          = vals['fsize']
                    file.md5sum         = vals['md5sum']
                    file.scope          = vals['scope']
                    if options.useCommonHalo:
                        # integrated dataset
                        file.dataset        = beamGasDataset
                        file.prodDBlock     = beamGasDataset
                        file.dispatchDBlock = commonDispName
                    else:
                        # separate datasets
                        if tmpIndex < options.nBeamGasH:
                            file.dataset        = beamGasHdataset
                            file.prodDBlock     = beamGasHdataset
                            file.dispatchDBlock = commonDispName
                        elif tmpIndex < (options.nBeamGasH+options.nBeamGasC):
                            file.dataset        = beamGasCdataset
                            file.prodDBlock     = beamGasCdataset
                            file.dispatchDBlock = commonDispName
                        else:
                            file.dataset        = beamGasOdataset
                            file.prodDBlock     = beamGasOdataset
                            file.dispatchDBlock = commonDispName
                    file.type           = 'input'
                    file.status         = 'ready'
                    file.destinationDBlockToken = 'noshadow'
                    if options.trf and not isDirectAccess:
                        file.prodDBlockToken = 'local'
                    jobR.addFile(file)
                    bgasList.append(fileName)
                    tmpIndex += 1
                    totalBeamGasSize += long(file.fsize)
            # size check
            if (not isDirectAccess) and \
                   totalSize+totalMinSize+totalCavSize+totalBeamHaloSize+totalBeamGasSize+dbrDsSize+totalTagSize > maxTotalSize and \
                   numInputFiles > 1:
                tmpStr  = "A subjob has %s input files and requires %sMB of disk space" \
                          % (numInputFiles, ((totalSize+totalMinSize+totalCavSize+totalBeamHaloSize+totalBeamGasSize+dbrDsSize+totalTagSize) >> 20))
                if dbrDsSize != 0:
                    tmpStr += " (DBRelease=%sMB)" % (dbrDsSize>>20)
                if useTagParentLookup:
                    tmpStr += " (TAG=%sMB)" % (totalTagSize>>20)
                if totalMinSize != 0:
                    tmpStr += " (MinimumBias=%sMB, %sfiles)" % (totalMinSize>>20,numMinFiles)
                if totalCavSize != 0:
                    tmpStr += " (Cavern=%sMB, %sfiles)" % (totalCavSize>>20,numCavFiles)
                if totalBeamHaloSize != 0:
                    tmpStr += " (BeamHalo=%sMB, %sfiles)" % (totalBeamHaloSize>>20,numBeamHaloFiles)
                if totalBeamGasSize != 0:
                    tmpStr += " (BeamGas=%sMB, %sfiles)" % (totalBeamGasSize>>20,numBeamGasFiles)
                tmpStr += ". It must be less than %sMB to avoid overflowing the remote disk. " \
                          % (maxTotalSize >> 20)
                tmpStr += "Please split the job using --nFilesPerJob. If file sizes vary in large range "
                tmpStr += "--nGBPerJob may help. e.g., --nGBPerJob=MAX"                
                tmpLog.error(tmpStr)
                sys.exit(EC_Split)
            #@
            #@ important
            #@ Done with an input files description of the job.
            # Increment pointer (index) to a next block of files
            #@ If split by events is requested Index file is incremented in a different place (above)
            if b_nEventsPerJob < 0:
                indexFiles += divF
            if b_nEventsPerJob > 0 and (useTagParentLookup or amiFileMetadataMap != {}):
                indexFiles += nUsedFilesForEventSplit
        elif options.trf and b_nEventsPerJob > 0 :
            # Calculate how many events to skip mainly to set event number for evgen
            nEventsToSkip = nSkips*b_nEventsPerJob
            # @Increment number of skipped blocks
            nSkips = nSkips + 1

        # output files
        outMap = {}
        AthenaUtils.convertConfToOutput(runConfig,jobR,outMap,options.individualOutDS,options.extOutFile,
                                        original_outDS_Name,b_descriptionInLFN)
        # set space token
        for file in jobR.Files:
            if file.type in ['output','log']:
                if options.spaceToken != '':
                    if Client.PandaSites[options.site]['setokens'].has_key(options.spaceToken):
                        file.destinationDBlockToken = options.spaceToken
                else:
                    if options.burstSubmit == '':                
                        defaulttoken = Client.PandaSites[options.site]['defaulttoken']
                        file.destinationDBlockToken = Client.getDefaultSpaceToken(vomsFQAN,defaulttoken)
        # job parameters
        param = ''
        if not (options.nobuild and not options.noCompile):
            param  += '-l %s ' % fileS.lfn
        param += '-r %s ' % runDir
        if not options.trf:
            tmpJobO = jobO        
            # modify one-liner for G4 random seeds
            if runConfig.other.G4RandomSeeds > 0:
                if options.singleLine != '':
                    tmpJobO = re.sub('-c "%s" ' % options.singleLine,
                                     '-c "%s;from G4AtlasApps.SimFlags import SimFlags;SimFlags.SeedsG4=%s" ' \
                                     % (options.singleLine,runConfig.other.G4RandomSeeds+iSubJob),
                                     jobO)
                else:
                    tmpJobO = ('-c "from G4AtlasApps.SimFlags import SimFlags;SimFlags.SeedsG4=%s" ' \
                               % (runConfig.other.G4RandomSeeds+iSubJob)) + jobO
            # replace full-path jobOs
            for tmpFullName,tmpLocalName in AthenaUtils.fullPathJobOs.iteritems():
                tmpJobO = re.sub(tmpFullName,tmpLocalName,tmpJobO)
            # set jobO parameter
            param += '-j "%s" ' % urllib.quote(tmpJobO)
            # DBRelease
            if options.dbRelease != '':
                tmpItems = options.dbRelease.split(':')
                tmpDbrDS  = tmpItems[0]
                tmpDbrLFN = tmpItems[1]
                # instantiate  FileSpec
                fileName = tmpDbrLFN
                vals     = dbrFiles[tmpDbrLFN]
                file = FileSpec()
                file.lfn            = fileName
                file.GUID           = vals['guid']
                file.fsize          = vals['fsize']
                file.md5sum         = vals['md5sum']
                file.scope          = vals['scope']
                file.dataset        = tmpDbrDS
                file.prodDBlock     = tmpDbrDS
                file.dispatchDBlock = tmpDbrDS
                file.type       = 'input'
                file.status     = 'ready'
                jobR.addFile(file)
                # set DBRelease parameter
                param += '--dbrFile %s ' % file.lfn
                if options.dbRunNumber != '':
                    param += '--dbrRun %s ' % options.dbRunNumber
        else:
            # replace parameters for TRF
            tmpJobO = jobO
            # output : basenames are in outMap['IROOT'] trough extOutFile
            tmpOutMap = []
            for tmpName,tmpLFN in outMap['IROOT']:
                tmpJobO = tmpJobO.replace('%OUT.' + tmpName,tmpName)
                # set correct name in outMap
                tmpOutMap.append((tmpName,tmpLFN))
            # set output for normal TRF (not for ARA)
            if not options.ara:
                outMap['IROOT'] = tmpOutMap 
            # input
            inPattList = []
            if not AthenaUtils.checkUseTagInTrf(jobO,options.useTagInTRF):
                # keep %IN to be replaced on WN if they are TAGs
                inPattList += [('%IN',inList)]
            inPattList += [('%MININ',minList),
                           ('%CAVIN',cavList),
                           ('%BHIN',bhaloList),
                           ('%BGIN',bgasList),
                           ('%LOMBIN',lowminList),
                           ('%HIMBIN',highminList)]
            for tmpPatt,tmpInList in inPattList:
                if tmpJobO.find(tmpPatt) != -1 and len(tmpInList) > 0:
                    tmpJobO = AthenaUtils.replaceParam(tmpPatt,tmpInList,tmpJobO,options.useNewTRF)
            # DBRelease
            tmpItems = tmpJobO.split()
            if options.dbRelease != '':
                # mimic a trf parameter to reuse following algorithm
                tmpItems += ['%DB='+options.dbRelease]
            for tmpItem in tmpItems:
                match = re.search('%DB=([^:]+):(.+)$',tmpItem)
                if match:
                    tmpDbrDS  = match.group(1)
                    tmpDbrLFN = match.group(2)
                    # skip if it is already extracted
                    if tmpDbrLFN in inList:
                        continue
                    # instantiate  FileSpec
                    fileName = tmpDbrLFN
                    vals     = dbrFiles[tmpDbrLFN]
                    file = FileSpec()
                    file.lfn            = fileName
                    file.GUID           = vals['guid']
                    file.fsize          = vals['fsize']
                    file.md5sum         = vals['md5sum']
                    file.scope          = vals['scope']
                    file.dataset        = tmpDbrDS
                    file.prodDBlock     = tmpDbrDS
                    file.dispatchDBlock = tmpDbrDS
                    file.type       = 'input'
                    file.status     = 'ready'
                    jobR.addFile(file)
                    inList.append(fileName)
                    # replace parameters
                    tmpJobO = tmpJobO.replace(match.group(0),tmpDbrLFN)
                    # set DBRelease parameter to trigger DBR setup when DBR is not set in --trf
                    if not '%DB=' in jobO and not file.lfn in jobO:
                        param += '--dbrFile %s ' % file.lfn
                        if options.dbRunNumber != '':
                            param += '--dbrRun %s ' % options.dbRunNumber
            # random seed
            for tmpItem in tmpJobO.split():
                match = re.search('%RNDM=(\d+)( |$|\'|\"|;)',tmpItem)
                if match:
                    tmpRndmNum = int(match.group(1)) + iSubJob
                    # replace parameters
                    tmpJobO = re.sub(match.group(0),'%s%s' % (tmpRndmNum,match.group(2)),tmpJobO)
            # skipEvent
            tmpUseSkipEventsInTrf = False
            for tmpItem in tmpJobO.split():
                match = re.search('%SKIPEVENTS(:*)(\d*)( |$|\'|\"|;)',tmpItem)
                if match != None:
                    tmpUseSkipEventsInTrf = True
                    if match.group(2) == '':
                        # replace %SKIPEVENTS
                        tmpJobO = re.sub(match.group(0),'%s%s' % (nEventsToSkip,match.group(3)),tmpJobO)
                    else:
                        # replace %SKIPEVENTS:NN
                        tmpSkipEvents = int(match.group(2)) + nEventsToSkip 
                        tmpJobO = re.sub(match.group(0),'%s%s' % (tmpSkipEvents,match.group(3)),tmpJobO)
            # give a warning to avoid event duplication when %SKIPEVENTS is unused in --trf
            if not tmpUseSkipEventsInTrf and nEventsToSkip > 0 and not givenErrorMessages.has_key('missingSKIPEVENTS'):
                tmpMsgStr  = "The number of events in an input file is larger than --nEventsPerJob "
                tmpMsgStr += "but you didn't specify %SKIPEVENTS in --trf, "
                tmpMsgStr += "so that some subjobs might run on the same events. "
                tmpMsgStr += "Please make sure that %XYZ in --trf is automatically replaced to a real value "
                tmpMsgStr += "but no parameter is automatically appended to --trf. "
                tmpMsgStr += "You need to specify all necessary parameters in --trf"
                print
                tmpLog.warning(tmpMsgStr)
                print
                givenErrorMessages['missingSKIPEVENTS'] = tmpMsgStr
            # set jobO parameter
            param += '-j "%s" ' % urllib.quote(tmpJobO)		
        param += '-i "%s" ' % inList
        param += '-m "%s" ' % (minList+lowminList+highminList)
        param += '-n "%s" ' % cavList
        if bhaloList != []:
            param += '--beamHalo "%s" ' % bhaloList
        if bgasList != []:
            param += '--beamGas "%s" ' % bgasList
        param += '-o "%s" ' % outMap
        if runConfig.input.inColl or (options.trf and AthenaUtils.checkUseTagInTrf(jobO,options.useTagInTRF)):
            param += '-c '
        if runConfig.input.inBS:
            param += '-b '
        if runConfig.input.backNavi:
            param += '-e '
        if options.shipinput:
            param += '--shipInput '
        # GUID boundaries
        if (options.shipinput and devidedByGUID) or useTagParentLookup:
            param += '--guidBoundary "%s" ' % guidBoundary
            param += '--collRefName %s ' % runConfig.input.collRefName
        if useTagParentLookup and inTagListStr != '':
            param += '--tagFileList %s ' % inTagListStr
        pStr1 = ''    
        if runConfig.other.rndmStream != []:
            pStr1 = "AtRndmGenSvc=Service('AtRndmGenSvc');AtRndmGenSvc.Seeds=["
            for stream in runConfig.other.rndmStream:
                num = runConfig.other.rndmNumbers[runConfig.other.rndmStream.index(stream)]
                pStr1 += "'%s %d %d'," % (stream,num[0]+iSubJob,num[1]+iSubJob)
            pStr1 += "]"

        # @ If split by event option was invoked
        pStr2 = ''
        if b_nEventsPerJob > 0 and (not options.trf):
            # @ Number of events to be processed per job
            param1 = "theApp.EvtMax=%s" % b_nEventsPerJob
            # @ possibly skip events in a file
            if runConfig.input.noInput:
                pStr2 = param1
            else:
                param2 = "EventSelector.SkipEvents=%s" % nEventsToSkip
                # @ Form a string to add to job parameters
                pStr2 = '%s;%s' % (param1,param2)
        # parameter
        if pStr1 != '' or pStr2 != '':
            if pStr1 == '' or pStr2 == '':
                param += '-f "%s" ' % (pStr1+pStr2)
            else:
                param += '-f "%s;%s" ' % (pStr1,pStr2)
        # libDS 
        if options.libDS != "" or (options.nobuild and not options.noCompile):
            param += '-a %s ' % archiveName
        # addPoolFC
        if options.addPoolFC != "":
            param += '--addPoolFC %s ' % options.addPoolFC
        # use corruption checker
        if options.corCheck:
            param += '--corCheck '
        # disable to skip missing files
        if options.notSkipMissing:
            param += '--notSkipMissing '
        # given PFN 
        if options.pfnList != '':
            param += '--givenPFN '
        # create symlink for MC data
        if options.mcData != '':
            param += '--mcData %s ' % options.mcData
        # source URL
        matchURL = re.search("(http.*://[^/]+)/",Client.baseURLCSRVSSL)
        if matchURL != None:
            param += " --sourceURL %s " % matchURL.group(1)
        # run TRF
        if options.trf:
            param += '--trf '
        # use ARA 
        if options.ara:
            param += '--ara '
        # event picking
        if orig_eventPickEvtList:
            param += '--eventPickTxt=%s ' % eventPickRunEvtDat.split('/')[-1]
        # general input format
        if options.generalInput:
            param += '--generalInput '
        # use local access for TRF and BS
        if (options.trf or runConfig.input.inBS) and not isDirectAccess:
            param += '--useLocalIO '        
        # use theApp.nextEvent
        if options.useNextEvent:
            param += '--useNextEvent '
        # use code tracer 
        if options.codeTrace:
            param += '--codeTrace '
        # JEM
        if options.enableJEM:
            param += "--enable-jem "
            if options.configJEM != '':
                param += "--jem-config %s " % options.configJEM
        # debug parameters
        if options.queueData != '':
            param += "--overwriteQueuedata=%s " % options.queueData
        # merge
        if options.mergeOutput:
            param += "--mergeOutput "
            if options.mergeScript != '':
                param += '--mergeScript "%s" ' % options.mergeScript
        # assign    
        jobR.jobParameters = param

        if options.verbose:
            tmpLog.debug(param)
        # set prodDBlock
        jobR.prodDBlock = PsubUtils.getProdDBlock(jobR,options.inDS)
        # set destinationDBlock to associate jobs with the same output container
        # do it here since destinationDBock is used for output file naming
        jobR.destinationDBlock = original_outDS_Name
        # write outputs to Chirp Server
        if options.useChirpServer != '':
            PsubUtils.setCHIRPtokenToOutput(jobR,options.useChirpServer)
	# write outputs to GlobusOnline    
        if options.useGOForOutput != '':
            PsubUtils.setCHIRPtokenToOutput(jobR,options.useGOForOutput,True)
        # disable redundant transfer if needed
        PsubUtils.disableRedundantTransfer(jobR,options.transferredDS)
        # check job spec
        PsubUtils.checkJobSpec(jobR)
        # append
        jobList.append(jobR)


# split jobs by the number of output files
splitJobList = PsubUtils.splitJobsNumOutputFiles(jobList,options.buildInLastChunk)


# check output dataset length for individualOutDS
tmpIdvOutDsList = []
if options.individualOutDS:
    for tmpFile in jobList[-1].Files:
        if tmpFile.type in ['output','log']:
            if not tmpFile.dataset in tmpIdvOutDsList:
                if not PsubUtils.checkOutDsName(tmpFile.dataset,distinguishedName,options.official,nickName,
                                                options.site,vomsFQAN,options.mergeOutput):
                    tmpLog.info("a suffix is added to datasetname for each output stream when --individualOutDS is used. e.g. outDS_suffix:%s. Please use shorter name for --outDS" % tmpFile.dataset)
                    sys.exit(EC_Config)
                tmpIdvOutDsList.append(tmpFile.dataset)


# check the number of output files
if not options.unlimitNumOutputs:
    tmpStat = PsubUtils.checkNumOutputs(jobList)
    if not tmpStat:
        sys.exit(EC_Config)


# jobsetID
jobsetID = options.panda_jobsetID

# no submit
if options.nosubmit:
    tmpLog.info("%s subjobs to be submitted to %s" % (len(jobList),options.site))
    if options.verbose:
	tmpLog.debug("brokerage info")
	for tmpBrokerageMsg in brokerageLogs+userBrokerLogs:
            print tmpBrokerageMsg

# submit 
if not options.nosubmit:

    # upload proxy for glexec
    if Client.PandaSites.has_key(options.site):
        # delegation
        delResult = PsubUtils.uploadProxy(options.site,options.myproxy,gridPassPhrase,
                                          Client.PandaClouds[options.cloud]['pilotowners'],
                                          options.verbose)
        if not delResult:
            tmpLog.error("failed to upload proxy")
            sys.exit(EC_MyProxy)

    # normal/burst submission
    if options.burstSubmit == '':
        # normal submission

	# dataset location
	tmpOutDsLocation = Client.PandaSites[options.site]['ddm']
	if options.spaceToken != '':
	    if Client.PandaSites[options.site]['setokens'].has_key(options.spaceToken):
		tmpOutDsLocation = Client.PandaSites[options.site]['setokens'][options.spaceToken]
	# always use site's SE for libDS
      	tmpLibDsLocation = tmpOutDsLocation
        # dataset registration
        if not Client.isDQ2free(options.site):
            # register output dataset
            tmpIdvOutDsList = []
            tmpDsContMap = {}
            # loop over all job lists
            for serIndex,tmpJobList in enumerate(splitJobList): 
                if (not outputDSexist) or options.destSE != '':
                    if not options.individualOutDS:
                        tmpDsName = options.outDS+PsubUtils.getSuffixToSplitJobList(serIndex)
                        Client.addDataset(tmpDsName,options.verbose,location=tmpOutDsLocation,dsExist=outputDSexist)
                        tmpIdvOutDsList.append(tmpDsName)
                if options.individualOutDS:        
                    # register individual datasets
                    for tmpFile in tmpJobList[-1].Files:
                        if tmpFile.type in ['output','log']:
                            if (not outputIndvDSlist.has_key(tmpFile.destinationDBlock)) and (not tmpFile.destinationDBlock in tmpIdvOutDsList):
                                Client.addDataset(tmpFile.destinationDBlock,options.verbose,location=tmpOutDsLocation,dsExist=outputDSexist)
                                tmpIdvOutDsList.append(tmpFile.destinationDBlock)
                            tmpDsContMap[tmpFile.destinationDBlock] = tmpFile.dataset

            # register output dataset container
            if original_outDS_Name.endswith('/'):
                # create
                if not outputContExist:
                    tmpIdvContList = []
                    Client.createContainer(original_outDS_Name,options.verbose)
                    tmpIdvContList.append(original_outDS_Name)
                    if options.individualOutDS:
                        for tmpFile in jobList[-1].Files:
                            if tmpFile.type in ['output','log'] and tmpFile.dataset.endswith('/'):
                                if not tmpFile.dataset in tmpIdvContList:
                                    Client.createContainer(tmpFile.dataset,options.verbose)
                                    tmpIdvContList.append(tmpFile.dataset)
                                
                # add dataset
                if not outputDSexist:
                    for tmpIdvOutDS in tmpIdvOutDsList:
                        if not options.individualOutDS:
                            Client.addDatasetsToContainer(original_outDS_Name,[tmpIdvOutDS],options.verbose)
                        else:
                            Client.addDatasetsToContainer(tmpDsContMap[tmpIdvOutDS],[tmpIdvOutDS],options.verbose)                            

            # register shadow dataset
            """
            # disable shadow stuff
            if not outputDSexist:
                # loop over all job lists
                for serIndex,tmpJobList in enumerate(splitJobList):
                    tmpDsName = options.outDS+PsubUtils.getSuffixToSplitJobList(serIndex)+suffixShadow
                    Client.addDataset(tmpDsName,options.verbose)
            """        
            # register libDS
            if options.libDS == '' and not (options.nobuild and not options.noCompile):
                Client.addDataset(jobB.destinationDBlock,options.verbose,location=tmpLibDsLocation)

        # submit
        tmpLog.info("submit %s subjobs to %s" % (len(jobList),options.site))
        out = []
        tmpJobsetID = jobsetID
        for tmpJobList in splitJobList:
            # set jobsetID
            if not tmpJobsetID in [None,'NULL']:
                for tmpJob in tmpJobList:
                    tmpJob.jobsetID = tmpJobsetID
            status,tmpOut = Client.submitJobs(tmpJobList,options.verbose)
            out += tmpOut
            # get jobsetID
            if tmpJobsetID in [None,'NULL',-1]:
                if out != [] and out[0] != None and isinstance(out[0][2],types.DictionaryType):
                    tmpJobsetID = out[0][2]['jobsetID']
        if not Client.PandaSites[options.site]['status'] in ['online','brokeroff']:
            tmpLog.warning("%s is %s. Your jobs will wait until it becomes online" % \
                           (options.site,Client.PandaSites[options.site]['status']))
    else:
        print "\n=========="
        # burst submission
        origLibDS = jobB.destinationDBlock
        origOutDS = options.outDS
        prevLibDS = jobB.destinationDBlock
        prevOutDS = options.outDS
        # loop over all sites
        for tmpSite in options.burstSubmit.split(','):
            # cloud
            tmpCloud = Client.PandaSites[tmpSite]['cloud']
            newJobList = []
            newLibDS = '%s.%s' % (origLibDS,tmpSite)
            newOutDS = '%s.%s' % (origOutDS,tmpSite)
            repPatt  = {}
            if options.removeBurstLimit:
                # use all jobs
                limitedJobList = jobList
            else:
                # use buildJob and one runAthena by default
                limitedJobList = jobList[:2]
            for tmpJob in limitedJobList:
                job = copy.deepcopy(tmpJob)
                # set cloud/site
                job.cloud          = tmpCloud
                job.computingSite  = tmpSite
                job.destinationSE  = job.computingSite
                # set destDBlock
                if job.prodSourceLabel == 'panda':
                    job.destinationDBlock = newLibDS
                    # pattern to modify LFN
                    subLibDSPatt = '^'+prevLibDS
                else:
                    job.destinationDBlock = newOutDS
                    # pattern to modify LFN
                    subOutDSPatt = '^'+prevOutDS
                # correct files
                for file in job.Files:
                    # process output/log and lib.tgz
                    if file.type == 'input':
                        # skip buildJob
                        if job.prodSourceLabel == 'panda':
                            continue
                        # only lib.tgz
                        if re.search(subLibDSPatt,file.dataset) == None:
                            continue
                        # set datasets 
                        file.dataset           = newLibDS
                        file.prodDBlock        = newLibDS
                        file.dispatchDBlock    = newLibDS
                    else:
                        # set datasets
                        if job.prodSourceLabel == 'panda':
                            file.dataset           = newLibDS
                            file.prodDBlock        = newLibDS                    
                            file.destinationDBlock = newLibDS
                        else:
                            file.dataset           = newOutDS
                            file.prodDBlock        = newOutDS                    
                            file.destinationDBlock = newOutDS
                        file.destinationSE = job.destinationSE
                    # modify LFN
                    oldLFN = file.lfn
                    if job.prodSourceLabel == 'panda' or file.type == 'input':
                        newLFN = re.sub(subLibDSPatt,file.dataset,oldLFN)
                    else:
                        newLFN = re.sub(subOutDSPatt,file.dataset,oldLFN)
                    file.lfn = newLFN
                    # pattern for parameter replacement
                    repPatt[oldLFN] = newLFN
                # modify jobParams
                for oldLFN,newLFN in repPatt.iteritems():
                    job.jobParameters = re.sub(oldLFN,newLFN,job.jobParameters)
                # append
                newJobList.append(job)
            # submit
            tmpLog.info("submit to %s" % tmpSite)
            status,out = Client.submitJobs(newJobList,options.verbose)
            if status==0:
                tmpLog.info(" OK")
		jobID = out[0][1]
                # record jobID
		tmpJobIdFile = open(jobid_file,'w')
                tmpJobIdFile.write(str(jobID))
                tmpJobIdFile.close()
            else:
                tmpLog.info(" NG : %s" % status)
            time.sleep(2)
        # don't update DB
        sys.exit(0)


    if out == []:
        print '==================='
        tmpLog.error("Job submission was denied")
        sys.exit(EC_Submit)
    # check length
    if len(jobList) != len(out):
        wStr  = "Only %s/%s sub-jobs were accepted " % (len(out),len(jobList))
        wStr += "since the number of sub-jobs in a single submission is limited. "
        wStr += "Please submit the same job again (i.e., the same inDS and outDS). "
        wStr += "New sub-jobs will run only on unused files and outputs will be added to the same outDS\n"
        tmpLog.warning(wStr)
    outstr   = ''
    buildStr = ''
    runStr   = ''
    for index,o in enumerate(out):
        if o != None:
            if index==0:
                # set JobID
                jobID = o[1]
                # set jobsetID
                if isinstance(o[2],types.DictionaryType):
                    jobsetID = o[2]['jobsetID']
            if index==0 and options.libDS=='' and not (options.nobuild and not options.noCompile):
                outstr += "  > build\n"
                outstr += "    PandaID=%s\n" % o[0]
                buildStr = '%s' % o[0]            
            elif (index==1 and options.libDS=='' and not (options.nobuild and not options.noCompile)) or \
                 (index==0 and (options.libDS!='' or options.nobuild)):
                outstr += "  > run\n"
                outstr += "    PandaID=%s" % o[0]
                runStr = '%s' % o[0]                        
            elif index+1==len(out):
                outstr += "-%s" % o[0]
                runStr += '-%s' % o[0]
    if status == 0 and (brokerageLogs != [] or userBrokerLogs != []):
        Client.sendBrokerageLog(jobID,jobsetID,brokerageLogs+userBrokerLogs,options.verbose)
    print '==================='
    if not jobsetID in [None,'NULL']:            
        print ' JobsetID  : %s' % jobsetID
    print ' JobID  : %s' % jobID
    print ' Status : %d' % status
    print outstr

    # create dir for DB
    dbdir = os.path.expanduser(os.environ['PANDA_CONFIG_ROOT'])
    if not os.path.exists(dbdir):
        os.makedirs(dbdir)

    # record jobID
    tmpJobIdFile = open(jobid_file,'w')
    tmpJobIdFile.write(str(jobID))
    tmpJobIdFile.close()

    # record libDS
    if options.libDS == '' and not (options.nobuild and not options.noCompile):
        tmpFile = open(libds_file,'w')
        tmpFile.write(jobB.destinationDBlock)
        tmpFile.close()

# go back to current dir
os.chdir(currentDir)

# check site occupancy
if (siteSpecified or expCloudFlag) and options.burstSubmit == '':
    Client.checkQueuedAnalJobs(options.site,options.verbose)

# try another site if input files remain
options.crossSite -= 1
if options.crossSite > 0 and options.inDS != '' and not siteSpecified:
    if missList != []:
        PsubUtils.runPathenaRec(runConfig,missList,tmpDir,fullExecString,options.nfiles,inputFileMap,
                                options.site,options.crossSite,archiveName,options.removedDS,
                                options.inDS,options.goodRunListXML,options.eventPickEvtList,
                                options.panda_devidedByGUID,options.panda_dbRelease,
                                jobsetID,orig_trfStr,options.singleLine,
                                True,eventPickRunEvtDat,useTagParentLookup,options.verbose)
# succeeded
sys.exit(0)
