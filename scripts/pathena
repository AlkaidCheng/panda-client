#!/bin/bash

"exec" "python" "-u" "-Wignore" "$0" "$@"

import os
import re
import sys
import time
import copy
import atexit
import commands
import optparse
import shelve
import datetime
import urllib
import random
import fcntl
import types
import traceback
import pickle

####################################################################

# error code
EC_Config    = 10
EC_CMT       = 20
EC_Extractor = 30
EC_Dataset   = 40
EC_Post      = 50
EC_Archive   = 60
EC_Split     = 70
EC_MyProxy   = 80
EC_Submit    = 90

#@ Number of events to skip in the file
nEventsToSkip=0
#@ Events blok counter per file
nSkips =0

# default cloud/site
defaultCloud = None

# suffix for shadow dataset
suffixShadow = "_shadow"


usage = """%prog [options] <jobOption1.py> [<jobOption2.py> [...]]

'%prog --help' prints a summary of the options

  HowTo is available at https://twiki.cern.ch/twiki/bin/view/Atlas/PandaAthena"""


# command-line parameters
optP = optparse.OptionParser(usage=usage,conflict_handler="resolve")
# special options
optP.add_option('--version',action='store_const',const=True,dest='version',default=False,
                help='Displays version')
optP.add_option('--split', action='store', dest='split',  default=-1,
                type='int',    help='Number of sub-jobs to which a job is split')
optP.add_option('--nFilesPerJob', action='store', dest='nFilesPerJob',  default=-1, type='int', help='Number of files on which each sub-job runs')
optP.add_option('--nEventsPerJob', action='store', dest='nEventsPerJob',  default=-1,
                type='int',    help='Number of events on which each sub-job runs')
optP.add_option('--nEventsPerFile', action='store', dest='nEventsPerFile',  default=0,
                type='int',    help='Number of events per file')
optP.add_option('--nGBPerJob',action='store',dest='nGBPerJob',default=-1, help='Instantiate one sub job per NGBPERJOB GB of input files. --nGBPerJob=MAX sets the size to the default maximum value')
optP.add_option('--site', action='store', dest='site',  default="AUTO",
                type='string',    help='Site name where jobs are sent (default:AUTO)')
optP.add_option('--inDS',  action='store', dest='inDS',  default='',
                type='string', help='Input dataset names. wildcard and/or comma can be used to concatenate multiple datasets')
optP.add_option('--minDS',  action='store', dest='minDS',  default='',
                type='string', help='Dataset name for minimum bias stream')
optP.add_option('--nMin',  action='store', dest='nMin',  default=-1,
                type='int', help='Number of minimum bias files per one signal file')
optP.add_option('--cavDS',  action='store', dest='cavDS',  default='',
                type='string', help='Dataset name for cavern stream')
optP.add_option('--nCav',  action='store', dest='nCav',  default=-1,
                type='int', help='Number of cavern files per one signal file')
optP.add_option('--libDS', action='store', dest='libDS', default='',
                type='string', help='Name of a library dataset')
optP.add_option('--goodRunListXML', action='store', dest='goodRunListXML', default='',
                type='string', help='Good Run List XML which will be converted to datasets by AMI')
optP.add_option('--goodRunListDataType', action='store', dest='goodRunDataType', default='',
                type='string', help='specify data type when converting Good Run List XML to datasets, e.g, AOD (default)')
optP.add_option('--goodRunListProdStep', action='store', dest='goodRunProdStep', default='',
                type='string', help='specify production step when converting Good Run List to datasets, e.g, merge (default)')
optP.add_option('--goodRunListDS', action='store', dest='goodRunListDS', default='',
                type='string', help='A comma-separated list of pattern strings. Datasets which are converted from Good Run List XML will be used when they match with one of the pattern strings. Either \ or "" is required when a wild-card is used. If this option is omitted all datasets will be used')
optP.add_option('--eventPickEvtList',action='store',dest='eventPickEvtList',default='',
                type='string', help='a file name which contains a list of runs/events for event picking')
optP.add_option('--eventPickDataType',action='store',dest='eventPickDataType',default='',
                type='string', help='type of data for event picking. one of AOD,ESD,RAW')
optP.add_option('--eventPickStreamName',action='store',dest='eventPickStreamName',default='',
                type='string', help='stream name for event picking. e.g., physics_CosmicCaloEM')
optP.add_option('--eventPickDS',action='store',dest='eventPickDS',default='',
                type='string', help='A comma-separated list of pattern strings. Datasets which are converted from the run/event list will be used when they match with one of the pattern strings. Either \ or "" is required when a wild-card is used. e.g., data\*')
optP.add_option('--useCommonHalo', action='store_const', const=False, dest='useCommonHalo',  default=True,
                help="use an integrated DS for BeamHalo")
optP.add_option('--beamHaloDS',  action='store', dest='beamHaloDS',  default='',
                type='string', help='Dataset name for beam halo')
optP.add_option('--beamHaloADS',  action='store', dest='beamHaloADS',  default='',
                type='string', help='Dataset name for beam halo A-side')
optP.add_option('--beamHaloCDS',  action='store', dest='beamHaloCDS',  default='',
                type='string', help='Dataset name for beam halo C-side')
optP.add_option('--nBeamHalo',  action='store', dest='nBeamHalo',  default=-1,
                type='int', help='Number of beam halo files per sub job')
optP.add_option('--nBeamHaloA',  action='store', dest='nBeamHaloA',  default=-1,
                type='int', help='Number of beam halo files for A-side per sub job')
optP.add_option('--nBeamHaloC',  action='store', dest='nBeamHaloC',  default=-1,
                type='int', help='Number of beam halo files for C-side per sub job')
optP.add_option('--useCommonGas', action='store_const', const=False, dest='useCommonGas',  default=True,
                help="use an integrated DS for BeamGas")
optP.add_option('--beamGasDS',  action='store', dest='beamGasDS',  default='',
                type='string', help='Dataset name for beam gas')
optP.add_option('--beamGasHDS',  action='store', dest='beamGasHDS',  default='',
                type='string', help='Dataset name for beam gas Hydrogen')
optP.add_option('--beamGasCDS',  action='store', dest='beamGasCDS',  default='',
                type='string', help='Dataset name for beam gas Carbon')
optP.add_option('--beamGasODS',  action='store', dest='beamGasODS',  default='',
                type='string', help='Dataset name for beam gas Oxygen')
optP.add_option('--nBeamGas',  action='store', dest='nBeamGas',  default=-1,
                type='int', help='Number of beam gas files per sub job')
optP.add_option('--nBeamGasH',  action='store', dest='nBeamGasH',  default=-1,
                type='int', help='Number of beam gas files for Hydrogen per sub job')
optP.add_option('--nBeamGasC',  action='store', dest='nBeamGasC',  default=-1,
                type='int', help='Number of beam gas files for Carbon per sub job')
optP.add_option('--nBeamGasO',  action='store', dest='nBeamGasO',  default=-1,
                type='int', help='Number of beam gas files for Oxygen per sub job')
optP.add_option('--parentDS', action='store', dest='parentDS', default='',
                type='string', help='Parent dataset names. The brokerage takes their locations into account for TAG-based analysis')
optP.add_option('--outDS', action='store', dest='outDS', default='',
                type='string', help='Name of an output dataset. OUTDS will contain all output files')
optP.add_option('--destSE',action='store', dest='destSE',default='',
                type='string', help='Destination strorage element')
optP.add_option('--nFiles', '--nfiles', action='store', dest='nfiles',  default=0,
                type='int',    help='Use an limited number of files in the input dataset')
optP.add_option('--nSkipFiles', action='store', dest='nSkipFiles',  default=0,
                type='int',    help='Skip N files in the input dataset')
optP.add_option('-v', action='store_const', const=True, dest='verbose',  default=False,
                help='Verbose')
optP.add_option('-l', '--long', action='store_const', const=True, dest='long',  default=False,
                help='Send job to a long queue')
optP.add_option('--blong', action='store_const', const=True, dest='blong',  default=False,
                help='Send build job to a long queue')
optP.add_option('--update', action='store_const', const=True, dest='update',  default=False,
                help='Update panda-client to the latest version')
optP.add_option('--cloud',action='store', dest='cloud',default=None,
                type='string', help='cloud where jobs are submitted. default is set according to your VOMS country group')
optP.add_option('--noBuild', action='store_const', const=True, dest='nobuild',  default=False,
                help='Skip buildJob')
optP.add_option('--individualOutDS', action='store_const', const=True, dest='individualOutDS',  default=False,
                help='Create individual output dataset for each data-type. By default, all output files are added to one output dataset')
optP.add_option('--noRandom', action='store_const', const=True, dest='norandom',  default=False,
                help='Enter random seeds manually')
optP.add_option('--useAMIAutoConf',action='store_const',const=True,dest='useAMIAutoConf',default=False,
                help='Use AMI for AutoConfiguration')
optP.add_option('--memory', action='store', dest='memory',  default=-1,
                type='int',    help='Required memory size')
optP.add_option('--maxCpuCount', action='store', dest='maxCpuCount', default=-1, type='int',
                help='Required CPU count in seconds. Mainly to extend time limit for looping detection')
optP.add_option('--official', action='store_const', const=True, dest='official',  default=False,
                help='Produce official dataset')
optP.add_option('--extFile', action='store', dest='extFile',  default='',
                help='pathena exports files with some special extensions (.C, .dat, .py .xml) in the current directory. If you want to add other files, specify their names, e.g., data1.root,data2.doc')
optP.add_option('--excludeFile',action='store',dest='excludeFile',default='',
                help='specify a comma-separated string to exclude files and/or directories when gathering files in local working area. Either \ or "" is required when a wildcard is used. e.g., doc,\*.C')
optP.add_option('--extOutFile', action='store', dest='extOutFile',  default='',
                help='A comma-separated list of extra output files which cannot be extracted automatically. Either \ or "" is required when a wildcard is used. e.g., output1.txt,output2.dat,JiveXML_\*.xml')
optP.add_option('--supStream', action='store', dest='supStream',  default='',
                help='suppress some output streams. e.g., ESD,TAG ')
optP.add_option('--gluePackages', action='store', dest='gluePackages',  default='',
                help='list of glue packages which pathena cannot fine due to empty i686-slc4-gcc34-opt. e.g., External/AtlasHepMC,External/Lhapdf')
optP.add_option('--excludedSite', action='store', dest='excludedSite',  default='',
                help="list of sites which are not used for site section, e.g., ANALY_ABC,ANALY_XYZ")
optP.add_option('--noSubmit', action='store_const', const=True, dest='nosubmit',  default=False,
                help="Don't submit jobs")
optP.add_option('--prodSourceLabel', action='store', dest='prodSourceLabel',  default='',
                help="set prodSourceLabel")
optP.add_option('--processingType', action='store', dest='processingType',  default='pathena',
                help="set processingType")
optP.add_option('--seriesLabel', action='store', dest='seriesLabel',  default='',
                help="set seriesLabel")
optP.add_option('--workingGroup', action='store', dest='workingGroup',  default=None,
                help="set workingGroup")
optP.add_option('--generalInput', action='store_const', const=True, dest='generalInput',  default=False,
                help='Read input files with general format except POOL,ROOT,ByteStream')
optP.add_option('--crossSite',action='store',dest='crossSite',default=5,
                type='int',help='submit jobs to N sites at most when datasets in container split over many sites (N=5 by default)')
optP.add_option('--tmpDir', action='store', dest='tmpDir', default='',
                type='string', help='Temporary directory in which an archive file is created')
optP.add_option('--shipInput', action='store_const', const=True, dest='shipinput',  default=False,
                help='Ship input files to remote WNs')
optP.add_option('--noLock', action='store_const', const=True, dest='nolock',  default=False,
                help="Don't create a lock for local database access")
optP.add_option('--fileList', action='store', dest='filelist', default='',
                type='string', help='List of files in the input dataset to be run')
optP.add_option('--myproxy', action='store', dest='myproxy', default='myproxy.cern.ch',
                type='string', help='Name of the myproxy server')
optP.add_option('--dbRelease', action='store', dest='dbRelease', default='',
                type='string', help='DBRelease or CDRelease (DatasetName:FileName). e.g., ddo.000001.Atlas.Ideal.DBRelease.v050101:DBRelease-5.1.1.tar.gz. If --dbRelease=LATEST, the latest DBRelease is used')
optP.add_option('--dbRunNumber', action='store', dest='dbRunNumber', default='',
                type='string', help='RunNumber for DBRelease or CDRelease. If this option is used some redundant files are removed to save disk usage when unpacking DBRelease tarball. e.g., 0091890')
optP.add_option('--addPoolFC', action='store', dest='addPoolFC',  default='',
                help="file names to be inserted into PoolFileCatalog.xml except input files. e.g., MyCalib1.root,MyGeom2.root") 
optP.add_option('--skipScan', action='store_const', const=True, dest='skipScan', default=False,
                help='Skip LRC/LFC lookup at job submission')
optP.add_option('--inputFileList', action='store', dest='inputFileList', default='',
                type='string', help='name of file which contains a list of files to be run in the input dataset')
optP.add_option('--removeFileList', action='store', dest='removeFileList', default='',
                type='string', help='name of file which contains a list of files to be removed from the input dataset')
optP.add_option('--removedDS', action='store', dest='removedDS', default='',
                type='string', help="don't use datasets in the input dataset container")
optP.add_option('--corCheck', action='store_const', const=True, dest='corCheck',  default=False,
                help='Enable a checker to skip corrupted files')
optP.add_option('--prestage', action='store_const', const=True, dest='prestage',  default=False,
                help='EXPERIMENTAL : Enable prestager. Make sure that you are authorized')
optP.add_option('--voms', action='store', dest='vomsRoles',  default=None, type='string',
                help="generate proxy with paticular roles. e.g., atlas:/atlas/ca/Role=production,atlas:/atlas/fr/Role=pilot")
optP.add_option('--useNextEvent', action='store_const', const=True, dest='useNextEvent',  default=False,
                help="Set this option if your jobO uses theApp.nextEvent() e.g. for G4")
optP.add_option('--ara', action='store_const', const=True, dest='ara',  default=False,
                help='obsolete. Please use prun instead')
optP.add_option('--ares', action='store_const', const=True, dest='ares',  default=False,
                help='obsolete. Please use prun instead')
optP.add_option('--araOutFile', action='store', dest='araOutFile',  default='',
                help='define output files for ARA, e.g., output1.root,output2.root')
optP.add_option('--trf', action='store', dest='trf',  default=False,
                help='run transformation, e.g. --trf "csc_atlfast_trf.py %IN %OUT.AOD.root %OUT.ntuple.root -1 0"')
optP.add_option('--spaceToken', action='store', dest='spaceToken', default='',
                type='string', help='spacetoken for outputs. e.g., ATLASLOCALGROUPDISK')
optP.add_option('--notSkipMissing', action='store_const', const=True, dest='notSkipMissing',  default=False,
                help='If input files are not read from SE, they will be skipped by default. This option disables the functionality')
optP.add_option('--burstSubmit', action='store', dest='burstSubmit', default='',
                type='string', help="Please don't use this option. Only for site validation by experts")
optP.add_option('--removeBurstLimit', action='store_const', const=True, dest='removeBurstLimit', default=False,
                help="Please don't use this option. Only for site validation by experts")
optP.add_option('--devSrv', action='store_const', const=True, dest='devSrv',  default=False,
                help="Please don't use this option. Only for developers to use the dev panda server")
optP.add_option('--useAIDA', action='store_const', const=True, dest='useAIDA',  default=False,
                help="use AIDA")
optP.add_option('--inputType', action='store', dest='inputType', default='',
                type='string', help='File type in input dataset which contains multiple file types')
optP.add_option('--mcData', action='store', dest='mcData', default='',
                type='string', help='Create a symlink with linkName to .dat which is contained in input file')
optP.add_option('--pfnList', action='store', dest='pfnList', default='',
                type='string', help='Name of file which contains a list of input PFNs. Those files can be un-registered in DDM')
optP.add_option('--outputPath',action='store',dest='outputPath', default='./',
                type='string', help='Physical path of output directory relative to a root path')
optP.add_option('--useExperimental', action='store_const', const=True, dest='useExperimental',  default=False,
                help='use experimental features')
# athena options
optP.add_option('-c',action='store',dest='singleLine',type='string',default='',metavar='COMMAND',
                help='One-liner, runs before any jobOs')
optP.add_option('-p',action='store',dest='preConfig',type='string',default='',metavar='BOOTSTRAP',
                help='location of bootstrap file')
optP.add_option('-s',action='store_const',const=True,dest='codeTrace',default=False,
                help='show printout of included files')
# internal parameters
optP.add_option('--panda_srvURL', action='store', dest='panda_srvURL', default='',
                type='string', help='internal parameter')
optP.add_option('--panda_runConfig', action='store', dest='panda_runConfig', default='',
                type='string', help='internal parameter')
optP.add_option('--panda_srcName', action='store', dest='panda_srcName', default='',
                type='string', help='internal parameter')
optP.add_option('--panda_inDS', action='store', dest='panda_inDS', default='',
                type='string', help='internal parameter')
optP.add_option('--panda_inDSForEP', action='store', dest='panda_inDSForEP', default='',
                type='string', help='internal parameter')
optP.add_option('--panda_origFullExecString', action='store', dest='panda_origFullExecString', default='',
                type='string', help='internal parameter')


# parse options
options,args = optP.parse_args()
if options.verbose:
    print options

# display version
from pandatools import PandaToolsPkgInfo
if options.version:
    print "Version: %s" % PandaToolsPkgInfo.release_version
    sys.exit(0)

from pandatools import Client
from pandatools import PsubUtils
from pandatools import AthenaUtils
from pandatools import GlobalConfig
from pandatools import PLogger

# update panda-client
if options.update:
    res = PsubUtils.updatePackage(options.verbose)
    if res:
	sys.exit(0)
    else:
	sys.exit(1)

# full execution string
fullExecString = PsubUtils.convSysArgv()

# max size per job
maxTotalSize = Client.maxTotalSize
safetySize   = Client.safetySize

# set grid source file
globalConf = GlobalConfig.getConfig()
if globalConf.grid_src != '' and not os.environ.has_key('PATHENA_GRID_SETUP_SH'):
    os.environ['PATHENA_GRID_SETUP_SH'] = globalConf.grid_src

# get logger
tmpLog = PLogger.getPandaLogger()

# use dev server
if options.devSrv:
    Client.useDevServer()

# set server
if options.panda_srvURL != '':
    Client.setServer(options.panda_srvURL)

# version check
PsubUtils.checkPandaClientVer(options.verbose)

# exclude sites
if options.excludedSite != '':
    Client.excludeSite(options.excludedSite)

# use certain sites
useRandomCloud = []
if re.search(',',options.site) != None:
    options.site,useRandomCloud = Client.useCertainSites(options.site)

# site specified
siteSpecified = True
if options.site == 'AUTO':
    siteSpecified = False

# use outputPath as outDS
if Client.isDQ2free(options.site):
    if options.outDS != '':
	options.outputPath = options.outDS
    else:
	options.outputPath = './'
    options.outDS = options.outputPath

# keep original outDS
original_outDS_Name = options.outDS
     
# reset crossSite unless container is used for output 
if not original_outDS_Name.endswith('/'):
    options.crossSite = 0

# set inDS for recursive goodRunListXML
if options.panda_inDS != '':
    options.inDS = options.panda_inDS
    options.goodRunListXML = ''

# set inDS for recursive event picking
if options.panda_inDSForEP != '':
    options.inDS = options.panda_inDSForEP
    options.eventPickEvtList = ''

# error
if options.outDS == '':
    tmpLog.error("no outDS is given\n pathena [--inDS input] --outDS output myJobO.py")
    sys.exit(EC_Config)
if options.split < -1 :
    tmpLog.error("Number of jobs should be a positive integer")
    sys.exit(EC_Config)
if options.shipinput and options.inDS != '' and options.pfnList != '':
    tmpLog.error("--shipInput, --pfnList and --inDS cannot be used at the same time")
    sys.exit(EC_Config)

# libDS
libds_file = '%s/libds_pathena.dat' % os.environ['PANDA_CONFIG_ROOT']
if options.libDS == 'LAST':
    if not os.path.exists(libds_file):
        tmpLog.error("LAST cannot be used until you submit at least one job without --libDS")
        sys.exit(EC_Config)
    # read line
    tmpFile = open(libds_file)
    tmpLibDS = tmpFile.readline()
    tmpFile.close()
    # remove \n
    tmpLibDS = tmpLibDS.replace('\n','')
    # set
    options.libDS = tmpLibDS

# absolute path for PFN list
if options.pfnList != '':
    options.pfnList = os.path.realpath(options.pfnList)

# burst submission
if options.burstSubmit != '':
    # don't scan LRC/LFC
    options.skipScan = True
    # reset cloud/site. They will be overwritten at submission
    options.cloud = None
    options.site  = None
    # disable individual output
    options.individualOutDS = False
    # check libDS stuff
    if options.libDS != '' or options.nobuild:
        tmpLog.error("--libDS or --nobuild cannot be used together with --burstSubmit")
        sys.exit(EC_Config)
        

# split options are mutually exclusive
if (options.nFilesPerJob > 0 and options.nEventsPerJob > 0 and options.nGBPerJob != -1):
    tmpLog.error("split by files and split by events and split by file size can not be defined sumaltaneously")
    sys.exit(EC_Config)


# check nGBPerJob
if options.nGBPerJob != -1:
    # convert to int
    try:
        if options.nGBPerJob != 'MAX':
            options.nGBPerJob = int(options.nGBPerJob)
    except:
        tmpLog.error("nGBPerJob must be an integer or MAX")
        sys.exit(EC_Config)
    # check negative    
    if options.nGBPerJob <= 0:
        tmpLog.error("nGBPerJob must be positive")
        sys.exit(EC_Config)
    # convert MAX to default max value
    if options.nGBPerJob == 'MAX':
        options.nGBPerJob = maxTotalSize
    else:
        # convert to Bytes
        options.nGBPerJob = long(int(options.nGBPerJob)*1024*1024*1024)
        # reset max size per job
        maxTotalSize = options.nGBPerJob
    

# check DBRelease
if options.dbRelease != '' and (options.dbRelease.find(':') == -1 and options.dbRelease !='LATEST'):
    tmpLog.error("invalid argument for --dbRelease. Must be DatasetName:FileName or LATEST")  
    sys.exit(EC_Config)

# Good Run List
if options.goodRunListXML != '' and options.inDS != '':
    tmpLog.error("cannnot use --goodRunListXML and --inDS at the same time")
    sys.exit(EC_Config)

# event picking
if options.eventPickEvtList != '' and options.inDS != '':
    tmpLog.error("cannnot use --eventPickEvtList and --inDS at the same time")
    sys.exit(EC_Config)

# param check for event picking
if options.eventPickEvtList != '':
    if options.eventPickDataType == '':
        tmpLog.error("--eventPickDataType must be specified")
        sys.exit(EC_Config)
    if options.eventPickStreamName == '':
        tmpLog.error("--eventPickStreamName must be specified")
        sys.exit(EC_Config)
    if options.trf != False:
        tmpLog.error("--eventPickEvtList doesn't work with --trf until official transformations support event picking")
        sys.exit(EC_Config)
        
    
# additinal files
options.extFile = options.extFile.split(',')
options.extOutFile = options.extOutFile.split(',')
try:
    options.extOutFile.remove('')
except:
    pass

# removed datasets
if options.removedDS == '':
    options.removedDS = []
else:
    options.removedDS = options.removedDS.split(',')
    
# glue packages
options.gluePackages = options.gluePackages.split(',')
try:
    options.gluePackages.remove('')
except:
    pass

# set excludeFile
if options.excludeFile != '':
    AthenaUtils.setExcludeFile(options.excludeFile)

# set ara on when ares is used
if options.ares:
    options.ara = True

# output files for ARA
if options.ara and options.araOutFile == '':
    tmpLog.error("--araOutFile is needed when ARA (--ara) is used")
    sys.exit(EC_Config)
for tmpName in options.araOutFile.split(','):
    if tmpName != '':
        options.extOutFile.append(tmpName)

# file list
tmpList = options.filelist.split(',')
options.filelist = []
for tmpItem in tmpList:
    if tmpItem == '':
        continue
    # wild card
    tmpItem = tmpItem.replace('*','.*')
    # append
    options.filelist.append(tmpItem) 
# read file list from file
if options.inputFileList != '':
    rFile = open(options.inputFileList)
    for line in rFile:
        line = re.sub('\n','',line)
        options.filelist.append(line)
    rFile.close()

# removed files
if options.removeFileList == '':
    # empty
    options.removeFileList = []
else:
    # read from file
    rList = []
    rFile = open(options.removeFileList)
    for line in rFile:
        line = re.sub('\n','',line)        
        rList.append(line)
    rFile.close()
    options.removeFileList = rList

# file type
options.inputType = options.inputType.split(',')
try:
    options.inputType.remove('')
except:
    pass

# suppressed streams
options.supStream = options.supStream.upper().split(',')
try:
    options.supStream.remove('')
except:
    pass

# set nFilesPerJob for MC data
if options.mcData != '':
    options.nFilesPerJob = 1
    
# set nfiles
if options.nFilesPerJob > 0 and options.nfiles == 0 and options.split > 0:
    options.nfiles = options.nFilesPerJob * options.split

# check grid-proxy
gridPassPhrase,vomsFQAN = PsubUtils.checkGridProxy('',False,options.verbose,options.vomsRoles)

# add allowed sites# 
if (not siteSpecified) and options.burstSubmit == '':
    tmpSt = Client.addAllowedSites(options.verbose)
    if not tmpSt:
        tmpLog.error("Failed to get allowed site list")
        sys.exit(EC_Config)

# set cloud according to country FQAN
expCloudFlag = False
if options.cloud == None and options.burstSubmit == '':
    options.cloud = PsubUtils.getCloudUsingFQAN(defaultCloud,options.verbose,useRandomCloud)
elif options.cloud != None:
    # use cloud explicitly
    expCloudFlag = True

# correct site
if options.site != 'AUTO' and options.burstSubmit == '':
    origSite = options.site
    # patch for BNL
    if options.site in ['BNL',"ANALY_BNL"]:
        options.site = "ANALY_BNL_ATLAS_1"
    # patch for CERN
    if options.site in ['CERN']:
        options.site = "ANALY_CERN"        
    # try to convert DQ2ID to PandaID
    pID = PsubUtils.convertDQ2toPandaID(options.site)
    if pID != '':
        options.site = pID
    # add ANALY
    if not options.site.startswith('ANALY_'):
        options.site = 'ANALY_%s' % options.site
    # check
    if not Client.PandaSites.has_key(options.site):
        tmpLog.error("unknown siteID:%s" % origSite)
        sys.exit(EC_Config)
    # set cloud
    options.cloud = Client.PandaSites[options.site]['cloud']

# check cloud
if options.burstSubmit == '':
    foundCloud = False
    for tmpID,spec in Client.PandaSites.iteritems():
        if options.cloud == spec['cloud']:
            foundCloud = True
            break
    if not foundCloud:
        tmpLog.error("unsupported cloud:%s" % options.cloud)
        sys.exit(EC_Config)

# get DN
distinguishedName = PsubUtils.getDN()

# get nickname
nickName = PsubUtils.getNickname()

if distinguishedName == '' and nickName == '':
    sys.exit(EC_Config)

# check outDS format
if not PsubUtils.checkOutDsName(options.outDS,distinguishedName,options.official,nickName,options.site,vomsFQAN):
    tmpLog.error("invalid output datasetname:%s" % options.outDS)
    sys.exit(EC_Config)

# check destSE
if options.destSE != '':
    if not PsubUtils.checkDestSE(options.destSE,options.outDS,options.verbose):
        sys.exit(EC_Config)

# save current dir
currentDir = os.path.realpath(os.getcwd())

# create tmp dir
if options.tmpDir == '':
    tmpDir = '%s/%s' % (currentDir,commands.getoutput('uuidgen'))
else:
    tmpDir = '%s/%s' % (options.tmpDir,commands.getoutput('uuidgen'))    
os.makedirs(tmpDir)

# set tmp dir in Client
Client.setGlobalTmpDir(tmpDir)

# exit action
delFilesOnExit = []
def _onExit(dir,files):
    for tmpFile in files:
        commands.getoutput('rm -rf %s' % tmpFile)        
    commands.getoutput('rm -rf %s' % dir)
atexit.register(_onExit,tmpDir,delFilesOnExit)


# get Athena versions
stA,retA = AthenaUtils.getAthenaVer()
# failed
if not stA:
    sys.exit(EC_CMT)
workArea  = retA['workArea'] 
athenaVer = retA['athenaVer'] 
groupArea = retA['groupArea'] 
cacheVer  = retA['cacheVer'] 
nightVer  = retA['nightVer']

# get run directory
# remove special characters                    
sString=re.sub('[\+]','.',workArea)
runDir = re.sub('^%s' % sString, '', currentDir)
if runDir == currentDir:
    tmpLog.error("you need to run pathena in a directory under %s" % workArea)
    sys.exit(EC_Config)
elif runDir == '':
    runDir = '.'
elif runDir.startswith('/'):
    runDir = runDir[1:]
runDir = runDir+'/'

# check unmerge dataset
PsubUtils.checkUnmergedDataset(options.inDS,options.parentDS)

# good run list
if options.goodRunListXML != '':
    # look for pyAMI
    status,options.inDS = AthenaUtils.convertGoodRunListXMLtoDS(options.goodRunListXML,
                                                                options.goodRunDataType,
                                                                options.goodRunProdStep,
								options.goodRunListDS,
                                                                options.verbose)
    if not status:
        tmpLog.error("failed to convert GoodRunListXML")
        sys.exit(EC_Config)
    if options.inDS == '':
        tmpLog.error("no datasets were extracted from AMI using %s" % options.goodRunListXML)
        sys.exit(EC_Config)

# event picking
if options.eventPickEvtList != '':
    # convert run/evt list to dataset/LFN list
    epDsLFNs,epGuidEvtMap = PsubUtils.getDSsFilesByRunsEvents(currentDir,
                                                              options.eventPickEvtList,
                                                              options.eventPickDataType,
                                                              options.eventPickStreamName,
                                                              options.eventPickDS,
                                                              options.verbose)
    # set param
    options.inDS = ''
    options.filelist = []
    tmpDsNameList = []
    tmpLFNList = []    
    for tmpGUID,tmpDsLFNs in epDsLFNs.iteritems():
        tmpDsName,tmpLFN = tmpDsLFNs
        # set fileFist
        if not tmpLFN in tmpLFNList:
            tmpLFNList.append(tmpLFN)
            options.filelist.append(tmpLFN)
        # set inDS    
        if not tmpDsName in tmpDsNameList:
            tmpDsNameList.append(tmpDsName)
            options.inDS += '%s,' % tmpDsName
    options.inDS = options.inDS[:-1]
    # make run/event list
    eventPickRunEvtDat = '%s/ep_%s.dat' % (currentDir,commands.getoutput('uuidgen'))
    evFH = open(eventPickRunEvtDat,'w') 
    for tmpGUID,tmpRunEvtList in epGuidEvtMap.iteritems():
        for tmpRunNr,rmpEvtNr in tmpRunEvtList:
            evFH.write('%s %s\n' % (tmpRunNr,rmpEvtNr))
    # close        
    evFH.close()
    # add to be deleted on exit
    delFilesOnExit.append(eventPickRunEvtDat)
    
        
# get job options
jobO = ''
if options.trf:
    # replace : to = for backward compatibility
    for optArg in ['DB','RNDM']:
        options.trf = re.sub('%'+optArg+':','%'+optArg+'=',options.trf)
    # use trf's parameters
    jobO = options.trf
else:
    # get jobOs from command-line
    if options.preConfig != '':
        jobO += '-p %s ' % options.preConfig
    if options.singleLine != '':
        options.singleLine = options.singleLine.replace('"','\'')
        jobO += '-c "%s" ' % options.singleLine
    for arg in args:
        jobO += ' %s' % arg
if jobO == "":
    tmpLog.error("no jobOptions is given\n   pathena [--inDS input] --outDS output myJobO.py")
    sys.exit(EC_Config)

# ARA uses trf I/F
if options.ara:
    if options.ares:
        jobO = "athena.py " + jobO        
    elif jobO.endswith(".C"):
        jobO = "root -l " + jobO
    else:
        jobO = "python " + jobO        
    options.trf = jobO


if options.panda_runConfig == '':
    # extract run configuration    
    tmpLog.info('extracting run configuration')
    # run ConfigExtractor for normal jobO 
    ret,runConfig = AthenaUtils.extractRunConfig(jobO,options.supStream,options.useAIDA,options.shipinput,
                                                 options.trf,verbose=options.verbose,
						 useAMI=options.useAMIAutoConf,inDS=options.inDS,
						 tmpDir=tmpDir)
else:
    # load from file
    ret = True
    tmpRunConfFile = open(options.panda_runConfig)
    runConfig = pickle.load(tmpRunConfFile)
    tmpRunConfFile.close()
if not options.trf:
    # extractor failed
    if not ret:
        sys.exit(EC_Extractor)
    # shipped files
    if runConfig.other.inputFiles:
        for fileName in runConfig.other.inputFiles:
            # append .root for tag files
            if runConfig.other.inColl:
                match = re.search('\.root(\.\d+)*$',fileName)
                if match == None:
                    fileName = '%s.root' % fileName
            # check ship files in the current dir
            if not os.path.exists(fileName):
                tmpLog.error("%s needs exist in the current directory when --shipInput is used" % fileName)
                sys.exit(EC_Extractor)
            # append to extFile
            options.extFile.append(fileName)
            if not runConfig.input.shipFiles:
                runConfig.input['shipFiles'] = []
            runConfig.input['shipFiles'].append(fileName)
    # generator files
    if runConfig.other.rndmGenFile:
        # append to extFile
        for fileName in runConfig.other.rndmGenFile:
            options.extFile.append(fileName)
    # Condition file
    if runConfig.other.condInput:
        # append to extFile
        for fileName in runConfig.other.condInput:
            if options.addPoolFC == "":
                options.addPoolFC = fileName
            else:
                options.addPoolFC += ",%s" % fileName
    # set default ref name
    if not runConfig.input.collRefName:
        runConfig.input.collRefName = 'Token'
    # check dupication in extOutFile
    if runConfig.output.alloutputs != False:
        if options.verbose:
            tmpLog.debug("output files : %s" % str(runConfig.output.alloutputs)) 
        for tmpExtOutFile in tuple(options.extOutFile):
            if tmpExtOutFile in runConfig.output.alloutputs:
                tmpLog.warning("removed %s from extOutFile since it is automatically extracted from Athena. You don't need to specify it in extOutFile"
                            % tmpExtOutFile)
                options.extOutFile.remove(tmpExtOutFile)
else:
    # parse parameters for trf
    # AMI tag
    newJobO = ''
    for tmpString in jobO.split(';'):
        match = re.search(' AMI=',tmpString)
        if match == None:
            # use original command
            newJobO += (tmpString + ';')
        else:
            tmpLog.info('getting configration from AMI')
            # get configration using GetCommand.py
            com = 'GetCommand.py ' + re.sub('^[^ ]+ ','',tmpString.strip())
            if options.verbose:
                tmpLog.debug(com)
            amiSt,amiOut = commands.getstatusoutput(com)
            amiSt %= 255
            if amiSt != 0:
                tmpLog.error(amiOut)
                errSt =  'Failed to get configuration from AMI. '
                errSt += 'Using AMI=tag in --trf is disallowed since it may overload the AMI server. '
                errSt += 'Please use explicit configuration parameters in --trf'
                tmpLog.error(errSt)
                sys.exit(EC_Config)
            # get full command string
            fullCommand = ''
            for amiStr in amiOut.split('\n'):
                if amiStr != '':
                    fullCommand = amiStr
            # failed to extract configration        
            if fullCommand == '':
                tmpLog.error(amiOut)
                errSt =  "Failed to extract configuration from AMI's output"
                tmpLog.error(errSt)
                sys.exit(EC_Config)
            # replace
            newJobO += (fullCommand + ';')
    # remove redundant ;
    newJobO = newJobO[:-1]
    # replace
    if newJobO != '':
        jobO = newJobO
        if options.verbose:
            tmpLog.debug('new jobO : '+jobO)
    # output                
    oneOut = False
    # replace ; for job sequence
    tmpString = re.sub(';',' ',jobO)
    # look for %OUT
    for tmpItem in tmpString.split():
        match = re.search('\%OUT\.(.+)',tmpItem)
        if match:
            # append basenames to extOutFile
            tmpOutName = match.group(1)
            if not tmpOutName in options.extOutFile:
                options.extOutFile.append(tmpOutName)
                oneOut = True
    # warning if no output
    if not oneOut:
        if not options.ara:
            tmpLog.warning("argument of --trf doesn't contain any %OUT")

# no output jobs
tmpOutKeys = runConfig.output.keys()
for tmpIgnorKey in ['outUserData','alloutputs']:
    try:
        tmpOutKeys.remove(tmpIgnorKey)
    except:
        pass
if tmpOutKeys == [] and options.extOutFile == []:
    errStr  = "No output stream was extracted from jobOs or --trf. "
    if not options.trf:
	errStr += "If your job defines an output without Athena framework "
	errStr += "(e.g., using ROOT.TFile.Open instead of THistSvc) "
	errStr += "please specify the output filename by using --extOutFile. "
	errStr += "Or if you define the output with a relatively new mechanism "
	errStr += "please report it to Savannah to update the automatic extractor" 
    tmpLog.error(errStr)  
    sys.exit(EC_Extractor)

# set extOutFile to runConfig
if options.extOutFile != []:
    runConfig.output['extOutFile'] = options.extOutFile

# check ship files in the current dir
if not runConfig.input.shipFiles:
    runConfig.input.shipFiles = []
for file in runConfig.input.shipFiles:
    if not os.path.exists(file):
        tmpLog.error("%s needs exist in the current directory when using --shipInput" % file)
        sys.exit(EC_Extractor)

# get random number
runConfig.other['rndmNumbers'] = []
if not runConfig.other.rndmStream:
    runConfig.other.rndmStream = []
if len(runConfig.other.rndmStream) != 0:
    if options.norandom:
        print
        print "Initial random seeds need to be defined."
        print "Enter two numbers for each random stream."
        print "  e.g., PYTHIA : 4789899 989240512"
        print
    for stream in runConfig.other.rndmStream:
        if options.norandom:
            # enter manually
            while True:
                randStr = raw_input("%s : " % stream)
                num = randStr.split()
                if len(num) == 2:
                    break
                print " Two numbers are needed"
            runConfig.other.rndmNumbers.append([int(num[0]),int(num[1])])
        else:
            # automatic
            runConfig.other.rndmNumbers.append([random.randint(1,5000000),random.randint(1,5000000)])
    if options.norandom:
        print
if runConfig.other.G4RandomSeeds:
    if options.norandom:
        print
        print "Initial G4 random seeds need to be defined."
        print "Enter one positive number."
        print
        # enter manually
        while True:
            num = raw_input("SimFlags.SeedsG4=")
            try:
                num = int(num)
                if num > 0:
                    runConfig.other.G4RandomSeeds = num
                    break
            except:
                pass
        print    
    else:
        # automatic
        runConfig.other.G4RandomSeeds = random.randint(1,10000)
else:
    # set -1 to disable G4 Random Seeds
    runConfig.other.G4RandomSeeds = -1
    


#####################################################################
# archive sources and send it to HTTP-reachable location

if options.panda_srcName != '':
    # reuse src
    tmpLog.info('reuse source files')
    archiveName = options.panda_srcName
    # go to tmp dir
    os.chdir(tmpDir)
else:
    # extract jobOs with full pathnames
    for tmpItem in jobO.split():
        if re.search('^/.*\.py$',tmpItem) != None:
            # set random name to avoid overwriting
            tmpName = tmpItem.split('/')[-1]
            tmpName = '%s_%s' % (commands.getoutput('uuidgen'),tmpName)
            # set
            AthenaUtils.fullPathJobOs[tmpItem] = tmpName
            
    # copy some athena specific files
    AthenaUtils.copyAthenaStuff(currentDir)

    # set extFile
    AthenaUtils.setExtFile(options.extFile)

    archiveName = ""
    if options.libDS == '' and not options.nobuild:
        # archive sources
	archiveName,archiveFullName = AthenaUtils.archiveSourceFiles(workArea,runDir,currentDir,tmpDir,
                                                                     options.verbose,options.gluePackages) 
    else:
        # archive jobO
	archiveName,archiveFullName = AthenaUtils.archiveJobOFiles(workArea,runDir,currentDir,
                                                                   tmpDir,options.verbose)

    # archive InstallArea
    if options.libDS == '':
	AthenaUtils.archiveInstallArea(workArea,groupArea,archiveName,archiveFullName,
                                       tmpDir,options.nobuild,options.verbose)

    # back to tmp dir        
    os.chdir(tmpDir)

    # remove some athena specific files
    AthenaUtils.deleteAthenaStuff(currentDir)

    # compress
    status,out = commands.getstatusoutput('gzip %s' % archiveName)
    archiveName += '.gz'
    if status != 0 or options.verbose:
        print out

    # check archive
    status,out = commands.getstatusoutput('ls -l %s' % archiveName)
    if status != 0:
        print out
        tmpLog.error("Failed to archive working area.\n        If you see 'Disk quota exceeded', try '--tmpDir /tmp'") 
        sys.exit(EC_Archive)

    # check symlinks
    tmpLog.info("checking symbolic links")
    status,out = commands.getstatusoutput('tar tvfz %s' % archiveName)
    if status != 0:
        tmpLog.error("Failed to expand archive")
        sys.exit(EC_Archive)
    symlinks = []    
    for line in out.split('\n'):
        items = line.split()
        if items[0].startswith('l') and items[-1].startswith('/'):
            symlinks.append(line)
    if symlinks != []:
        tmpStr  = "Found some unresolved symlinks which may cause a problem\n"
        tmpStr += "     See, e.g., http://savannah.cern.ch/bugs/?43885\n"
        tmpStr += "   Please ignore if you believe they are harmless"
        tmpLog.warning(tmpStr)
        for symlink in symlinks:
            print "  %s" % symlink

    # put sources/jobO via HTTP POST
    if not options.nosubmit:
        tmpLog.info("uploading source/jobO files")
        status,out = Client.putFile(archiveName,options.verbose)
        if out != 'True':
            print out
            tmpLog.error("Failed with %s" % status)
            sys.exit(EC_Post)


####################################################################3
# datasets 

# get the latest of DBRelease
if options.dbRelease == 'LATEST':
    options.dbRelease = Client.getLatestDBRelease(options.verbose)

# check if output dataset is unique
outputDSexist = False
outputContExist = False
outputIndvDSlist = {}
if not Client.isDQ2free(options.site):
    tmpDatasets = Client.getDatasets(options.outDS,options.verbose)
    if len(tmpDatasets) != 0:
        if original_outDS_Name.endswith('/'):
            outputContExist = True
        else:
            outputDSexist = True
        # get real datasetname for case sensitivity    
        options.outDS = PsubUtils.getRealDatasetName(options.outDS,tmpDatasets)
# get exsisting individualOutDS
if (outputDSexist or outputContExist) and options.individualOutDS:
    outputIndvDSlist = Client.getDatasets("%s*" % options.outDS,options.verbose,True)    

# check if shadow dataset exists
shadowDSexist = False
if not Client.isDQ2free(options.site):
    tmpDatasets = Client.getDatasets("%s%s" % (options.outDS,suffixShadow),options.verbose)
    if len(tmpDatasets) != 0:
        shadowDSexist = True

# set location when outDS or libDS already exists
checkLibDS = True
if outputDSexist and options.destSE == '':
    checkLibDS = False
    if options.verbose:
        tmpLog.debug("get locations for outDS:%s" % options.outDS)
    outDSlocations = Client.getLocations(options.outDS,[],options.cloud,True,options.verbose)
    if outDSlocations == []:
        tmpLog.error("cannot find locations for existing output dataset:%s" % options.outDS)
        sys.exit(EC_Dataset)
    # keep origianl site
    if Client.PandaSites.has_key(options.site) and \
       (Client.PandaSites[options.site]['ddm'] in outDSlocations or \
        Client.convSrmV2ID(Client.PandaSites[options.site]['ddm']) in outDSlocations):
        pass
    else:
        # convert DQ2ID to Panda siteID
	tmpConvIDs = []
        for outDSlocation in outDSlocations:
            tmpConvIDs += Client.convertDQ2toPandaIDList(outDSlocation)
        # not found
        if tmpConvIDs == []:
	    checkLibDS = True
	    options.destSE = outDSlocations[0]
	    tmpLog.info("set destSE:%s because outDS:%s already exists there" % \
		(options.destSE,options.outDS))
	# chose one
	if len(tmpConvIDs) == 1:
	    convID = tmpConvIDs[0]
        else:
            # run brokerage to be sent to free site
            convID = PsubUtils.runBrokerageForCompSite(tmpConvIDs,'Atlas-%s' % athenaVer,cacheVer,options.verbose)
        # set    
        options.site  = convID
        options.cloud = Client.PandaSites[convID]['cloud']
        tmpLog.info("set site:%s cloud:%s because outDS:%s already exists at %s" % \
                    (options.site,options.cloud,options.outDS,outDSlocations))                

if options.libDS != '' and not Client.isDQ2free(options.site):
    # get real datasetname for case sensitivity    
    tmpDatasets = Client.getDatasets(options.libDS,options.verbose)
    options.libDS = PsubUtils.getRealDatasetName(options.libDS,tmpDatasets)
    # get location for libDS
    if options.verbose:
        tmpLog.debug("get locations for libDS:%s" % options.libDS)
    libDSlocations = Client.getLocations(options.libDS,[],options.cloud,True,options.verbose)
    if libDSlocations == []:
        tmpLog.error("cannot find locations for existing lib dataset:%s" % options.libDS)
        sys.exit(EC_Dataset)
    # check consistency with outDS location    
    if not checkLibDS:
        PsubUtils.checkLocationConsistency(outDSlocations,libDSlocations)
    # keep origianl site
    if Client.PandaSites.has_key(options.site) and \
       (Client.PandaSites[options.site]['ddm'] in libDSlocations or \
        Client.convSrmV2ID(Client.PandaSites[options.site]['ddm']) in libDSlocations):
        pass
    else:
        # convert DQ2ID to Panda siteID
	tmpConvIDs = []
        for libDSlocation in libDSlocations:
            tmpConvIDs += Client.convertDQ2toPandaIDList(libDSlocation)
        # not found
        if tmpConvIDs == []:
            tmpLog.error("cannot find supported sites for existing lib datasete:%s" % options.libDS)
            sys.exit(EC_Dataset)
	# chose one
	if len(tmpConvIDs) == 1:
	    convID = tmpConvIDs[0]
        else:
            # run brokerage to be sent to free site
            convID = PsubUtils.runBrokerageForCompSite(tmpConvIDs,'Atlas-%s' % athenaVer,cacheVer,options.verbose)
        # set    
        options.site  = convID
        options.cloud = Client.PandaSites[convID]['cloud']
        tmpLog.info("set site:%s cloud:%s because libDS:%s exists at %s" % \
                    (options.site,options.cloud,options.libDS,libDSlocations))                
        
# parent datasets
parentDSlocations = None
if options.parentDS != '':
    parentDSlocations = Client.getLocations(options.parentDS,[],options.cloud,False,options.verbose,
                                            getDQ2IDs=True)
    if parentDSlocations == []:
        tmpLog.error("could not find replica locations for %s" % options.parentDS)
        sys.exit(EC_Dataset)


# get DB datasets
dbrFiles  = {}
dbrDsList = []
dbrDsSize = 0
if options.trf or options.dbRelease != '':
    if options.trf:
        # parse jobO for TRF
        tmpItems = jobO.split()
    else:
	tmpItems = []
    if options.dbRelease != '':    
        # mimic a trf parameter to reuse following algorithm
        tmpItems += ['%DB='+options.dbRelease]
    # look for DBRelease
    for tmpItem in tmpItems:
        match = re.search('%DB=([^:]+):(.+)$',tmpItem)
        if match:
            tmpDbrDS  = match.group(1)
            tmpDbrLFN = match.group(2)
            # get files in the dataset
            if not tmpDbrDS in dbrDsList:
                tmpLog.info("query files in %s" % tmpDbrDS)
                tmpList = Client.queryFilesInDataset(tmpDbrDS,options.verbose)
                # append
                for tmpLFN,tmpVal in tmpList.iteritems():
                    dbrFiles[tmpLFN] = tmpVal
                dbrDsList.append(tmpDbrDS)
            # check
            if not dbrFiles.has_key(tmpDbrLFN):
                tmpLog.error("%s is not in %s" % (tmpDbrLFN,tmpDbrDS))
		sys.exit(EC_Dataset)
            # DBR size
            dbrDsSize += long(dbrFiles[tmpDbrLFN]['fsize'])	


# input datasets    
if options.inDS != '' or options.shipinput or options.pfnList != '':
    # minimum bias dataset
    if options.trf and jobO.find('%MININ') != -1:
        runConfig.input.inMinBias = True
    # set dataset name    
    if runConfig.input.inMinBias:
        if options.minDS == "":
            # read from stdin   
            print
            print "This job uses Minimum-Bias stream"
            while True:
                minbiasDataset = raw_input("Enter dataset name for Minimum Bias : ")
                minbiasDataset = minbiasDataset.strip()
                if minbiasDataset != "":
                    break
        else:
            minbiasDataset = options.minDS
    # cavern dataset
    if options.trf and jobO.find('%CAVIN') != -1:
        runConfig.input.inCavern = True
    if runConfig.input.inCavern:
        if options.cavDS == "":
            # read from stdin                  
            print
            print "This job uses Cavern stream"
            while True:
                cavernDataset = raw_input("Enter dataset name for Cavern : ")
                cavernDataset = cavernDataset.strip()
                if cavernDataset != "":
                    break
        else:
            cavernDataset = options.cavDS
    # beam halo dataset
    if options.trf and jobO.find('%BHIN') != -1:
        runConfig.input.inBeamHalo = True 
    if runConfig.input.inBeamHalo:
	# use common DS
	if options.useCommonHalo:
	    if options.beamHaloDS == "":
                # read from stdin                  
                print
                print "This job uses BeamHalo stream"
                while True:
                    beamHaloDataset = raw_input("Enter dataset name for BeamHalo : ")
                    beamHaloDataset = beamHaloDataset.strip()
                    if beamHaloDataset != "":
                        break
            else:
                beamHaloDataset = options.beamHaloDS
	else:	
            # get DS for A-side        
            if options.beamHaloADS == "":
                # read from stdin                  
                print
                print "This job uses BeamHalo stream"
                while True:
                    beamHaloAdataset = raw_input("Enter dataset name for BeamHalo A-side : ")
                    beamHaloAdataset = beamHaloAdataset.strip()
                    if beamHaloAdataset != "":
                        break
            else:
                beamHaloAdataset = options.beamHaloADS
            # get DS for C-side
            if options.beamHaloCDS == "":
                # read from stdin                  
                while True:
                    beamHaloCdataset = raw_input("Enter dataset name for BeamHalo C-side : ")
                    beamHaloCdataset = beamHaloCdataset.strip()
                    if beamHaloCdataset != "":
                        break
            else:
                beamHaloCdataset = options.beamHaloCDS
    # beam gas dataset
    if options.trf and jobO.find('%BGIN') != -1:
        runConfig.input.inBeamGas = True  
    if runConfig.input.inBeamGas: 
	# use common DS
	if options.useCommonGas:
            # get BeamGas DS
            if options.beamGasDS == "":
                # read from stdin                  
                print
                print "This job uses BeamGas stream"
                while True:
                    beamGasDataset = raw_input("Enter dataset name for BeamGas : ")
                    beamGasDataset = beamGasDataset.strip()
                    if beamGasDataset != "":
                        break
            else:
                beamGasDataset = options.beamGasDS
        else:
            # get DS for H
            if options.beamGasHDS == "":
                # read from stdin                  
                print
                print "This job uses BeamGas stream"
                while True:
                    beamGasHdataset = raw_input("Enter dataset name for BeamGas Hydrogen : ")
                    beamGasHdataset = beamGasHdataset.strip()
                    if beamGasHdataset != "":
                        break
            else:
                beamGasHdataset = options.beamGasHDS
            # get DS for C
            if options.beamGasCDS == "":
                # read from stdin                  
                while True:
                    beamGasCdataset = raw_input("Enter dataset name for BeamGas Carbon : ")
                    beamGasCdataset = beamGasCdataset.strip()
                    if beamGasCdataset != "":
                        break
            else:
                beamGasCdataset = options.beamGasCDS
            # get DS for O
            if options.beamGasODS == "":
                # read from stdin                  
                while True:
                    beamGasOdataset = raw_input("Enter dataset name for BeamGas Oxygen : ")
                    beamGasOdataset = beamGasOdataset.strip()
                    if beamGasOdataset != "":
                        break
            else:
                beamGasOdataset = options.beamGasODS
    # input dataset        
    if options.inDS != '':
        # query files in shadow dataset
        shadowList = []
        if shadowDSexist and not outputContExist:
            # query files in PandaDB first to get running/failed files + files which are being added
            tmpShadowList = Client.getFilesInUseForAnal(options.outDS,options.verbose)
            for tmpItem in tmpShadowList:
                shadowList.append(tmpItem)
            # query files in shadow dataset        
            for tmpItem in Client.queryFilesInDataset("%s%s" % (options.outDS,suffixShadow),options.verbose):
                if not tmpItem in shadowList:
		    shadowList.append(tmpItem)
        elif outputContExist:
            shadowList = Client.getFilesInShadowDataset(options.outDS,suffixShadow,options.verbose)
        # query files in dataset
        tmpLog.info("query files in %s" % options.inDS)
        inputFileMap,inputDsString  = Client.queryFilesInDataset(options.inDS,options.verbose,getDsString=True)
        # remove files
        for tmpKey in inputFileMap.keys():
            if tmpKey in options.removeFileList:
                del inputFileMap[tmpKey]
        # remove log, and check matching
        for fileName in inputFileMap.keys():
            # ignore log file
            if re.search('\.log(\.\d+)*(\.tgz)*$',fileName) != None or \
                   re.search('\.log(\.tgz)*(\.\d+)*$',fileName) != None:
                del inputFileMap[fileName]
                continue
            # check type
            if options.inputType != []:
                matchType = False
                for tmpType in options.inputType:
                    if tmpType in fileName:
                        matchType = True
                        break
                if not matchType:
                    del inputFileMap[fileName]                    
                    continue
            # filename matching        
            if options.filelist != []:
                # check matching    
                matchFlag = False
                for pattern in options.filelist:
                    if re.search('\*',pattern) != None:
                        # wildcard matching
			if re.search(pattern,fileName) != None:
			    matchFlag = True
			    break
                    else:
                        # normal matching
                        if pattern == fileName:
                            matchFlag =True
                            break
                # doesn't match
                if not matchFlag:
                    del inputFileMap[fileName]
        # no files in filelist are available
        if options.filelist != [] and inputFileMap == {}:
            if options.inputFileList != '':
                errStr =  "No files in %s are available in %s. " % (options.inputFileList,options.inDS)
            else:
                errStr =  "%s are not available in %s. " % (options.filelist,options.inDS)
            errStr += "Make sure if you specify correct LFNs"
            tmpLog.error(errStr)
            sys.exit(EC_Config)
        # get locations when site==AUTO
        if options.site == "AUTO":
            if inputDsString == '':
                # not using wildcard
                inputDsString = options.inDS
            # take MinBias/Cavern datasets into account
            if runConfig.input.inMinBias:
                inputDsString = inputDsString + ',' + minbiasDataset
            if runConfig.input.inCavern:
                inputDsString = inputDsString + ',' + cavernDataset
            # take beamgas/halo datasets into account
            if runConfig.input.inBeamHalo:
                if options.useCommonHalo:
                    inputDsString = inputDsString + ',' + beamHaloDataset
                else:
                    inputDsString = inputDsString + ',' + beamHaloAdataset
                    inputDsString = inputDsString + ',' + beamHaloCdataset
            if runConfig.input.inBeamGas:
                if options.useCommonGas:
                    inputDsString = inputDsString + ',' + beamGasDataset                    
                else:
                    inputDsString = inputDsString + ',' + beamGasHdataset
                    inputDsString = inputDsString + ',' + beamGasCdataset
                    inputDsString = inputDsString + ',' + beamGasOdataset
            # DBRelease
            dbrDsString = ''
            if options.trf or options.dbRelease != '':
                dbrDsList = []
                if options.trf:
                    # parse jobO for TRF
                    tmpItems = jobO.split()
                else:
                    tmpItems = []
                if options.dbRelease != '':
                    # mimic a trf parameter to reuse following algorithm
                    tmpItems += ['%DB='+options.dbRelease]
                # look for DBRelease
                for tmpItem in tmpItems:
                    match = re.search('%DB=([^:]+):(.+)$',tmpItem)
                    if match:
                        tmpDbrDS  = match.group(1)
                        if not tmpDbrDS in dbrDsList:
                            if dbrDsString == '':
                                dbrDsString = tmpDbrDS
                            else:
                                dbrDsString = dbrDsString + ',' + tmpDbrDS
                            dbrDsList.append(tmpDbrDS)
            # get location        
            dsLocationMap,dsLocationMapBack,dsTapeSites,dsUsedDsMap = Client.getLocations(inputDsString,inputFileMap,options.cloud,
                                                                                          False,options.verbose,
                                                                                          expCloud=expCloudFlag,getReserved=True,
                                                                                          getTapeSites=True,
                                                                                          locCandidates=parentDSlocations,
                                                                                          removeDS=True,
                                                                                          removedDatasets=options.removedDS)
            # get locations for DBR
            if dbrDsString != '':
                dbrDsLocationMap,dbrDsLocationMapBack,dbrDsTapeSites = Client.getLocations(dbrDsString,{},options.cloud,
                                                                                           False,options.verbose,
                                                                                           expCloud=expCloudFlag,getReserved=True,
                                                                                           getTapeSites=True,
                                                                                           locCandidates=parentDSlocations)
                # remove if DBR is unavailable
                tmpSiteKeys = dsLocationMap.keys()
                for tmpSite in tmpSiteKeys:
                    if not tmpSite in dbrDsLocationMap and not tmpSite in dbrDsLocationMapBack:
                        tmpLog.info("%s is unavailabe at %s although input dataset exists there" % (dbrDsString,tmpSite))
                        del dsLocationMap[tmpSite]
                tmpSiteKeys = dbrDsLocationMapBack.keys()
                for tmpSite in tmpSiteKeys:
                    if not tmpSite in dbrDsLocationMap and not tmpSite in dbrDsLocationMapBack:
                        tmpLog.info("%s is unavailabe at %s although input datasets exist there" % (dbrDsString,tmpSite))
                        del dbrDsLocationMapBack[tmpSite]
	    # no location
            if dsLocationMap == {} and dsLocationMapBack == {}:
                if expCloudFlag:
                    errStr = "could not find supported/online sites in the %s cloud for %s" % (options.cloud,options.inDS)
                else:
                    errStr = "could not find supported/online sites for %s" % options.inDS
                if options.parentDS != '':
                    errStr += ' and %s' % options.parentDS
                if dbrDsString != '':
                    errStr += ' and %s' % dbrDsString
                tmpLog.error(errStr)
                if dsTapeSites != []:
                    tmpLog.error("Tape sites %s hold the dataset. Please request subscription to disk area first if needed" % dsTapeSites)
                sys.exit(EC_Dataset)
            # run brorage
            tmpDsLocationMapList = [dsLocationMap]
            if dsLocationMapBack != {}:
                tmpDsLocationMapList.append(dsLocationMapBack)
            tmpBrokerErr = ''   
            for idxDsLocationMap,tmpDsLocationMap in enumerate(tmpDsLocationMapList):    
                tmpSites = []
                for tmpItems in tmpDsLocationMap.values():
                    for tmpItem in tmpItems:
                        # convert to long
                        if options.long:
                            tmpItem = Client.convertToLong(tmpItem)
                        tmpSites.append(tmpItem)
		if tmpSites == []:
                    continue
                status,out = Client.runBrokerage(tmpSites,'Atlas-%s' % athenaVer,verbose=options.verbose,trustIS=True,cacheVer=cacheVer)
                if status != 0:
                    tmpLog.error('failed to run brokerage for automatic assignment: %s' % out)
                    sys.exit(EC_Config)
                if out.startswith('ERROR :'):
		    if idxDsLocationMap == 0:
			tmpBrokerErr += out
			tmpBrokerErr += " Could not find sites in the %s cloud.\n" % options.cloud
		    else:
			if tmpBrokerErr != '':
			    tmpBrokerErr += out
			    tmpBrokerErr += " Could not find sites in other clouds.\n"
			else:
			    tmpBrokerErr += out
			    tmpBrokerErr += " Could not find sites.\n"
                    if idxDsLocationMap+1 >= len(tmpDsLocationMapList):
                        tmpLog.error('brokerage failed')
                        print tmpBrokerErr[:-1]
                        sys.exit(EC_Config)
                    continue    
                if not Client.PandaSites.has_key(out):
                    tmpLog.error('brokerage gave wrong PandaSiteID:%s' % out)
                    sys.exit(EC_Config)
                break    
	    # set site/cloud
            options.site  = out
            options.cloud = Client.PandaSites[options.site]['cloud']
            if options.verbose:
                tmpLog.debug("chosen site=%s" % options.site)
            # update used the list of used datasets
            for tmpDsUsedDsMapKey,tmpDsUsedDsVal in dsUsedDsMap.iteritems():
                if options.site in [tmpDsUsedDsMapKey,Client.convertToLong(tmpDsUsedDsMapKey)]:
                    options.removedDS += tmpDsUsedDsVal
                    break
        # reset max input size
        if not Client.PandaSites[options.site]['maxinputsize'] in [0,'']:
            maxTotalSize = Client.PandaSites[options.site]['maxinputsize'] * 1024*1024
        # scan local replica catalog
        if options.skipScan:
            # skip remote scan
            missList = []
        else:
            dsLocation = Client.PandaSites[options.site]['ddm']
            if Client.getLFC(dsLocation) != None:
		tmpLog.info("scanning LFC %s for %s" % (Client.getLFC(dsLocation),options.site))
                # LFC
                if options.nfiles == 0 and options.nSkipFiles != 0:
                    missList = Client.getMissLFNsFromLFC(inputFileMap,options.site,True,options.verbose)
                else:
                    missList = Client.getMissLFNsFromLFC(inputFileMap,options.site,True,options.verbose,
                                                         options.nfiles+options.nSkipFiles,shadowList)
            elif Client.getLRC(dsLocation) != None:
		tmpLog.info("scanning LRC %s for %s" % (Client.getLRC(dsLocation),options.site))
                # LRC
                missList = Client.getMissLFNsFromLRC(inputFileMap,Client.getLRC(dsLocation),options.verbose,
                                                     options.nfiles+options.nSkipFiles)
            else:
                missList = []
            # choose min missList
            if options.verbose:
                tmpLog.debug("%s holds %s files" % (dsLocation,len(inputFileMap)-len(missList)))
        # No files available
        if len(inputFileMap) == len(missList) and shadowList == []:
            tmpLog.error("No files available on disk at %s" % options.site)
            sys.exit(EC_Dataset)
        # remove missing
        for fileName in inputFileMap.keys():
            # missing at the site
            if fileName in missList:
                del inputFileMap[fileName]                    
                continue
        # remove shadow
        for fileName in shadowList:
            if inputFileMap.has_key(fileName):
                del inputFileMap[fileName]
        # no input
        if len(inputFileMap) == 0:
            tmpStr  = "all input files at %s had already been used or remaining files are unavailable on disk. " % options.site
	    tmpStr += "pathena runs on files which were failed or were not used in "
	    tmpStr += "previous submissions if it runs with the same inDS and outDS"
            if options.crossSite == 0:
		tmpLog.error(tmpStr)
		sys.exit(EC_Dataset)
            else:
		tmpLog.info(tmpStr)
                # go back to current dir
                os.chdir(currentDir)
                # try another site if input files remain
                options.crossSite -= 1
                if options.crossSite > 0 and options.inDS != '' and not siteSpecified:
                    if missList != []:
                        PsubUtils.runPathenaRec(runConfig,missList,tmpDir,fullExecString,options.nfiles,inputFileMap,
                                                options.site,options.crossSite,archiveName,options.removedDS,
                                                options.inDS,options.goodRunListXML,options.eventPickEvtList,
                                                options.verbose)
                # exit        
                sys.exit(0)
        # make list
        inputFileList = inputFileMap.keys()
        inputFileList.sort()
    elif options.pfnList != '':
        # read PFNs from a file
        devidedByGUID = False 
        rFile = open(options.pfnList)
        inputFileList = []
        for line in rFile:
            line = re.sub('\n','',line)
            line.strip()
            if line != '':
                inputFileList.append(line)
        rFile.close()
        inputFileList.sort()
    else:
        # ship input files
        devidedByGUID = False 
        # extract GUIDs
        guidCollMap,guidCollList = AthenaUtils.getGUIDfromColl(athenaVer,runConfig.input.shipFiles,
                                                               currentDir,
                                                               runConfig.input.collRefName,
                                                               options.verbose)
        # if works
        if guidCollList != []:
            # use GUIDs for looping
            inputFileList = guidCollList
            # use GUID boundaries
            devidedByGUID = True
        else:
            # use input collections for looping
            inputFileList = runConfig.input.shipFiles
    # skip files
    if options.nSkipFiles > len(inputFileList):
        tmpStr  = "the number of files in %s is less than nSkipFiles\n" % options.inDS
        tmpStr += " N of files=%s : nSkipFiles=%s" % (len(inputFileList),options.nSkipFiles)
        tmpLog.error(tmpStr)
        sys.exit(EC_Dataset)
    inputFileList = inputFileList[options.nSkipFiles:]
    # use limited number of files
    if options.nfiles > 0:
        inputFileList = inputFileList[:options.nfiles]

    # set # of events for shipInput
    if options.shipinput and options.nFilesPerJob == -1 and options.nEventsPerJob == -1:
	# non GUID boundaries
	if not devidedByGUID:
	    options.nEventsPerJob = 2000

    # set # of split
    #@some interesting default behaviour here
    #@ If number of jobs is "undefined" or "not specified by user"
    #@ split dataset in number of chunsk with nFilesPerJob files per job
    #@ if nFilesPerJob are not specified by user then split AOD files list in chunks of 10 files
    #@ split other file types in chunks of 20 files.
    #
    if options.nEventsPerJob != -1:
        if options.nEventsPerFile == 0:
            options.nEventsPerFile = Client.nEvents(options.inDS,options.verbose,(not options.shipinput),inputFileList,currentDir)
	if options.nEventsPerJob > options.nEventsPerFile:
	    tmpDiv,tmpMod = divmod(options.nEventsPerJob,options.nEventsPerFile)
	    options.nFilesPerJob = tmpDiv
	    if tmpMod != 0:
		options.nFilesPerJob += 1
	    options.nEventsPerJob = -1

	    
    if options.split == -1:
        # count total size for inputs
        totalSize = 0 
        for fileName in inputFileList:
            try:
                vals = inputFileMap[fileName]
                totalSize += long(vals['fsize'])
            except:
                pass
        #@ If number of jobs is not defined then....
        #@ For splitting by files case
        if options.nEventsPerJob == -1 and options.nGBPerJob == -1:
            if options.nFilesPerJob > 0:
                defaultNFile = options.nFilesPerJob
            else:
                defaultNFile = 20
            tmpNSplit,tmpMod = divmod(len(inputFileList),defaultNFile)
            if tmpMod != 0:
                tmpNSplit += 1
            # check size limit
            if totalSize/tmpNSplit > maxTotalSize-dbrDsSize-safetySize:
                # reset to meet the size limit
                tmpNSplit,tmpMod = divmod(totalSize,maxTotalSize-dbrDsSize-safetySize)
                if tmpMod != 0:
                    tmpNSplit += 1
                # calculate N files
                divF,modF = divmod(len(inputFileList),tmpNSplit)
		if divF == 0:
		    divF = 1
		# reset tmpNSplit
                tmpNSplit,tmpMod = divmod(len(inputFileList),divF)
                if tmpMod != 0:
                    tmpNSplit += 1
                # check again just in case
                iDiv = 0
                subTotal = 0
                for fileName in inputFileList:
                    vals = inputFileMap[fileName]
                    try:
                        subTotal += long(vals['fsize'])
                    except:
                        pass
                    iDiv += 1
                    if iDiv >= divF:
                        # check
                        if subTotal > maxTotalSize-dbrDsSize-safetySize:
                            # recalcurate
                            if divF != 1:
                                divF -= 1
                            tmpNSplit,tmpMod = divmod(len(inputFileList),divF)
                            if tmpMod != 0:
                                tmpNSplit += 1
                            break
			# reset
                        iDiv = 0
                        subTotal = 0
            # set            
            options.split = tmpNSplit
        #@ For splitting by events case
        elif options.nGBPerJob == -1:
            #@ split by number of events defined
            defaultNFile=1 #Each job has one input file in this case
            #@ tmpNSplit - number of jobs per file in case of splitting by event number
            tmpNSplit, tmpMod = divmod(options.nEventsPerFile, options.nEventsPerJob)
            if tmpMod != 0:
                tmpNSplit +=1
            #@ Number of Jobs calculated here:
            options.split = tmpNSplit*len(inputFileList)
        else:
            # calcurate number of jobs for nGBPerJob
            options.split = 0
            tmpSubTotal = 0
            for fileName in inputFileList:
                vals = inputFileMap[fileName]
                tmpSize = long(vals['fsize'])
                singleLargeFile = False
                if tmpSubTotal+tmpSize > options.nGBPerJob-dbrDsSize-safetySize:
                    options.split += 1
                    # single large file uses one job
                    if tmpSubTotal == 0:
                        singleLargeFile = True
                    tmpSubTotal = 0
                if not singleLargeFile:
                    tmpSubTotal += tmpSize
            # remaining
            if tmpSubTotal != 0:
                options.split += 1    
                    

    # input stream for minimum bias
    if runConfig.input.inMinBias:
        # query files in dataset
        tmpLog.info("query files in %s" % minbiasDataset)
        tmpList = Client.queryFilesInDataset(minbiasDataset,options.verbose)
	minbiasList = []
        for item in tmpList.keys():
            # remove log
            if re.search('log(\.tgz)*(\.\d+)*$',item) != None or \
                   re.search('\.log(\.\d+)*(\.tgz)*$',item) != None:
                continue
            minbiasList.append((item,tmpList[item]))
        # sort
        minbiasList.sort()
        # number of files per one signal
        if options.nMin < 0:
            while True:
                tmpStr = raw_input("Enter the number of Minimum-Bias files per one signal file : ")
                try:
                    options.nMin = int(tmpStr)
                    break
                except:
                    pass
        # check # of files
        if len(minbiasList) < options.nMin:
            tmpLog.error("%s contains only %s files which is less than %s" % \
                         (minbiasDataset,len(minbiasList),options.nMin))
            sys.exit(EC_Dataset)
    # input stream for cavern
    if runConfig.input.inCavern:
        # query files in dataset
        tmpLog.info("query files in %s" % cavernDataset)
        tmpList = Client.queryFilesInDataset(cavernDataset,options.verbose)
	cavernList = []
        for item in tmpList.keys():
            # remove log
            if re.search('\.log(\.\d+)*(\.tgz)*$',item) != None or \
                   re.search('\.log(\.tgz)*(\.\d+)*$',item) != None:
                continue
            cavernList.append((item,tmpList[item]))
        # sort
        cavernList.sort()
        # number of files per one signal
        if options.nCav < 0:
            while True:
                tmpStr = raw_input("Enter the number of Cavern files per one signal file : ")
                try:
                    options.nCav = int(tmpStr)
                    break
                except:
                    pass
        # check # of files
        if len(cavernList) < options.nCav:
            tmpLog.error("%s contains only %s files which is less than %s" % \
                         (cavernDataset,len(cavernList),options.nCav))
            sys.exit(EC_Dataset)
    # input stream for beam halo
    if runConfig.input.inBeamHalo:
	# use common DS
	if options.useCommonHalo:
            # query files in dataset
            tmpLog.info("query files in %s" % beamHaloDataset)
            tmpList = Client.queryFilesInDataset(beamHaloDataset,options.verbose)
	    beamHaloList = []
            for item in tmpList.keys():
                # remove log
                if re.search('\.log(\.\d+)*(\.tgz)*$',item) != None or \
                       re.search('\.log(\.tgz)*(\.\d+)*$',item) != None:
                    continue
                beamHaloList.append((item,tmpList[item]))
            # sort
            beamHaloList.sort()
            # number of files per one sub job
            if options.nBeamHalo < 0:
                while True:
                    tmpStr = raw_input("Enter the number of BeamHalo files per one sub job : ")
                    try:
                        options.nBeamHalo = int(tmpStr)
                        break
                    except:
                        pass
            # check # of files
            if len(beamHaloList) < options.nBeamHalo:
                tmpLog.error("%s contains only %s files which is less than %s" % \
                             (beamHaloDataset,len(beamHaloList),options.nBeamHalo))
                sys.exit(EC_Dataset)
	else:	
            # query files in dataset
            tmpLog.info("query files in %s" % beamHaloAdataset)
            tmpList = Client.queryFilesInDataset(beamHaloAdataset,options.verbose)
	    beamHaloAList = []
            for item in tmpList.keys():
                # remove log
                if re.search('\.log(\.\d+)*(\.tgz)*$',item) != None or \
                       re.search('\.log(\.tgz)*(\.\d+)*$',item) != None:
                    continue
                beamHaloAList.append((item,tmpList[item]))
            tmpLog.info("query files in %s" % beamHaloCdataset)
            tmpList = Client.queryFilesInDataset(beamHaloCdataset,options.verbose)
	    beamHaloCList = []
            for item in tmpList.keys():
                # remove log
                if re.search('\.log(\.\d+)*(\.tgz)*$',item) != None or \
                       re.search('\.log(\.tgz)*(\.\d+)*$',item) != None:
                    continue
                beamHaloCList.append((item,tmpList[item]))
            # sort
            beamHaloAList.sort()
            beamHaloCList.sort()
            # number of files per one sub job
            if options.nBeamHaloA < 0:
                while True:
                    tmpStr = raw_input("Enter the number of BeamHalo files for A-side per one sub job : ")
                    try:
                        options.nBeamHaloA = int(tmpStr)
                        break
                    except:
                        pass
            if options.nBeamHaloC < 0:
                # use default ratio
                options.nBeamHaloC = int(0.02/1.02*options.nBeamHaloA)
            # check # of files
            if len(beamHaloAList) < options.nBeamHaloA:
                tmpLog.error("%s contains only %s files which is less than %s" % \
                             (beamHaloAdataset,len(beamHaloAList),options.nBeamHaloA))
                sys.exit(EC_Dataset)
            if len(beamHaloCList) < options.nBeamHaloC:
                tmpLog.error("%s contains only %s files which is less than %s" % \
                             (beamHaloCdataset,len(beamHaloCList),options.nBeamHaloC))
                sys.exit(EC_Dataset)
    # beam gas dataset
    if runConfig.input.inBeamGas: 
	# use common DS
	if options.useCommonGas:
            # query files in dataset
            tmpLog.info("query files in %s" % beamGasDataset)
            tmpList = Client.queryFilesInDataset(beamGasDataset,options.verbose)
	    beamGasList = []
            for item in tmpList.keys():
                # remove log
                if re.search('\.log(\.\d+)*(\.tgz)*$',item) != None or \
                       re.search('\.log(\.tgz)*(\.\d+)*$',item) != None:
                    continue
                beamGasList.append((item,tmpList[item]))
            # sort
            beamGasList.sort()
            # number of files per one sub job
            if options.nBeamGas < 0:
                while True:
                    tmpStr = raw_input("Enter the number of BeamGas files per one sub job : ")
                    try:
                        options.nBeamGas = int(tmpStr)
                        break
                    except:
                        pass
            # check # of files
            if len(beamGasList) < options.nBeamGas:
                tmpLog.error("%s contains only %s files which is less than %s" % \
                             (beamGasDataset,len(beamGasList),options.nBeamGas))
                sys.exit(EC_Dataset)
        else:
            # query files in dataset
            tmpLog.info("query files in %s" % beamGasHdataset)
            tmpList = Client.queryFilesInDataset(beamGasHdataset,options.verbose)
	    beamGasHList = []
            for item in tmpList.keys():
                # remove log
                if re.search('\.log(\.\d+)*(\.tgz)*$',item) != None or \
                       re.search('\.log(\.tgz)*(\.\d+)*$',item) != None:
                    continue
                beamGasHList.append((item,tmpList[item]))
            tmpLog.info("query files in %s" % beamGasCdataset)
            tmpList = Client.queryFilesInDataset(beamGasCdataset,options.verbose)
	    beamGasCList = []
            for item in tmpList.keys():
                # remove log
                if re.search('\.log(\.\d+)*(\.tgz)*$',item) != None or \
                       re.search('\.log(\.tgz)*(\.\d+)*$',item) != None:
                    continue
                beamGasCList.append((item,tmpList[item]))
            tmpLog.info("query files in %s" % beamGasOdataset)
            tmpList = Client.queryFilesInDataset(beamGasOdataset,options.verbose)
	    beamGasOList = []
            for item in tmpList.keys():
                # remove log
                if re.search('\.log(\.\d+)*(\.tgz)*$',item) != None or \
                       re.search('\.log(\.tgz)*(\.\d+)*$',item) != None:
                    continue
                beamGasOList.append((item,tmpList[item]))
            # sort
            beamGasHList.sort()
            beamGasCList.sort()
            beamGasOList.sort()        
            # number of files per one sub job
            if options.nBeamGasH < 0:
                while True:
                    tmpStr = raw_input("Enter the number of BeamGas files for Hydrogen per one sub job : ")
                    try:
                        options.nBeamGasH = int(tmpStr)
                        break
                    except:
                        pass
            if options.nBeamGasC < 0:
                # use default ratio
                options.nBeamGasC = int(options.nBeamGasH*7/90)
            if options.nBeamGasO < 0:
                # use default ratio
                options.nBeamGasO = int(options.nBeamGasH*3/90)
            # check # of files
            if len(beamGasHList) < options.nBeamGasH:
                tmpLog.error("%s contains only %s files which is less than %s" % \
                             (beamGasHdataset,len(beamGasHList),options.nBeamGasH))
                sys.exit(EC_Dataset)
            if len(beamGasCList) < options.nBeamGasC:
                tmpLog.error("%s contains only %s files which is less than %s" % \
                             (beamGasCdataset,len(beamGasCList),options.nBeamGasC))
                sys.exit(EC_Dataset)
            if len(beamGasOList) < options.nBeamGasO:
                tmpLog.error("%s contains only %s files which is less than %s" % \
                             (beamGasOdataset,len(beamGasOList),options.nBeamGasO))
                sys.exit(EC_Dataset)
else:
    if options.split <= 0:
        options.split = 1


# choose site automatically when it is still AUTO
if options.site == "AUTO":
    # convert candidates for SRM v2
    if options.parentDS != '':
        parentDSlocationsSrmV2 = []
        for locTmp in parentDSlocations:
            parentDSlocationsSrmV2.append(Client.convSrmV2ID(locTmp))
    # get DBR locations
    if dbrDsList != []:
	dbrDsString = ''
        for tmpDS in dbrDsList:
            dbrDsString += (tmpDS + ',')
        dbrDsString = dbrDsString[:-1]    
        dbrDsLocationMap,dbrDsLocationMapBack,dbrDsTapeSites = Client.getLocations(dbrDsString,{},options.cloud,
                                                                                   False,options.verbose,
                                                                                   expCloud=expCloudFlag,getReserved=True,
                                                                                   getTapeSites=True,
                                                                                   locCandidates=parentDSlocations)
    # get sites belonging to a cloud and others
    tmpPriSites = []
    tmpSecSites = []
    for tmpID,spec in Client.PandaSites.iteritems():
        if spec['status']=='online':
            # exclude long,xrootd,local queues
            if Client.isExcudedSite(tmpID):
                continue
            # check DDM for parent datasets
            if options.parentDS != '':            
                if not Client.convSrmV2ID(spec['ddm']) in parentDSlocationsSrmV2:
                    continue
            # check DBR locations
            if dbrDsList != []:
                if not Client.convSrmV2ID(spec['ddm']) in dbrDsLocationMap.keys()+dbrDsLocationMapBack.keys():
                    continue
	    # convert to long
            if options.long:
                tmpID = Client.convertToLong(tmpID)
            # check cloud if it is specified   
            if spec['cloud']==options.cloud or (not expCloudFlag):
                tmpPriSites.append(tmpID)
            elif not expCloudFlag:
                tmpSecSites.append(tmpID)
    tmpSitesList = [tmpPriSites]
    if not expCloudFlag:
        tmpSitesList.append(tmpSecSites)
    # run brokerage
    tmpBrokerErr = ''
    for idxTmpSites,tmpSites in enumerate(tmpSitesList):
        if tmpSites != []:
            status,out = Client.runBrokerage(tmpSites,'Atlas-%s' % athenaVer,verbose=options.verbose,trustIS=True,cacheVer=cacheVer)
        else:
            status,out = 0,'ERROR : site list is empty.' 
        if status != 0:
            tmpLog.error('failed to run brokerage for automatic assignment: %s' % out)  
            sys.exit(EC_Config)
        if out.startswith('ERROR :'):
            tmpBrokerErr += out
            if idxTmpSites == 0:
                tmpBrokerErr += " Could not find sites in the %s cloud.\n" % options.cloud
            else:
                tmpBrokerErr += " Could not find sites in other clouds.\n"
            if idxTmpSites+1 >= len(tmpSitesList):
                tmpLog.error('brokerage failed')
                print tmpBrokerErr[:-1]
                sys.exit(EC_Config)
            continue    
        if not Client.PandaSites.has_key(out):
            tmpLog.error('brokerage gave wrong PandaSiteID:%s' % out)
            sys.exit(EC_Config)
        break    
    # set site
    options.site = out

# long queue
if options.long and not options.site.startswith('ANALY_LONG_'):
    options.site = Client.convertToLong(options.site)
        
# modify outDS name when container is used for output
if original_outDS_Name.endswith('/'):
    options.outDS = re.sub('/$','.%s' % options.site,options.outDS)
    # check outDS
    tmpDatasets = Client.getDatasets(options.outDS,options.verbose)
    if len(tmpDatasets) != 0:
        outputDSexist = True

# index
indexFiles   = 0
indexCavern  = 0
indexMin     = 0
indexBHalo   = 0
indexBHaloA  = 0
indexBHaloC  = 0
indexBGas    = 0
indexBGasH   = 0
indexBGasC   = 0
indexBGasO   = 0
        
# set index of outputs
if outputDSexist:
    AthenaUtils.setInitOutputIndex(runConfig,options.outDS,options.individualOutDS,options.extOutFile,outputIndvDSlist,options.verbose)

# check permission
if not Client.checkSiteAccessPermission(options.site,options.workingGroup,options.verbose):
    sys.exit(EC_Config)

# reset destSE if it is redundant
if options.burstSubmit == '':
    tmpOutDsLocation = Client.PandaSites[options.site]['ddm']
    if options.spaceToken != '':
	if Client.PandaSites[options.site]['setokens'].has_key(options.spaceToken):
	    tmpOutDsLocation = Client.PandaSites[options.site]['setokens'][options.spaceToken]
    if options.destSE == tmpOutDsLocation:
	options.destSE = ''

if options.verbose:
    print "== parameters =="
    print "Site       : %s" % options.site
    print "Athena     : %s" % athenaVer
    if groupArea != '':
        print "Group Area : %s" % groupArea
    if cacheVer != '':
        print "ProdCache  : %s" % cacheVer[1:]
    if nightVer != '':
        print "Nightly    : %s" % nightVer[1:]        
    print "RunDir     : %s" % runDir
    print "jobO       : %s" % jobO.lstrip()


####################################################################3
# submit jobs

# read jobID
jobDefinitionID = 1
jobid_file = '%s/pjobid.dat' % os.environ['PANDA_CONFIG_ROOT']
if os.path.exists(jobid_file):
    try:
        # read line
        tmpJobIdFile = open(jobid_file)
        tmpID = tmpJobIdFile.readline()
        tmpJobIdFile.close()
        # remove \n
        tmpID = tmpID.replace('\n','')
        # convert to int
        jobDefinitionID = long(tmpID) + 1
    except:
        pass

# look for pandatools package
for path in sys.path:
    if path == '':
        path = curDir
    if os.path.exists(path) and 'pandatools' in os.listdir(path):
        # make symlink for module name.
        os.symlink('%s/pandatools' % path,'taskbuffer')
        break

# append tmpdir to import taskbuffer module
sys.path = [tmpDir]+sys.path
from taskbuffer.JobSpec  import JobSpec
from taskbuffer.FileSpec import FileSpec

jobList = []

# job name
jobName = commands.getoutput('uuidgen')

# build job
if options.nobuild:
    pass
elif options.libDS == '': 
    jobB = JobSpec()
    jobB.jobDefinitionID   = jobDefinitionID
    jobB.jobName           = jobName
    jobB.lockedby          = 'panda-client-%s' % PandaToolsPkgInfo.release_version 
    jobB.AtlasRelease      = 'Atlas-%s' % athenaVer
    jobB.homepackage       = 'AnalysisTransforms'+cacheVer+nightVer
    jobB.transformation    = '%s/buildJob-00-00-03' % Client.baseURLSUB
    jobB.cmtConfig         = AthenaUtils.getCmtConfig(athenaVer,cacheVer,nightVer)
    hostName = commands.getoutput('hostname').split('.')[0]
    if nickName == '':
        # old convention
        tmpDsPrefix = 'user%s.%s' % (time.strftime('%y',time.gmtime()),distinguishedName)
    else:
        tmpDsPrefix = 'user.%s' % nickName
    libDsName = '%s.%s.%s.lib._%06d' % (tmpDsPrefix,
                                        time.strftime('%m%d%H%M%S',time.gmtime()),
                                        datetime.datetime.utcnow().microsecond,
                                        jobDefinitionID)
    if Client.isDQ2free(options.site):
        # user specified output path
        jobB.destinationDBlock = re.sub('/+$','',options.outputPath)+'/%s' % libDsName
    else:
        jobB.destinationDBlock = libDsName
    if Client.isDQ2free(options.site):
        # flag to use DQ2-free output
	jobB.destinationSE = 'local'
    else:
	jobB.destinationSE = options.site
    if options.prodSourceLabel != '':
        jobB.prodSourceLabel = options.prodSourceLabel
    else:
        jobB.prodSourceLabel = 'panda'        
    if options.processingType != '':    
        jobB.processingType = options.processingType
    if options.seriesLabel != '':    
        jobB.prodSeriesLabel = options.seriesLabel
    jobB.workingGroup      = options.workingGroup    
    jobB.assignedPriority  = 2000
    jobB.computingSite     = options.site
    if options.burstSubmit == '':
        jobB.cloud = Client.PandaSites[options.site]['cloud']
    if options.panda_origFullExecString == '':    
        jobB.metadata = fullExecString
    else:
        jobB.metadata = urllib.unquote(options.panda_origFullExecString)
    fileBO = FileSpec()
    fileBO.lfn = '%s.lib.tgz' % libDsName
    fileBO.type = 'output'
    fileBO.dataset = jobB.destinationDBlock
    fileBO.destinationDBlock = jobB.destinationDBlock
    fileBO.destinationSE = jobB.destinationSE
    jobB.addFile(fileBO)
    fileBI = FileSpec()
    fileBI.lfn = archiveName
    fileBI.type = 'input'
    jobB.jobParameters     = '-i %s -o %s' % (fileBI.lfn,fileBO.lfn)
    # source URL
    matchURL = re.search("(http.*://[^/]+)/",Client.baseURLSSL)
    if matchURL != None:
        jobB.jobParameters += " --sourceURL %s " % matchURL.group(1)
    # log    
    file = FileSpec()
    file.lfn  = '%s.log.tgz' % libDsName
    file.type = 'log'
    file.dataset = jobB.destinationDBlock
    file.destinationDBlock = jobB.destinationDBlock
    file.destinationSE = jobB.destinationSE
    jobB.addFile(file)
    # set space token
    for file in jobB.Files:
        if file.type in ['output','log']:
            if options.spaceToken != '':
                file.destinationDBlockToken = options.spaceToken
            else:
                if options.burstSubmit == '':
                    defaulttoken = Client.PandaSites[options.site]['defaulttoken']
                    file.destinationDBlockToken = Client.getDefaultSpaceToken(vomsFQAN,defaulttoken)
    # append
    jobList.append(jobB)
else:
    # query files in lib dataset to reuse libraries
    if not Client.isDQ2free(options.site):
        tmpLog.info("query files in %s" % options.libDS)
        tmpList = Client.queryFilesInDataset(options.libDS,options.verbose)
    else:
        # don't check for DQ2 free sites
        tmpList = {}
    tmpFileList = []
    tmpGUIDmap = {}
    for fileName in tmpList.keys():
        # ignore log file
        if re.search('\.log(\.\d+)*(\.tgz)*$',fileName) != None or \
               re.search('\.log(\.tgz)*(\.\d+)*$',fileName) != None:
            continue
        tmpFileList.append(fileName)
        tmpGUIDmap[fileName] = tmpList[fileName]['guid'] 
    # incomplete libDS
    if tmpFileList == []:
        # query files in dataset from Panda
	status,tmpMap = Client.queryLastFilesInDataset([options.libDS],options.verbose)
        # look for lib.tgz
	for fileName in tmpMap[options.libDS]:
            # ignore log file
            if re.search('\.log(\.\d+)*(\.tgz)*$',fileName) != None or \
                   re.search('\.log(\.tgz)*(\.\d+)*$',fileName) != None:
                continue
            tmpFileList.append(fileName)
            tmpGUIDmap[fileName] = None
    # incomplete libDS
    if tmpFileList == []:
        if not Client.isDQ2free(options.site):
            tmpLog.error("lib dataset %s is empty" % options.libDS)
            sys.exit(EC_Dataset)
        else:
            # try lib.tgz for DQ2 free sites as this might be in ArchivedDB 
            fileName = "%s.lib.tgz" % options.libDS
            tmpFileList.append(fileName)
            tmpGUIDmap[fileName] = commands.getoutput('uuidgen')
    # check file list                
    if len(tmpFileList) != 1:
        tmpLog.error("dataset %s contains multiple lib.tgz files : %s" % (options.libDS,tmpFileList))
        sys.exit(EC_Dataset)
    # instantiate FileSpec
    fileBO = FileSpec()
    fileBO.lfn = tmpFileList[0]
    fileBO.GUID = tmpGUIDmap[fileBO.lfn]
    fileBO.dataset = options.libDS
    fileBO.destinationDBlock = options.libDS
    if fileBO.GUID != 'NULL':
        fileBO.status = 'ready'
    
# run athena            

if options.inDS != '' and options.burstSubmit == '':
    if len(missList) == 0:
        tmpLog.info("all files are available at %s" % options.site)
    else:
        tmpLog.info("%s files are missing or unchecked at %s" % (len(missList),options.site))
    tmpLog.info("use %s files" % len(inputFileList)) 

# disp datasetname to identify inputs
commonDispName = 'user_disp.%s' % commands.getoutput('uuidgen')

# check access method
isDirectAccess = PsubUtils.isDirectAccess(options.site,runConfig.input.inBS,options.trf,options.ara)

# split job
#@ Loop for jobs here
for iSubJob in range(options.split):
    # terminate condition: no remaining files
    if (options.inDS != '' or options.shipinput or options.pfnList != '') and indexFiles >= len(inputFileList):
        break
    # instantiate sub-job
    jobR = JobSpec()
    jobR.jobDefinitionID   = jobDefinitionID
    jobR.jobName           = jobName
    jobR.lockedby          = 'panda-client-%s' % PandaToolsPkgInfo.release_version     
    jobR.AtlasRelease      = 'Atlas-%s' % athenaVer
    jobR.homepackage       = 'AnalysisTransforms'+cacheVer+nightVer
    jobR.transformation    = '%s/runAthena-00-00-11' % Client.baseURLSUB
    jobR.cmtConfig         = AthenaUtils.getCmtConfig(athenaVer,cacheVer,nightVer)
    if options.inDS == '':
        jobR.prodDBlock     = 'NULL'
    else:
        if re.search(',',options.inDS) != None:
            jobR.prodDBlock = options.inDS.split(',')[0]
        else:
            jobR.prodDBlock = options.inDS
    jobR.destinationDBlock = options.outDS
    if Client.isDQ2free(options.site):
        # flag to use DQ2-free output 
        jobR.destinationSE = 'local'
    elif options.destSE != '':
        # write outputs to destSE
        jobR.destinationSE = options.destSE
    else:
        jobR.destinationSE = options.site
    if options.prodSourceLabel != '':
        jobR.prodSourceLabel = options.prodSourceLabel
    else:
        jobR.prodSourceLabel   = 'user'        
    if options.processingType != '':
        jobR.processingType = options.processingType
    if options.seriesLabel != '':    
        jobR.prodSeriesLabel = options.seriesLabel
    jobR.workingGroup      = options.workingGroup            
    jobR.assignedPriority  = 1000
    if options.panda_origFullExecString == '':
        jobR.metadata = fullExecString
    else:
        jobR.metadata = urllib.unquote(options.panda_origFullExecString)
    # memory
    if options.memory != -1:
        jobR.minRamCount = options.memory
    # CPU count
    if options.maxCpuCount != -1:
        jobR.maxCpuCount = options.maxCpuCount
    jobR.computingSite = options.site
    if options.burstSubmit == '':
        jobR.cloud = Client.PandaSites[options.site]['cloud']
    # source files
    if not options.nobuild:
        fileS = FileSpec()
        fileS.lfn     = fileBO.lfn
        fileS.GUID    = fileBO.GUID
        fileS.type    = 'input'
        fileS.status  = fileBO.status
        fileS.dataset = fileBO.destinationDBlock
        fileS.dispatchDBlock = fileBO.destinationDBlock
        jobR.addFile(fileS)
    # input files
    inList       = []
    minList      = []
    cavList      = []
    bhaloList    = []
    bgasList     = []
    guidBoundary = []
    if options.inDS != '' or options.shipinput or options.pfnList != '':
        # calculate N files
        if options.nGBPerJob == -1:
            (divF,modF) = divmod(len(inputFileList),options.split)
            if modF != 0:
                divF += 1
            # if split by files was specified
            if options.nFilesPerJob > 0:
                divF = options.nFilesPerJob 
            # if split by events was specified then
            if options.nEventsPerJob > 0:
               #@Calculate how many events to skip
               nEventsToSkip = nSkips*options.nEventsPerJob
               # @Increment number of skipped blocks
               nSkips = nSkips + 1
               #Take just one file
               divF = 1
               # @ If splitting of file per event is complete then take the next file
               if nEventsToSkip >= options.nEventsPerFile :
                   nEventsToSkip = 0
                   nSkips        = 1
                   indexFiles   += divF
        else:
            # calculate number of files for nGBPerJob
            divF = 0
            tmpSubTotal = 0
            for fileName in inputFileList[indexFiles:]:
                vals = inputFileMap[fileName]
                tmpSize = long(vals['fsize'])
                if tmpSubTotal+tmpSize > options.nGBPerJob-dbrDsSize-safetySize:
                    break
                divF += 1
                tmpSubTotal += tmpSize
            # avoid zero-divide    
            if divF == 0:
                divF = 1
        # File Selector    
        tmpList = inputFileList[indexFiles:indexFiles+divF]
        if options.inDS != '':
            totalSize = 0
	    for fileName in tmpList:
                vals = inputFileMap[fileName]
                # instantiate  FileSpec
                file = FileSpec()
                file.lfn            = fileName
		file.GUID           = vals['guid']
		file.fsize          = vals['fsize']
		file.md5sum         = vals['md5sum']
                if vals.has_key('dataset'):
                    file.dataset    = vals['dataset']
                else:
                    file.dataset    = options.inDS                    
                file.prodDBlock     = file.dataset
                file.dispatchDBlock = commonDispName
                file.type           = 'input'
                file.status         = 'ready'
                # use local access for TRF and BS
                if (options.trf or runConfig.input.inBS) and not isDirectAccess:
                    file.prodDBlockToken = 'local'
                jobR.addFile(file)
                inList.append(fileName)
                try:
                    totalSize += long(file.fsize)
                except:
                    pass
            # size check    
            if (not isDirectAccess) and totalSize > maxTotalSize-dbrDsSize and len(tmpList) > 1:
                tmpStr  = "A subjob has %s input files and requires %sMB of disk space " \
                          % (len(tmpList), ((totalSize+dbrDsSize) >> 20))
                if dbrDsSize != 0:
                    tmpStr += "(DBRelease=%sMB)" % (dbrDsSize>>20)
                tmpStr += ". It must be less than %sMB to avoid overflowing the remote disk. " \
                          % (maxTotalSize >> 20)
                tmpStr += "Please split the job using --nFilesPerJob. If file sizes vary in large range "
                tmpStr += "--nGBPerJob may help. e.g., --nGBPerJob=MAX"                
                tmpLog.error(tmpStr)
                sys.exit(EC_Split)
        else:        
	    for fileName in tmpList:
                # use GUID boundaries or not
                if devidedByGUID:
                    # collect GUIDs
                    guidBoundary.append(fileName)
                    # fileName is a GUID in this case 
                    realFileName = guidCollMap[fileName]
                    if not realFileName in inList:
                        inList.append(realFileName)
                else:
                    inList.append(fileName)
        # Minimum Bias
        if runConfig.input.inMinBias:
            if indexMin+options.nMin*divF >= len(minbiasList):
                # re-use files when Minimum-Bias files are not enough   
                indexMin = 0   
            tmpList = minbiasList[indexMin:indexMin+options.nMin*divF]
            indexMin += options.nMin*divF
            for fileName,vals in tmpList:
                # instantiate  FileSpec
                file = FileSpec()
                file.lfn            = fileName
                file.GUID           = vals['guid']
                file.fsize          = vals['fsize']
                file.md5sum         = vals['md5sum']
                file.dataset        = minbiasDataset
                file.prodDBlock     = minbiasDataset
                file.dispatchDBlock = commonDispName
                file.type       = 'input'
                file.status     = 'ready'
                if options.trf and not isDirectAccess:
                    file.prodDBlockToken = 'local'
                jobR.addFile(file)
                minList.append(fileName)
        # Cavern
        if runConfig.input.inCavern:
            if indexCavern+options.nCav*divF >= len(cavernList):
                # re-use files when Cavern files are not enough   
                indexCavern = 0   
            tmpList = cavernList[indexCavern:indexCavern+options.nCav*divF]
            indexCavern += options.nCav*divF
            for fileName,vals in tmpList:
                # instantiate  FileSpec
                file = FileSpec()
                file.lfn            = fileName
                file.GUID           = vals['guid']
                file.fsize          = vals['fsize']
                file.md5sum         = vals['md5sum']
                file.dataset        = cavernDataset
                file.prodDBlock     = cavernDataset
                file.dispatchDBlock = commonDispName
                file.type           = 'input'
                file.status         = 'ready'
                if options.trf and not isDirectAccess:
                    file.prodDBlockToken = 'local'
                jobR.addFile(file)
                cavList.append(fileName)
        # BeamHalo
        if runConfig.input.inBeamHalo:
            if options.useCommonHalo:
                # integrated dataset
                if indexBHalo+options.nBeamHalo >= len(beamHaloList):
                    # re-use files when BeamHalo files are not enough   
                    indexBHalo = 0   
                tmpList = beamHaloList[indexBHalo:indexBHalo+options.nBeamHalo]
                indexBHalo += options.nBeamHalo
            else:
                # separate datasets
                if indexBHaloA+options.nBeamHaloA >= len(beamHaloAList):
                    # re-use files when BeamHalo files are not enough   
                    indexBHaloA = 0   
                if indexBHaloC+options.nBeamHaloC >= len(beamHaloCList):
                    # re-use files when BeamHalo files are not enough   
                    indexBHaloC = 0   
                tmpList = beamHaloAList[indexBHaloA:indexBHaloA+options.nBeamHaloA] + \
                          beamHaloCList[indexBHaloC:indexBHaloC+options.nBeamHaloC]
                indexBHaloA += options.nBeamHaloA
                indexBHaloC += options.nBeamHaloC
            tmpIndex = 0
            for fileName,vals in tmpList:
                # instantiate  FileSpec
                file = FileSpec()
                file.lfn            = fileName
                file.GUID           = vals['guid']
                file.fsize          = vals['fsize']
                file.md5sum         = vals['md5sum']
                if options.useCommonHalo:
                    # integrated dataset
                    file.dataset        = beamHaloDataset
                    file.prodDBlock     = beamHaloDataset
                    file.dispatchDBlock = commonDispName
                else:
                    # separate datasets
                    if tmpIndex < options.nBeamHaloA:
                        file.dataset        = beamHaloAdataset
                        file.prodDBlock     = beamHaloAdataset
                        file.dispatchDBlock = commonDispName
                    else:
                        file.dataset        = beamHaloCdataset
                        file.prodDBlock     = beamHaloCdataset
                        file.dispatchDBlock = commonDispName
                file.type           = 'input'
                file.status         = 'ready'
                if options.trf and not isDirectAccess:
                    file.prodDBlockToken = 'local'
                jobR.addFile(file)
                bhaloList.append(fileName)
                tmpIndex += 1
        # BeamGas
        if runConfig.input.inBeamGas:        
            if options.useCommonGas:
                # integrated dataset
                if indexBGas+options.nBeamGas >= len(beamGasList):
                    # re-use files when BeamGas files are not enough   
                    indexBGas = 0   
                tmpList = beamGasList[indexBGas:indexBGas+options.nBeamGas]
                indexBGas += options.nBeamGas
            else:
                # separate dataset
                if indexBGasH+options.nBeamGasH >= len(beamGasHList):
                    # re-use files when BeamGas files are not enough   
                    indexBGasH = 0   
                if indexBGasC+options.nBeamGasC >= len(beamGasCList):
                    # re-use files when BeamGas files are not enough   
                    indexBGasC = 0   
                if indexBGasO+options.nBeamGasO >= len(beamGasOList):
                    # re-use files when BeamGas files are not enough   
                    indexBGasO = 0   
                tmpList = beamGasHList[indexBGasH:indexBGasH+options.nBeamGasH] + \
                          beamGasCList[indexBGasC:indexBGasC+options.nBeamGasC] + \
                          beamGasOList[indexBGasO:indexBGasO+options.nBeamGasO]
                indexBGasH += options.nBeamGasH
                indexBGasC += options.nBeamGasC
                indexBGasO += options.nBeamGasO
            tmpIndex = 0
            for fileName,vals in tmpList:
                # instantiate  FileSpec
                file = FileSpec()
                file.lfn            = fileName
                file.GUID           = vals['guid']
                file.fsize          = vals['fsize']
                file.md5sum         = vals['md5sum']
                if options.useCommonHalo:
                    # integrated dataset
                    file.dataset        = beamGasDataset
                    file.prodDBlock     = beamGasDataset
                    file.dispatchDBlock = commonDispName
                else:
                    # separate datasets
                    if tmpIndex < options.nBeamGasH:
                        file.dataset        = beamGasHdataset
                        file.prodDBlock     = beamGasHdataset
                        file.dispatchDBlock = commonDispName
                    elif tmpIndex < (options.nBeamGasH+options.nBeamGasC):
                        file.dataset        = beamGasCdataset
                        file.prodDBlock     = beamGasCdataset
                        file.dispatchDBlock = commonDispName
                    else:
                        file.dataset        = beamGasOdataset
                        file.prodDBlock     = beamGasOdataset
                        file.dispatchDBlock = commonDispName
                file.type           = 'input'
                file.status         = 'ready'
                if options.trf and not isDirectAccess:
                    file.prodDBlockToken = 'local'
                jobR.addFile(file)
                bgasList.append(fileName)
                tmpIndex += 1
        #@
        #@ important
        #@ Done with an input files description of the job.
        # Increment pointer (index) to a next block of files
        #@ If split by events is requested Index file is incremented in a different place (above)

        if options.nEventsPerJob < 0:
          indexFiles += divF
    elif options.trf and options.nEventsPerJob > 0 :
        # Calculate how many events to skip mainly to set event number for evgen
        nEventsToSkip = nSkips*options.nEventsPerJob
        # @Increment number of skipped blocks
        nSkips = nSkips + 1

    # output files
    outMap = {}
    AthenaUtils.convertConfToOutput(runConfig,jobR,outMap,options.individualOutDS,options.extOutFile)
    # set space token
    for file in jobR.Files:
        if file.type in ['output','log']:
            if options.spaceToken != '':
                file.destinationDBlockToken = options.spaceToken
            else:
                if options.burstSubmit == '':                
                    defaulttoken = Client.PandaSites[options.site]['defaulttoken']
                    file.destinationDBlockToken = Client.getDefaultSpaceToken(vomsFQAN,defaulttoken)
    # job parameters
    param = ''
    if not options.nobuild:
        param  += '-l %s ' % fileS.lfn
    param += '-r %s ' % runDir
    if not options.trf:
        tmpJobO = jobO        
        # modify one-liner for G4 random seeds
        if runConfig.other.G4RandomSeeds > 0:
            if options.singleLine != '':
                tmpJobO = re.sub('-c "%s" ' % options.singleLine,
                                 '-c "%s;from G4AtlasApps.SimFlags import SimFlags;SimFlags.SeedsG4=%s" ' \
                                 % (options.singleLine,runConfig.other.G4RandomSeeds+iSubJob),
                                 jobO)
            else:
                tmpJobO = ('-c "from G4AtlasApps.SimFlags import SimFlags;SimFlags.SeedsG4=%s" ' \
                           % (runConfig.other.G4RandomSeeds+iSubJob)) + jobO
        # replace full-path jobOs
        for tmpFullName,tmpLocalName in AthenaUtils.fullPathJobOs.iteritems():
            tmpJobO = re.sub(tmpFullName,tmpLocalName,tmpJobO)
        # set jobO parameter
        param += '-j "%s" ' % urllib.quote(tmpJobO)
        # DBRelease
        if options.dbRelease != '':
            tmpItems = options.dbRelease.split(':')
            tmpDbrDS  = tmpItems[0]
            tmpDbrLFN = tmpItems[1]
            # instantiate  FileSpec
            fileName = tmpDbrLFN
            vals     = dbrFiles[tmpDbrLFN]
            file = FileSpec()
            file.lfn            = fileName
            file.GUID           = vals['guid']
            file.fsize          = vals['fsize']
            file.md5sum         = vals['md5sum']
            file.dataset        = tmpDbrDS
            file.prodDBlock     = tmpDbrDS
            file.dispatchDBlock = tmpDbrDS
            file.type       = 'input'
            file.status     = 'ready'
            jobR.addFile(file)
            # set DBRelease parameter
            param += '--dbrFile %s ' % file.lfn
            if options.dbRunNumber != '':
                param += '--dbrRun %s ' % options.dbRunNumber
    else:
        # replace parameters for TRF
        tmpJobO = jobO
        # output : basenames are in outMap['IROOT'] trough extOutFile
        tmpOutMap = []
        for tmpName,tmpLFN in outMap['IROOT']:
            tmpJobO = tmpJobO.replace('%OUT.' + tmpName,tmpName)
            # set correct name in outMap
            tmpOutMap.append((tmpName,tmpLFN))
        # set output for normal TRF (not for ARA)
        if not options.ara:
            outMap['IROOT'] = tmpOutMap 
        # input
	inPattList = [('%IN',inList),('%MININ',minList),('%CAVIN',cavList),
                      ('%BHIN',bhaloList),('%BGIN',bgasList)]    
	for tmpPatt,tmpInList in inPattList:
            if tmpJobO.find(tmpPatt) != -1 and len(tmpInList) > 0:
                tmpJobO = AthenaUtils.replaceParam(tmpPatt,tmpInList,tmpJobO)
        # DBRelease
        tmpItems = tmpJobO.split()
        if options.dbRelease != '':
            # mimic a trf parameter to reuse following algorithm
            tmpItems += ['%DB='+options.dbRelease]
        for tmpItem in tmpItems:
            match = re.search('%DB=([^:]+):(.+)$',tmpItem)
            if match:
                tmpDbrDS  = match.group(1)
                tmpDbrLFN = match.group(2)
                # skip if it is already extracted
                if tmpDbrLFN in inList:
                    continue
                # instantiate  FileSpec
                fileName = tmpDbrLFN
                vals     = dbrFiles[tmpDbrLFN]
                file = FileSpec()
                file.lfn            = fileName
                file.GUID           = vals['guid']
                file.fsize          = vals['fsize']
                file.md5sum         = vals['md5sum']
                file.dataset        = tmpDbrDS
                file.prodDBlock     = tmpDbrDS
                file.dispatchDBlock = tmpDbrDS
                file.type       = 'input'
                file.status     = 'ready'
                jobR.addFile(file)
                inList.append(fileName)
                # replace parameters
                tmpJobO = tmpJobO.replace(match.group(0),tmpDbrLFN)
        # random seed
        for tmpItem in tmpJobO.split():
            match = re.search('%RNDM=(\d+)( |$|\'|\"|;)',tmpItem)
            if match:
                tmpRndmNum = int(match.group(1)) + iSubJob
                # replace parameters
                tmpJobO = re.sub(match.group(0),'%s%s' % (tmpRndmNum,match.group(2)),tmpJobO)
        # skipEvent
        for tmpItem in tmpJobO.split():
            match = re.search('%SKIPEVENTS(:*)(\d*)( |$|\'|\"|;)',tmpItem)
            if match != None:
                if match.group(2) == '':
                    # replace %SKIPEVENTS
                    tmpJobO = re.sub(match.group(0),'%s%s' % (nEventsToSkip,match.group(3)),tmpJobO)
                else:
                    # replace %SKIPEVENTS:NN
                    tmpSkipEvents = int(match.group(2)) + nEventsToSkip 
                    tmpJobO = re.sub(match.group(0),'%s%s' % (tmpSkipEvents,match.group(3)),tmpJobO)
        # set jobO parameter
        param += '-j "%s" ' % urllib.quote(tmpJobO)		
    param += '-i "%s" ' % inList
    param += '-m "%s" ' % minList
    param += '-n "%s" ' % cavList
    if bhaloList != []:
        param += '--beamHalo "%s" ' % bhaloList
    if bgasList != []:
        param += '--beamGas "%s" ' % bgasList
    param += '-o "%s" ' % outMap
    if runConfig.input.inColl:
        param += '-c '
    if runConfig.input.inBS:
        param += '-b '
    if runConfig.input.backNavi:
        param += '-e '
    if options.shipinput:
        param += '--shipInput '
        # GUID boundaries
        if devidedByGUID:
            param += '--guidBoundary "%s" ' % guidBoundary
            param += '--collRefName %s ' % runConfig.input.collRefName
    pStr1 = ''    
    if runConfig.other.rndmStream != []:
        pStr1 = "AtRndmGenSvc=Service('AtRndmGenSvc');AtRndmGenSvc.Seeds=["
        for stream in runConfig.other.rndmStream:
            num = runConfig.other.rndmNumbers[runConfig.other.rndmStream.index(stream)]
            pStr1 += "'%s %d %d'," % (stream,num[0]+iSubJob,num[1]+iSubJob)
        pStr1 += "]"

    # @ If split by event option was invoked
    pStr2 = ''
    if options.nEventsPerJob > 0 and (not options.trf):
        # @ Number of events to be processed per job
        param1 = "theApp.EvtMax=%s" % options.nEventsPerJob
        # @ possibly skip events in a file
        if runConfig.input.noInput:
            pStr2 = param1
        else:
            param2 = "EventSelector.SkipEvents=%s" % nEventsToSkip
            # @ Form a string to add to job parameters
            pStr2 = '%s;%s' % (param1,param2)
    # parameter
    if pStr1 != '' or pStr2 != '':
	if pStr1 == '' or pStr2 == '':
	    param += '-f "%s" ' % (pStr1+pStr2)
	else:
            param += '-f "%s;%s" ' % (pStr1,pStr2)
    # libDS 
    if options.libDS != "" or options.nobuild:
        param += '-a %s ' % archiveName
    # addPoolFC
    if options.addPoolFC != "":
        param += '--addPoolFC %s ' % options.addPoolFC
    # use corruption checker
    if options.corCheck:
        param += '--corCheck '
    # disable to skip missing files
    if options.notSkipMissing:
        param += '--notSkipMissing '
    # given PFN 
    if options.pfnList != '':
        param += '--givenPFN '
    # create symlink for MC data
    if options.mcData != '':
        param += '--mcData %s ' % options.mcData
    # source URL
    matchURL = re.search("(http.*://[^/]+)/",Client.baseURLSSL)
    if matchURL != None:
        param += " --sourceURL %s " % matchURL.group(1)
    # run TRF
    if options.trf:
        param += '--trf '
    # use ARA 
    if options.ara:
        param += '--ara '
    # event picking
    if options.eventPickEvtList:
        param += '--eventPickTxt=%s ' % eventPickRunEvtDat.split('/')[-1]
    # general input format
    if options.generalInput:
        param += '--generalInput '
    # use local access for TRF and BS
    if (options.trf or runConfig.input.inBS) and not isDirectAccess:
        param += '--useLocalIO '        
    # use theApp.nextEvent
    if options.useNextEvent:
        param += '--useNextEvent '
    # use code tracer 
    if options.codeTrace:
        param += '--codeTrace '
    # assign    
    jobR.jobParameters = param

    if options.verbose:
        tmpLog.debug(param)
    # set destinationDBlock to associate jobs with the same output container
    # do it here since destinationDBock is used for output file naming
    jobR.destinationDBlock = original_outDS_Name
    # append
    jobList.append(jobR)


# check output dataset length for individualOutDS
tmpIdvOutDsList = []
if options.individualOutDS:
    for tmpFile in jobList[-1].Files:
        if tmpFile.type in ['output','log']:
            if not tmpFile.dataset in tmpIdvOutDsList:
                if not PsubUtils.checkOutDsName(tmpFile.dataset,distinguishedName,options.official,nickName,options.site,vomsFQAN):
                    tmpLog.info("a suffix is added to datasetname for each output stream when --individualOutDS is used. e.g. outDS_suffix:%s. Please use shorter name for --outDS" % tmpFile.dataset)
                    sys.exit(EC_Config)
                tmpIdvOutDsList.append(tmpFile.dataset)


# no submit 
if not options.nosubmit:

    # upload proxy for glexec
    if Client.PandaSites.has_key(options.site):
        # delegation
        delResult = PsubUtils.uploadProxy(options.site,options.myproxy,gridPassPhrase,
                                          Client.PandaClouds[options.cloud]['pilotowners'],
                                          options.verbose)
        if not delResult:
            tmpLog.error("failed to upload proxy")
            sys.exit(EC_MyProxy)

    # normal/burst submission
    if options.burstSubmit == '':
        # normal submission

	# dataset location
	tmpOutDsLocation = Client.PandaSites[options.site]['ddm']
	if options.spaceToken != '':
	    if Client.PandaSites[options.site]['setokens'].has_key(options.spaceToken):
		tmpOutDsLocation = Client.PandaSites[options.site]['setokens'][options.spaceToken]
	# always use site's SE for libDS
      	tmpLibDsLocation = tmpOutDsLocation

        # dataset registration
        if not Client.isDQ2free(options.site):
            # register output dataset
            tmpIdvOutDsList = []
            if (not outputDSexist) or options.destSE != '':
                if not options.individualOutDS:
                    Client.addDataset(options.outDS,options.verbose,location=tmpOutDsLocation,dsExist=outputDSexist)
                    tmpIdvOutDsList.append(options.outDS)
            if options.individualOutDS:        
                # register individual datasets
                for tmpFile in jobList[-1].Files:
                    if tmpFile.type in ['output','log']:
                        if (not outputIndvDSlist.has_key(tmpFile.dataset)) and (not tmpFile.dataset in tmpIdvOutDsList):
                            Client.addDataset(tmpFile.dataset,options.verbose,location=tmpOutDsLocation,dsExist=outputDSexist)
                            tmpIdvOutDsList.append(tmpFile.dataset)

            # register output dataset container
            if original_outDS_Name.endswith('/'):
                # create
                if not outputContExist:
                    Client.createContainer(original_outDS_Name,options.verbose)
                # add dataset
                if not outputDSexist:
                    for tmpIdvOutDS in tmpIdvOutDsList:
                        Client.addDatasetsToContainer(original_outDS_Name,[tmpIdvOutDS],options.verbose)

            # register shadow dataset
            if not outputDSexist:
                Client.addDataset("%s%s" % (options.outDS,suffixShadow),options.verbose)

            # register libDS
            if options.libDS == '' and (not options.nobuild):
                Client.addDataset(jobB.destinationDBlock,options.verbose,location=tmpLibDsLocation)

        # submit
        tmpLog.info("submit to %s" % options.site)
        status,out = Client.submitJobs(jobList,options.verbose)
        if not Client.PandaSites[options.site]['status'] in ['online','brokeroff']:
            tmpLog.warning("%s is %s. Your jobs will wait until it becomes online" % \
                           (options.site,Client.PandaSites[options.site]['status']))
    else:
        print "\n=========="
        # burst submission
        origLibDS = jobB.destinationDBlock
        origOutDS = options.outDS
        prevLibDS = jobB.destinationDBlock
        prevOutDS = options.outDS
        # loop over all sites
        for tmpSite in options.burstSubmit.split(','):
            # cloud
            tmpCloud = Client.PandaSites[tmpSite]['cloud']
            newJobList = []
            newLibDS = '%s.%s' % (origLibDS,tmpSite)
            newOutDS = '%s.%s' % (origOutDS,tmpSite)
            repPatt  = {}
            if options.removeBurstLimit:
                # use all jobs
                limitedJobList = jobList
            else:
                # use buildJob and one runAthena by default
                limitedJobList = jobList[:2]
            for tmpJob in limitedJobList:
                job = copy.deepcopy(tmpJob)
                # set cloud/site
                job.cloud          = tmpCloud
                job.computingSite  = tmpSite
                job.destinationSE  = job.computingSite
                # set destDBlock
                if job.prodSourceLabel == 'panda':
                    job.destinationDBlock = newLibDS
                    # pattern to modify LFN
                    subLibDSPatt = '^'+prevLibDS
                else:
                    job.destinationDBlock = newOutDS
                    # pattern to modify LFN
                    subOutDSPatt = '^'+prevOutDS
                # correct files
                for file in job.Files:
                    # process output/log and lib.tgz
                    if file.type == 'input':
                        # skip buildJob
                        if job.prodSourceLabel == 'panda':
                            continue
                        # only lib.tgz
                        if re.search(subLibDSPatt,file.dataset) == None:
                            continue
                        # set datasets 
                        file.dataset           = newLibDS
                        file.prodDBlock        = newLibDS
                        file.dispatchDBlock    = newLibDS
                    else:
                        # set datasets
                        if job.prodSourceLabel == 'panda':
                            file.dataset           = newLibDS
                            file.prodDBlock        = newLibDS                    
                            file.destinationDBlock = newLibDS
                        else:
                            file.dataset           = newOutDS
                            file.prodDBlock        = newOutDS                    
                            file.destinationDBlock = newOutDS
                        file.destinationSE = job.destinationSE
                    # modify LFN
                    oldLFN = file.lfn
                    if job.prodSourceLabel == 'panda' or file.type == 'input':
                        newLFN = re.sub(subLibDSPatt,file.dataset,oldLFN)
                    else:
                        newLFN = re.sub(subOutDSPatt,file.dataset,oldLFN)
                    file.lfn = newLFN
                    # pattern for parameter replacement
                    repPatt[oldLFN] = newLFN
                # modify jobParams
                for oldLFN,newLFN in repPatt.iteritems():
                    job.jobParameters = re.sub(oldLFN,newLFN,job.jobParameters)
                # append
                newJobList.append(job)
            # submit
            tmpLog.info("submit to %s" % tmpSite)
            status,out = Client.submitJobs(newJobList,options.verbose)
            if status==0:
                tmpLog.info(" OK")
		jobID = out[0][1]
                # record jobID
		tmpJobIdFile = open(jobid_file,'w')
                tmpJobIdFile.write(str(jobID))
                tmpJobIdFile.close()
            else:
                tmpLog.info(" NG : %s" % status)
            time.sleep(2)
        # don't update DB
        sys.exit(0)


    print '==================='
    if out == []:
        tmpLog.error("Job submission was denied")
        sys.exit(EC_Submit)
    # check length
    if len(jobList) != len(out):
        wStr  = "Only %s/%s sub-jobs were accepted " % (len(out),len(jobList))
        wStr += "since the number of sub-jobs in a single submission is limited. "
        wStr += "Please submit the same job again (i.e., the same inDS and outDS). "
        wStr += "New sub-jobs will run only on unused files and outputs will be added to the same outDS\n"
        tmpLog.warning(wStr)
    outstr   = ''
    buildStr = ''
    runStr   = ''
    for index,o in enumerate(out):
        if o != None:
            if index==0:
                # set JobID
                jobID = o[1]
            if index==0 and options.libDS=='' and (not options.nobuild):
                outstr += "  > build\n"
                outstr += "    PandaID=%s\n" % o[0]
                buildStr = '%s' % o[0]            
            elif (index==1 and options.libDS=='' and not options.nobuild) or \
                 (index==0 and (options.libDS!='' or options.nobuild)):
                outstr += "  > run\n"
                outstr += "    PandaID=%s" % o[0]
                runStr = '%s' % o[0]                        
            elif index+1==len(out):
                outstr += "-%s" % o[0]
                runStr += '-%s' % o[0]                                    
    print ' JobID  : %s' % jobID
    print ' Status : %d' % status
    print outstr

    # create dir for DB
    dbdir = os.path.expanduser(os.environ['PANDA_CONFIG_ROOT'])
    if not os.path.exists(dbdir):
        os.makedirs(dbdir)

    # record jobID
    tmpJobIdFile = open(jobid_file,'w')
    tmpJobIdFile.write(str(jobID))
    tmpJobIdFile.close()

    # record libDS
    if options.libDS == '' and not options.nobuild:
        tmpFile = open(libds_file,'w')
        tmpFile.write(jobB.destinationDBlock)
        tmpFile.close()

# go back to current dir
os.chdir(currentDir)

# check site occupancy
if (siteSpecified or expCloudFlag) and options.burstSubmit == '':
    Client.checkQueuedAnalJobs(options.site,options.verbose)

# try another site if input files remain
options.crossSite -= 1
if options.crossSite > 0 and options.inDS != '' and not siteSpecified:
    if missList != []:
        PsubUtils.runPathenaRec(runConfig,missList,tmpDir,fullExecString,options.nfiles,inputFileMap,
                                options.site,options.crossSite,archiveName,options.removedDS,
                                options.inDS,options.goodRunListXML,options.eventPickEvtList,
                                options.verbose)
# succeeded
sys.exit(0)
